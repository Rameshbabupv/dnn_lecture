{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/your-repo/Deep_Neural_Network_Architectures/blob/main/labs/Logic_Gates_TensorFlow_Colab_Exercise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "title-header"
   },
   "source": [
    "# 🔢 Logic Gates with TensorFlow - Hands-On Exercise\n",
    "\n",
    "## 📚 **Course**: Deep Neural Network Architectures (21CSE558T)\n",
    "## 🎯 **Module 1**: Introduction to Neural Networks and TensorFlow\n",
    "\n",
    "---\n",
    "\n",
    "### 🎓 **Learning Objectives**\n",
    "By the end of this exercise, you will be able to:\n",
    "1. Implement basic neural networks using TensorFlow/Keras\n",
    "2. Understand the difference between linearly separable and non-linearly separable problems\n",
    "3. Build and train models for AND, OR, and XOR logic gates\n",
    "4. Compare single-layer perceptron vs multi-layer perceptron architectures\n",
    "5. Visualize neural network learning progress\n",
    "\n",
    "---\n",
    "\n",
    "### ⏱️ **Estimated Time**: 60-90 minutes\n",
    "### 📊 **Difficulty Level**: Beginner\n",
    "### 🔧 **Prerequisites**: Basic Python knowledge, Understanding of boolean logic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-1-header"
   },
   "source": [
    "# 📦 Section 1: Setup and Dependencies\n",
    "\n",
    "Let's start by importing all the necessary libraries and setting up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imports-setup"
   },
   "outputs": [],
   "source": [
    "# Import essential libraries\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Display TensorFlow version\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(\"✅ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "concept-explanation"
   },
   "source": [
    "## 🧠 **Concept Review: Logic Gates**\n",
    "\n",
    "Before diving into neural networks, let's refresh our understanding of logic gates:\n",
    "\n",
    "- **AND Gate**: Output is 1 only when BOTH inputs are 1\n",
    "- **OR Gate**: Output is 1 when AT LEAST ONE input is 1  \n",
    "- **XOR Gate**: Output is 1 when inputs are DIFFERENT\n",
    "\n",
    "### 🤔 **Key Question**: Why is XOR different from AND/OR?\n",
    "\n",
    "**Linear Separability**: AND and OR gates can be solved with a straight line separating the classes, but XOR cannot!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-2-header"
   },
   "source": [
    "# 📊 Section 2: Data Preparation and Truth Tables\n",
    "\n",
    "Let's create our training data representing all possible input combinations for 2-bit logic gates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "data-preparation"
   },
   "outputs": [],
   "source": [
    "# Define input data (all possible 2-bit combinations)\n",
    "X = np.array([[0, 0],\n",
    "              [0, 1], \n",
    "              [1, 0],\n",
    "              [1, 1]], dtype=np.float32)\n",
    "\n",
    "# Define target outputs for each logic gate\n",
    "y_and = np.array([[0], [0], [0], [1]], dtype=np.float32)  # AND gate\n",
    "y_or  = np.array([[0], [1], [1], [1]], dtype=np.float32)  # OR gate\n",
    "y_xor = np.array([[0], [1], [1], [0]], dtype=np.float32)  # XOR gate\n",
    "\n",
    "print(\"📋 Training Data Shape:\")\n",
    "print(f\"Input X shape: {X.shape}\")\n",
    "print(f\"Output shapes: {y_and.shape}\")\n",
    "print(\"\\n📊 Truth Table:\")\n",
    "\n",
    "# Create and display truth table\n",
    "truth_table = pd.DataFrame({\n",
    "    'Input A': X[:, 0].astype(int),\n",
    "    'Input B': X[:, 1].astype(int), \n",
    "    'AND': y_and.flatten().astype(int),\n",
    "    'OR': y_or.flatten().astype(int),\n",
    "    'XOR': y_xor.flatten().astype(int)\n",
    "})\n",
    "\n",
    "print(truth_table.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "student-checkpoint-1"
   },
   "source": [
    "### 🎯 **Student Checkpoint 1**\n",
    "\n",
    "**Question**: Look at the truth table above. Can you identify which gates can be separated by a straight line in 2D space?\n",
    "\n",
    "**Hint**: Think about plotting the points (0,0), (0,1), (1,0), (1,1) and trying to draw a line that separates the 0s from 1s for each gate.\n",
    "\n",
    "*Write your answer in the cell below:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "student-answer-1"
   },
   "source": [
    "**Your Answer:**\n",
    "\n",
    "(Double-click to edit this cell and write your thoughts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-3-header"
   },
   "source": [
    "# 🏗️ Section 3: Building Neural Network Models\n",
    "\n",
    "Now let's create our neural network architectures. We'll start with the simplest case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "single-layer-explanation"
   },
   "source": [
    "## 🔵 **Part 3A: Single-Layer Perceptron (AND & OR Gates)**\n",
    "\n",
    "For linearly separable problems like AND and OR gates, a single neuron is sufficient!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "single-layer-function"
   },
   "outputs": [],
   "source": [
    "def create_single_layer_model():\n",
    "    \"\"\"\n",
    "    Creates a single-layer perceptron model\n",
    "    Architecture: Input(2) → Dense(1, sigmoid) → Output\n",
    "    \"\"\"\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid', input_shape=(2,), name='output_layer')\n",
    "    ])\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer='sgd',  # Stochastic Gradient Descent\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create and display model architecture\n",
    "sample_model = create_single_layer_model()\n",
    "print(\"🏗️ Single-Layer Perceptron Architecture:\")\n",
    "sample_model.summary()\n",
    "\n",
    "print(\"\\n🔢 Model Parameters:\")\n",
    "print(f\"Total parameters: {sample_model.count_params()}\")\n",
    "print(\"Parameters breakdown: 2 weights + 1 bias = 3 total\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "training-function"
   },
   "source": [
    "## 🎯 **Training Function for Single-Layer Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train-single-layer"
   },
   "outputs": [],
   "source": [
    "def train_logic_gate(gate_name, y_target, epochs=100, verbose=1):\n",
    "    \"\"\"\n",
    "    Train a single-layer perceptron for a specific logic gate\n",
    "    \n",
    "    Args:\n",
    "        gate_name (str): Name of the logic gate (\"AND\" or \"OR\")\n",
    "        y_target (np.array): Target outputs for the gate\n",
    "        epochs (int): Number of training epochs\n",
    "        verbose (int): Verbosity level for training\n",
    "    \n",
    "    Returns:\n",
    "        model: Trained TensorFlow model\n",
    "        history: Training history\n",
    "    \"\"\"\n",
    "    print(f\"\\n🚀 Training {gate_name} Gate...\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Create model\n",
    "    model = create_single_layer_model()\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        X, y_target,\n",
    "        epochs=epochs,\n",
    "        verbose=verbose,\n",
    "        batch_size=4  # Use all 4 samples in each batch\n",
    "    )\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions = model.predict(X, verbose=0)\n",
    "    rounded_predictions = np.round(predictions)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_target, rounded_predictions)\n",
    "    \n",
    "    print(f\"\\n✅ {gate_name} Gate Training Complete!\")\n",
    "    print(f\"Final Accuracy: {accuracy:.2%}\")\n",
    "    \n",
    "    # Display results\n",
    "    results_df = pd.DataFrame({\n",
    "        'Input A': X[:, 0].astype(int),\n",
    "        'Input B': X[:, 1].astype(int),\n",
    "        'Expected': y_target.flatten().astype(int),\n",
    "        'Predicted': rounded_predictions.flatten().astype(int),\n",
    "        'Raw Output': predictions.flatten()\n",
    "    })\n",
    "    \n",
    "    print(f\"\\n📊 {gate_name} Gate Results:\")\n",
    "    print(results_df.to_string(index=False))\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "print(\"✅ Training function defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-4-header"
   },
   "source": [
    "# 🔥 Section 4: Training AND Gate\n",
    "\n",
    "Let's train our first neural network to learn the AND logic gate!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train-and-gate"
   },
   "outputs": [],
   "source": [
    "# Train AND gate\n",
    "and_model, and_history = train_logic_gate(\"AND\", y_and, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "visualization-and"
   },
   "source": [
    "## 📈 **Visualizing AND Gate Training Progress**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "plot-and-training"
   },
   "outputs": [],
   "source": [
    "# Plot training progress for AND gate\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Plot loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(and_history.history['loss'], 'b-', linewidth=2)\n",
    "plt.title('AND Gate - Training Loss', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Binary Crossentropy Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(and_history.history['accuracy'], 'g-', linewidth=2)\n",
    "plt.title('AND Gate - Training Accuracy', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0, 1.1])\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"🎯 Final AND Gate Loss: {and_history.history['loss'][-1]:.4f}\")\n",
    "print(f\"🎯 Final AND Gate Accuracy: {and_history.history['accuracy'][-1]:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-5-header"
   },
   "source": [
    "# 🔥 Section 5: Training OR Gate\n",
    "\n",
    "Now let's train the OR gate using the same architecture!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train-or-gate"
   },
   "outputs": [],
   "source": [
    "# Train OR gate\n",
    "or_model, or_history = train_logic_gate(\"OR\", y_or, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "plot-or-training"
   },
   "outputs": [],
   "source": [
    "# Plot training progress for OR gate\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Plot loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(or_history.history['loss'], 'r-', linewidth=2)\n",
    "plt.title('OR Gate - Training Loss', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Binary Crossentropy Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(or_history.history['accuracy'], 'orange', linewidth=2)\n",
    "plt.title('OR Gate - Training Accuracy', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0, 1.1])\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"🎯 Final OR Gate Loss: {or_history.history['loss'][-1]:.4f}\")\n",
    "print(f\"🎯 Final OR Gate Accuracy: {or_history.history['accuracy'][-1]:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "student-checkpoint-2"
   },
   "source": [
    "### 🎯 **Student Checkpoint 2**\n",
    "\n",
    "**Observation Questions**:\n",
    "1. How quickly did the AND and OR gates reach high accuracy?\n",
    "2. What do you notice about the loss curves?\n",
    "3. Do you think a single-layer perceptron will work for XOR? Why or why not?\n",
    "\n",
    "*Write your observations below:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "student-answer-2"
   },
   "source": [
    "**Your Observations:**\n",
    "\n",
    "(Double-click to edit this cell and write your thoughts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-6-header"
   },
   "source": [
    "# ⚡ Section 6: The XOR Challenge - Multi-Layer Perceptron\n",
    "\n",
    "Now comes the interesting part! XOR cannot be solved with a single-layer perceptron. We need a hidden layer!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xor-explanation"
   },
   "source": [
    "## 🧩 **Why XOR is Special**\n",
    "\n",
    "XOR is **non-linearly separable**. This means:\n",
    "- You cannot draw a straight line to separate the outputs\n",
    "- We need a more complex neural network architecture\n",
    "- This requires at least one hidden layer\n",
    "\n",
    "**Historical Note**: This limitation of single-layer perceptrons was one of the main criticisms that led to the \"AI Winter\" in the 1970s, until multi-layer perceptrons were developed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "multi-layer-function"
   },
   "outputs": [],
   "source": [
    "def create_multi_layer_model(hidden_units=4):\n",
    "    \"\"\"\n",
    "    Creates a multi-layer perceptron model for XOR\n",
    "    Architecture: Input(2) → Dense(hidden_units, sigmoid) → Dense(1, sigmoid) → Output\n",
    "    \"\"\"\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(hidden_units, activation='sigmoid', input_shape=(2,), name='hidden_layer'),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid', name='output_layer')\n",
    "    ])\n",
    "    \n",
    "    # Use Adam optimizer (more sophisticated than SGD)\n",
    "    model.compile(\n",
    "        optimizer='adam',  # Adam adapts learning rate automatically\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create and display multi-layer model\n",
    "xor_model = create_multi_layer_model(hidden_units=4)\n",
    "print(\"🏗️ Multi-Layer Perceptron Architecture for XOR:\")\n",
    "xor_model.summary()\n",
    "\n",
    "print(\"\\n🔢 Model Parameters:\")\n",
    "print(f\"Total parameters: {xor_model.count_params()}\")\n",
    "print(\"Parameters breakdown:\")\n",
    "print(\"  Hidden layer: (2 inputs × 4 neurons) + 4 biases = 12\")\n",
    "print(\"  Output layer: (4 inputs × 1 neuron) + 1 bias = 5\")\n",
    "print(\"  Total: 12 + 5 = 17 parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xor-training-function"
   },
   "source": [
    "## 🎯 **Training Function for XOR Gate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train-xor-function"
   },
   "outputs": [],
   "source": [
    "def train_xor_gate(epochs=1000, hidden_units=4, verbose=1):\n",
    "    \"\"\"\n",
    "    Train a multi-layer perceptron for XOR gate\n",
    "    \n",
    "    Args:\n",
    "        epochs (int): Number of training epochs (more than AND/OR)\n",
    "        hidden_units (int): Number of neurons in hidden layer\n",
    "        verbose (int): Verbosity level for training\n",
    "    \n",
    "    Returns:\n",
    "        model: Trained TensorFlow model\n",
    "        history: Training history\n",
    "    \"\"\"\n",
    "    print(f\"\\n🚀 Training XOR Gate with Multi-Layer Perceptron...\")\n",
    "    print(f\"Hidden units: {hidden_units}, Epochs: {epochs}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Create model\n",
    "    model = create_multi_layer_model(hidden_units=hidden_units)\n",
    "    \n",
    "    # Train the model (more epochs needed for non-linear problem)\n",
    "    history = model.fit(\n",
    "        X, y_xor,\n",
    "        epochs=epochs,\n",
    "        verbose=verbose,\n",
    "        batch_size=4\n",
    "    )\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions = model.predict(X, verbose=0)\n",
    "    rounded_predictions = np.round(predictions)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_xor, rounded_predictions)\n",
    "    \n",
    "    print(f\"\\n✅ XOR Gate Training Complete!\")\n",
    "    print(f\"Final Accuracy: {accuracy:.2%}\")\n",
    "    \n",
    "    # Display results\n",
    "    results_df = pd.DataFrame({\n",
    "        'Input A': X[:, 0].astype(int),\n",
    "        'Input B': X[:, 1].astype(int),\n",
    "        'Expected': y_xor.flatten().astype(int),\n",
    "        'Predicted': rounded_predictions.flatten().astype(int),\n",
    "        'Raw Output': predictions.flatten()\n",
    "    })\n",
    "    \n",
    "    print(f\"\\n📊 XOR Gate Results:\")\n",
    "    print(results_df.to_string(index=False))\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "print(\"✅ XOR training function defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-7-header"
   },
   "source": [
    "# 🔥 Section 7: Training XOR Gate\n",
    "\n",
    "Time for the big challenge - training the XOR gate!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train-xor-gate"
   },
   "outputs": [],
   "source": [
    "# Train XOR gate\n",
    "xor_model, xor_history = train_xor_gate(epochs=1000, hidden_units=4, verbose=0)\n",
    "print(\"\\n🎉 XOR training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "plot-xor-training"
   },
   "outputs": [],
   "source": [
    "# Plot training progress for XOR gate\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Plot loss\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(xor_history.history['loss'], 'purple', linewidth=2)\n",
    "plt.title('XOR Gate - Training Loss', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Binary Crossentropy Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot accuracy\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(xor_history.history['accuracy'], 'teal', linewidth=2)\n",
    "plt.title('XOR Gate - Training Accuracy', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0, 1.1])\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot comparison of final accuracies\n",
    "plt.subplot(1, 3, 3)\n",
    "gates = ['AND', 'OR', 'XOR']\n",
    "final_accuracies = [\n",
    "    and_history.history['accuracy'][-1],\n",
    "    or_history.history['accuracy'][-1], \n",
    "    xor_history.history['accuracy'][-1]\n",
    "]\n",
    "colors = ['blue', 'red', 'purple']\n",
    "bars = plt.bar(gates, final_accuracies, color=colors, alpha=0.7)\n",
    "plt.title('Final Accuracies Comparison', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0, 1.1])\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, acc in zip(bars, final_accuracies):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "             f'{acc:.2%}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"🎯 Final XOR Gate Loss: {xor_history.history['loss'][-1]:.4f}\")\n",
    "print(f\"🎯 Final XOR Gate Accuracy: {xor_history.history['accuracy'][-1]:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-8-header"
   },
   "source": [
    "# 🔬 Section 8: Experiments and Analysis\n",
    "\n",
    "Let's experiment with different configurations to better understand the learning process!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "experiment-1"
   },
   "source": [
    "## 🧪 **Experiment 1: What happens if we try single-layer for XOR?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "experiment-single-layer-xor"
   },
   "outputs": [],
   "source": [
    "print(\"🧪 Experiment: Single-layer perceptron vs XOR\")\n",
    "print(\"This should fail to learn the XOR pattern!\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Try to train XOR with single-layer (this should fail!)\n",
    "failed_model, failed_history = train_logic_gate(\"XOR (Single-Layer)\", y_xor, epochs=200, verbose=0)\n",
    "\n",
    "print(\"\\n💡 Conclusion: Single-layer perceptron cannot learn XOR!\")\n",
    "print(\"This demonstrates why we need hidden layers for non-linearly separable problems.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "experiment-2"
   },
   "source": [
    "## 🧪 **Experiment 2: Effect of hidden layer size on XOR learning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "experiment-hidden-units"
   },
   "outputs": [],
   "source": [
    "print(\"🧪 Experiment: Different hidden layer sizes for XOR\")\n",
    "print(\"Testing hidden units: [2, 4, 8, 16]\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "hidden_units_to_test = [2, 4, 8, 16]\n",
    "results = []\n",
    "\n",
    "for units in hidden_units_to_test:\n",
    "    print(f\"\\n🔄 Testing {units} hidden units...\")\n",
    "    model, history = train_xor_gate(epochs=500, hidden_units=units, verbose=0)\n",
    "    final_accuracy = history.history['accuracy'][-1]\n",
    "    results.append(final_accuracy)\n",
    "    print(f\"Final accuracy: {final_accuracy:.2%}\")\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(hidden_units_to_test, results, 'o-', linewidth=2, markersize=8)\n",
    "plt.title('XOR Accuracy vs Hidden Layer Size', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Number of Hidden Units')\n",
    "plt.ylabel('Final Accuracy')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.ylim([0, 1.1])\n",
    "\n",
    "# Add value labels\n",
    "for i, (units, acc) in enumerate(zip(hidden_units_to_test, results)):\n",
    "    plt.annotate(f'{acc:.2%}', (units, acc), textcoords=\"offset points\", \n",
    "                xytext=(0,10), ha='center', fontweight='bold')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n💡 Conclusion: Even 2 hidden units can solve XOR!\")\n",
    "print(\"More units may learn faster but aren't always necessary.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "student-checkpoint-3"
   },
   "source": [
    "### 🎯 **Student Checkpoint 3 - Experiments Analysis**\n",
    "\n",
    "**Analysis Questions**:\n",
    "1. What was the accuracy of the single-layer perceptron on XOR?\n",
    "2. What's the minimum number of hidden units needed to solve XOR?\n",
    "3. Do more hidden units always lead to better performance?\n",
    "4. Why do you think XOR requires more training epochs than AND/OR?\n",
    "\n",
    "*Write your analysis below:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "student-answer-3"
   },
   "source": [
    "**Your Analysis:**\n",
    "\n",
    "(Double-click to edit this cell and write your thoughts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-9-header"
   },
   "source": [
    "# 🎯 Section 9: Model Weights Exploration\n",
    "\n",
    "Let's peek inside our trained models to understand what they learned!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "explore-weights"
   },
   "outputs": [],
   "source": [
    "def explore_model_weights(model, gate_name):\n",
    "    \"\"\"\n",
    "    Display the learned weights and biases of a trained model\n",
    "    \"\"\"\n",
    "    print(f\"\\n🔍 {gate_name} Model Weights Analysis\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    for i, layer in enumerate(model.layers):\n",
    "        weights, biases = layer.get_weights()\n",
    "        print(f\"\\nLayer {i+1} ({layer.name}):\")\n",
    "        print(f\"  Weights shape: {weights.shape}\")\n",
    "        print(f\"  Weights: {weights.flatten()}\")\n",
    "        print(f\"  Biases: {biases}\")\n",
    "\n",
    "# Explore weights for all trained models\n",
    "explore_model_weights(and_model, \"AND Gate\")\n",
    "explore_model_weights(or_model, \"OR Gate\")\n",
    "explore_model_weights(xor_model, \"XOR Gate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-10-header"
   },
   "source": [
    "# 🎨 Section 10: Visualization of Decision Boundaries\n",
    "\n",
    "Let's visualize how our neural networks make decisions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "decision-boundary-viz"
   },
   "outputs": [],
   "source": [
    "def plot_decision_boundary(model, gate_name, y_true):\n",
    "    \"\"\"\n",
    "    Plot the decision boundary learned by the model\n",
    "    \"\"\"\n",
    "    # Create a mesh of points\n",
    "    h = 0.01  # Step size\n",
    "    x_min, x_max = -0.5, 1.5\n",
    "    y_min, y_max = -0.5, 1.5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    \n",
    "    # Make predictions on the mesh\n",
    "    mesh_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "    Z = model.predict(mesh_points, verbose=0)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    \n",
    "    # Plot decision boundary\n",
    "    plt.contourf(xx, yy, Z, levels=50, alpha=0.6, cmap='RdYlBu')\n",
    "    plt.colorbar(label='Model Output')\n",
    "    \n",
    "    # Plot data points\n",
    "    for i in range(len(X)):\n",
    "        color = 'red' if y_true[i] == 1 else 'blue'\n",
    "        marker = 'o' if y_true[i] == 1 else 's'\n",
    "        plt.scatter(X[i, 0], X[i, 1], c=color, marker=marker, s=100, \n",
    "                   edgecolors='black', linewidth=2)\n",
    "    \n",
    "    plt.title(f'{gate_name} Gate - Decision Boundary', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Input A')\n",
    "    plt.ylabel('Input B')\n",
    "    plt.legend(['Output = 0 (Blue Squares)', 'Output = 1 (Red Circles)'], loc='upper right')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "# Plot decision boundaries for all gates\n",
    "plot_decision_boundary(and_model, \"AND\", y_and.flatten())\n",
    "plot_decision_boundary(or_model, \"OR\", y_or.flatten())\n",
    "plot_decision_boundary(xor_model, \"XOR\", y_xor.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-11-header"
   },
   "source": [
    "# 📝 Section 11: Practice Exercises\n",
    "\n",
    "Now it's your turn to experiment!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "exercise-1"
   },
   "source": [
    "## 🏋️ **Exercise 1: Implement NAND Gate**\n",
    "\n",
    "NAND (NOT AND) gate outputs 0 only when both inputs are 1.\n",
    "\n",
    "**Your task**: Create the target outputs for NAND and train a model.\n",
    "\n",
    "**Hint**: NAND truth table is the opposite of AND!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "exercise-1-solution"
   },
   "outputs": [],
   "source": [
    "# TODO: Define NAND gate outputs\n",
    "# NAND truth table: [1, 1, 1, 0]\n",
    "y_nand = np.array([[1], [1], [1], [0]], dtype=np.float32)  # Your code here\n",
    "\n",
    "# TODO: Train NAND gate model\n",
    "nand_model, nand_history = train_logic_gate(\"NAND\", y_nand, epochs=100)\n",
    "\n",
    "# TODO: Plot the results\n",
    "plot_decision_boundary(nand_model, \"NAND\", y_nand.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "exercise-2"
   },
   "source": [
    "## 🏋️ **Exercise 2: Experiment with Different Optimizers**\n",
    "\n",
    "Try training the XOR gate with different optimizers and compare the results.\n",
    "\n",
    "**Your task**: Test SGD, Adam, and RMSprop optimizers on XOR gate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "exercise-2-solution"
   },
   "outputs": [],
   "source": [
    "def train_xor_with_optimizer(optimizer_name, optimizer, epochs=500):\n",
    "    \"\"\"\n",
    "    Train XOR gate with specified optimizer\n",
    "    \"\"\"\n",
    "    model = create_multi_layer_model(hidden_units=4)\n",
    "    \n",
    "    # TODO: Compile model with the given optimizer\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(X, y_xor, epochs=epochs, verbose=0)\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "# TODO: Test different optimizers\n",
    "optimizers = {\n",
    "    'SGD': tf.keras.optimizers.SGD(learning_rate=0.1),\n",
    "    'Adam': tf.keras.optimizers.Adam(),\n",
    "    'RMSprop': tf.keras.optimizers.RMSprop()\n",
    "}\n",
    "\n",
    "optimizer_results = {}\n",
    "\n",
    "for name, optimizer in optimizers.items():\n",
    "    print(f\"Training XOR with {name} optimizer...\")\n",
    "    model, history = train_xor_with_optimizer(name, optimizer)\n",
    "    optimizer_results[name] = history\n",
    "    \n",
    "    # Check final accuracy\n",
    "    predictions = model.predict(X, verbose=0)\n",
    "    rounded_predictions = np.round(predictions)\n",
    "    accuracy = accuracy_score(y_xor, rounded_predictions)\n",
    "    print(f\"{name} final accuracy: {accuracy:.2%}\\n\")\n",
    "\n",
    "# TODO: Plot comparison of training curves\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot loss comparison\n",
    "plt.subplot(1, 2, 1)\n",
    "for name, history in optimizer_results.items():\n",
    "    plt.plot(history.history['loss'], label=name, linewidth=2)\n",
    "plt.title('Loss Comparison - Different Optimizers', fontweight='bold')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot accuracy comparison  \n",
    "plt.subplot(1, 2, 2)\n",
    "for name, history in optimizer_results.items():\n",
    "    plt.plot(history.history['accuracy'], label=name, linewidth=2)\n",
    "plt.title('Accuracy Comparison - Different Optimizers', fontweight='bold')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-12-header"
   },
   "source": [
    "# 🎯 Section 12: Key Takeaways and Summary\n",
    "\n",
    "Let's summarize what we've learned in this exercise!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "summary-comparison"
   },
   "source": [
    "## 📊 **Complete Comparison Table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "final-comparison"
   },
   "outputs": [],
   "source": [
    "# Create final comparison table\n",
    "comparison_data = {\n",
    "    'Logic Gate': ['AND', 'OR', 'XOR'],\n",
    "    'Linearly Separable': ['Yes', 'Yes', 'No'],\n",
    "    'Architecture': ['Single-layer', 'Single-layer', 'Multi-layer'],\n",
    "    'Hidden Units': [0, 0, 4],\n",
    "    'Total Parameters': [3, 3, 17],\n",
    "    'Training Epochs': [100, 100, 1000],\n",
    "    'Optimizer': ['SGD', 'SGD', 'Adam'],\n",
    "    'Final Accuracy': [\n",
    "        f\"{and_history.history['accuracy'][-1]:.2%}\",\n",
    "        f\"{or_history.history['accuracy'][-1]:.2%}\", \n",
    "        f\"{xor_history.history['accuracy'][-1]:.2%}\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"📋 COMPLETE LOGIC GATES COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"🎯 KEY INSIGHTS:\")\n",
    "print(\"=\"*80)\n",
    "print(\"✅ AND & OR gates: Simple single-layer perceptron works perfectly\")\n",
    "print(\"✅ XOR gate: Requires hidden layer due to non-linear separability\")\n",
    "print(\"✅ More complex problems need: More neurons, more epochs, better optimizers\")\n",
    "print(\"✅ Neural networks can learn any boolean function with sufficient architecture\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "learning-outcomes"
   },
   "source": [
    "## 🧠 **What You've Learned**\n",
    "\n",
    "### **Technical Skills:**\n",
    "1. ✅ **TensorFlow/Keras Usage**: Building, compiling, and training neural networks\n",
    "2. ✅ **Architecture Design**: Single-layer vs multi-layer perceptrons\n",
    "3. ✅ **Model Training**: Using different optimizers, loss functions, and metrics\n",
    "4. ✅ **Data Visualization**: Plotting training curves and decision boundaries\n",
    "5. ✅ **Model Evaluation**: Accuracy calculation and prediction analysis\n",
    "\n",
    "### **Conceptual Understanding:**\n",
    "1. ✅ **Linear Separability**: Why some problems need more complex architectures\n",
    "2. ✅ **Universal Approximation**: Neural networks can learn any boolean function\n",
    "3. ✅ **Architecture Selection**: Matching network complexity to problem complexity\n",
    "4. ✅ **Training Dynamics**: How different optimizers affect learning\n",
    "5. ✅ **Historical Context**: The XOR problem and the development of deep learning\n",
    "\n",
    "### **Next Steps:**\n",
    "- 🚀 **Module 2**: Optimization algorithms and regularization techniques\n",
    "- 🚀 **Module 3**: Applying neural networks to real image classification problems\n",
    "- 🚀 **Module 4**: Convolutional Neural Networks (CNNs)\n",
    "- 🚀 **Module 5**: Advanced architectures and transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "final-assessment"
   },
   "source": [
    "# 📝 Final Assessment Questions\n",
    "\n",
    "Test your understanding with these questions:\n",
    "\n",
    "### **Question 1**: Why can't a single-layer perceptron solve the XOR problem?\n",
    "**Answer**: _(Your answer here)_\n",
    "\n",
    "### **Question 2**: What is the minimum number of hidden neurons needed to solve XOR?\n",
    "**Answer**: _(Your answer here)_\n",
    "\n",
    "### **Question 3**: Why did we use different optimizers for simple gates vs XOR?\n",
    "**Answer**: _(Your answer here)_\n",
    "\n",
    "### **Question 4**: How would you modify the architecture to solve a 3-input logic gate?\n",
    "**Answer**: _(Your answer here)_\n",
    "\n",
    "### **Question 5**: What real-world applications might use similar neural network approaches?\n",
    "**Answer**: _(Your answer here)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "resources-references"
   },
   "source": [
    "# 📚 Additional Resources\n",
    "\n",
    "### **Course Materials:**\n",
    "- 📖 **Textbook**: \"Deep Learning with Python\" by François Chollet - Chapter 1 & 2\n",
    "- 📖 **Reference**: \"Deep Learning\" by Goodfellow, Bengio & Courville - Chapter 6\n",
    "\n",
    "### **Online Resources:**\n",
    "- 🌐 **TensorFlow Documentation**: https://www.tensorflow.org/tutorials\n",
    "- 🌐 **Keras Guide**: https://keras.io/guides/\n",
    "- 🌐 **Neural Networks Playground**: https://playground.tensorflow.org/\n",
    "\n",
    "### **Historical Papers:**\n",
    "- 📄 **Perceptrons** (Minsky & Papert, 1969) - The XOR Problem\n",
    "- 📄 **Learning representations by back-propagating errors** (Rumelhart et al., 1986)\n",
    "\n",
    "---\n",
    "\n",
    "## 🎉 **Congratulations!**\n",
    "\n",
    "You have successfully completed the Logic Gates with TensorFlow exercise! You've taken your first steps into the world of deep learning and neural networks.\n",
    "\n",
    "**Remember**: Every complex AI system, from computer vision to natural language processing, builds upon these fundamental concepts you've just learned.\n",
    "\n",
    "---\n",
    "\n",
    "*Course: Deep Neural Network Architectures (21CSE558T)*  \n",
    "*SRM University - M.Tech Program*  \n",
    "*Module 1: Introduction to Neural Networks*"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}