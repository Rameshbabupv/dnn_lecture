# 45 MCQ Question Bank for Formative Test I

## Complete Question List with Answers

### Module 1: Introduction to Deep Learning (20 questions)

**Q1.** The XOR problem cannot be solved by a single perceptron because:
- Answer: b) It is not linearly separable

**Q2.** In TensorFlow, which data structure is the fundamental building block?
- Answer: c) Tensor

**Q3.** The sigmoid activation function outputs values in the range:
- Answer: b) (0, 1)

**Q4.** Which loss function is typically used for binary classification?
- Answer: c) Binary Cross-entropy

**Q5.** Which activation function can output negative values?
- Answer: c) Tanh

**Q6.** The perceptron learning algorithm can solve:
- Answer: a) Only linearly separable problems

**Q7.** Which TensorFlow function is used to create a tensor filled with zeros?
- Answer: b) tf.zeros()

**Q8.** Which type of neural network layer performs weighted sum of inputs?
- Answer: b) Dense layer

**Q9.** In forward propagation, the output of each layer is:
- Answer: b) Input to the next layer

**Q10.** The softmax activation function is primarily used for:
- Answer: b) Multi-class classification

**Q11.** Which activation function is most commonly used in hidden layers of modern deep networks?
- Answer: c) ReLU

**Q12.** The backpropagation algorithm uses which mathematical concept?
- Answer: b) Chain rule of differentiation

**Q13.** The derivative of ReLU function for positive inputs is:
- Answer: b) 1

**Q14.** The bias term in a neuron:
- Answer: b) Allows shifting of the activation function

**Q15.** In a multilayer perceptron, the universal approximation theorem states:
- Answer: b) A single hidden layer can approximate any continuous function

**Q16.** The mathematical representation of a perceptron output is:
- Answer: b) y = Ïƒ(wx + b)

**Q17.** TensorFlow operations are executed:
- Answer: b) In a computational graph

**Q18.** The main advantage of using ReLU over sigmoid activation is:
- Answer: c) Mitigation of vanishing gradient problem

**Q19.** In TensorFlow, eager execution means:
- Answer: a) Operations are executed immediately

**Q20.** The vanishing gradient problem primarily affects:
- Answer: b) Deep networks with sigmoid activations

### Module 2: Optimization & Regularization (25 questions)

**Q21.** Which gradient descent variant uses the entire dataset for each update?
- Answer: c) Batch Gradient Descent

**Q22.** Learning rate determines:
- Answer: b) Size of steps toward minimum

**Q23.** Overfitting occurs when:
- Answer: c) Model performs well on training but poorly on test data

**Q24.** Which regularization technique randomly sets some neurons to zero during training?
- Answer: c) Dropout

**Q25.** Early stopping prevents overfitting by:
- Answer: a) Stopping training when validation loss increases

**Q26.** Underfitting can be reduced by:
- Answer: c) Increasing model complexity

**Q27.** Which normalization technique normalizes across the batch dimension?
- Answer: c) Batch normalization

**Q28.** The dropout rate typically ranges between:
- Answer: b) 0.2 to 0.5

**Q29.** Which statement about stochastic gradient descent is true?
- Answer: b) Uses one sample for each update

**Q30.** The purpose of validation set is to:
- Answer: b) Tune hyperparameters and monitor overfitting

**Q31.** The vanishing gradient problem is most severe with which activation function?
- Answer: b) Sigmoid

**Q32.** The Adam optimizer combines:
- Answer: a) Momentum and RMSprop

**Q33.** L2 regularization adds which penalty to the loss function?
- Answer: b) Sum of squared weights

**Q34.** The exploding gradient problem can be mitigated by:
- Answer: b) Gradient clipping

**Q35.** Which optimizer adapts the learning rate for each parameter individually?
- Answer: c) AdaGrad

**Q36.** The momentum parameter in gradient descent:
- Answer: b) Accelerates convergence in relevant directions

**Q37.** Batch normalization is applied:
- Answer: c) Between layers during training

**Q38.** In mini-batch gradient descent, the batch size affects:
- Answer: c) Both computational efficiency and gradient noise

**Q39.** L1 regularization tends to produce:
- Answer: b) Sparse weight matrices

**Q40.** Which problem is addressed by batch normalization?
- Answer: b) Internal covariate shift

**Q41.** Weight initialization using Xavier/Glorot method aims to:
- Answer: b) Maintain gradient flow through layers

**Q42.** The RMSprop optimizer addresses which problem of AdaGrad?
- Answer: c) Aggressive learning rate decay

**Q43.** The bias-variance tradeoff is related to:
- Answer: b) Overfitting and underfitting

**Q44.** Which technique can help with both vanishing and exploding gradients?
- Answer: b) Residual connections

**Q45.** The learning rate finder technique helps to:
- Answer: b) Determine optimal learning rate range

## Key Finding
**IMPORTANT**: The question "The ReLU activation function is defined as: a) max(0, x)" does NOT exist in this 45-question bank. The closest ReLU-related question is Q13 about the derivative of ReLU function.