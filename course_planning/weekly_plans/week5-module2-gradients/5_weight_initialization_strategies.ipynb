{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚öñÔ∏è Concept 5: Weight Initialization Strategies\n",
    "\n",
    "## Deep Neural Network Architectures - Week 5\n",
    "**Module:** 2 - Optimization and Regularization  \n",
    "**Topic:** Proper Weight Initialization for Gradient Flow\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Learning Objectives\n",
    "By the end of this notebook, you will:\n",
    "1. **Understand** why weight initialization is critical for gradient flow\n",
    "2. **Implement** Xavier/Glorot and He initialization strategies\n",
    "3. **Compare** different initialization methods experimentally\n",
    "4. **Choose** appropriate initialization for different activation functions\n",
    "\n",
    "---\n",
    "\n",
    "## üéº The Orchestra Tuning Analogy\n",
    "\n",
    "**Before a Concert:**\n",
    "- **Random tuning:** Each musician randomly tunes their instrument\n",
    "- **Result:** Cacophony, no harmony, terrible performance\n",
    "\n",
    "**Proper Tuning:**\n",
    "- **Concert master:** Sets the standard (A = 440 Hz)\n",
    "- **Each instrument:** Tunes relative to the standard\n",
    "- **Result:** Beautiful harmony, excellent performance\n",
    "\n",
    "**In Neural Networks:**\n",
    "- **Random weights:** Like random tuning ‚Üí poor gradient flow\n",
    "- **Proper initialization:** Like proper tuning ‚Üí excellent training\n",
    "\n",
    "---\n",
    "\n",
    "## üíª Mathematical Foundation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (14, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xavier_initialization():\n",
    "    \"\"\"Demonstrate Xavier/Glorot initialization principles\"\"\"\n",
    "    \n",
    "    print(\"üßÆ XAVIER/GLOROT INITIALIZATION ANALYSIS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Mathematical foundation\n",
    "    def calculate_xavier_variance(n_in, n_out):\n",
    "        \"\"\"Calculate optimal variance for Xavier initialization\"\"\"\n",
    "        return 2.0 / (n_in + n_out)\n",
    "\n",
    "    def calculate_he_variance(n_in):\n",
    "        \"\"\"Calculate optimal variance for He initialization\"\"\"\n",
    "        return 2.0 / n_in\n",
    "    \n",
    "    # Test different layer sizes\n",
    "    layer_configs = [\n",
    "        (784, 512),   # Input to first hidden\n",
    "        (512, 256),   # Hidden layer\n",
    "        (256, 128),   # Hidden layer\n",
    "        (128, 64),    # Hidden layer\n",
    "        (64, 10)      # Output layer\n",
    "    ]\n",
    "    \n",
    "    print(\"üìä OPTIMAL VARIANCE CALCULATIONS:\")\n",
    "    print()\n",
    "    print(f\"{'Layer':<15} {'Input':<8} {'Output':<8} {'Xavier Var':<12} {'He Var':<12} {'Xavier Std':<12} {'He Std':<12}\")\n",
    "    print(\"-\" * 85)\n",
    "    \n",
    "    for i, (n_in, n_out) in enumerate(layer_configs, 1):\n",
    "        xavier_var = calculate_xavier_variance(n_in, n_out)\n",
    "        he_var = calculate_he_variance(n_in)\n",
    "        xavier_std = np.sqrt(xavier_var)\n",
    "        he_std = np.sqrt(he_var)\n",
    "        \n",
    "        print(f\"Layer {i:<8} {n_in:<8} {n_out:<8} {xavier_var:<12.6f} {he_var:<12.6f} {xavier_std:<12.6f} {he_std:<12.6f}\")\n",
    "    \n",
    "    print()\n",
    "    print(\"üí° KEY INSIGHTS:\")\n",
    "    print(\"1. Xavier variance = 2/(n_in + n_out) - balances input and output\")\n",
    "    print(\"2. He variance = 2/n_in - optimized for ReLU activations\")\n",
    "    print(\"3. Larger input dimensions ‚Üí smaller initial weights\")\n",
    "    print(\"4. Proper scaling prevents activation/gradient saturation\")\n",
    "    \n",
    "    return layer_configs\n",
    "\n",
    "# Run Xavier initialization analysis\n",
    "layer_configs = xavier_initialization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_initialization_methods():\n",
    "    \"\"\"Compare different weight initialization strategies\"\"\"\n",
    "    \n",
    "    print(\"\\nüî¨ INITIALIZATION METHODS COMPARISON\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Define initialization strategies\n",
    "    initializers = {\n",
    "        'Random Normal (Bad)': tf.keras.initializers.RandomNormal(stddev=1.0),\n",
    "        'Random Small': tf.keras.initializers.RandomNormal(stddev=0.01),\n",
    "        'Xavier/Glorot Uniform': tf.keras.initializers.GlorotUniform(),\n",
    "        'Xavier/Glorot Normal': tf.keras.initializers.GlorotNormal(),\n",
    "        'He Uniform': tf.keras.initializers.HeUniform(),\n",
    "        'He Normal': tf.keras.initializers.HeNormal()\n",
    "    }\n",
    "    \n",
    "    # Create models with different initializations\n",
    "    models = {}\n",
    "    \n",
    "    for name, initializer in initializers.items():\n",
    "        if 'He' in name:\n",
    "            # Use ReLU for He initialization\n",
    "            activation = 'relu'\n",
    "        else:\n",
    "            # Use tanh for Xavier initialization\n",
    "            activation = 'tanh'\n",
    "        \n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(128, activation=activation, input_shape=(784,),\n",
    "                                 kernel_initializer=initializer, name=f'layer1_{name}'),\n",
    "            tf.keras.layers.Dense(64, activation=activation,\n",
    "                                 kernel_initializer=initializer, name=f'layer2_{name}'),\n",
    "            tf.keras.layers.Dense(32, activation=activation,\n",
    "                                 kernel_initializer=initializer, name=f'layer3_{name}'),\n",
    "            tf.keras.layers.Dense(16, activation=activation,\n",
    "                                 kernel_initializer=initializer, name=f'layer4_{name}'),\n",
    "            tf.keras.layers.Dense(10, activation='softmax', name=f'output_{name}')\n",
    "        ], name=name.replace(' ', '_').replace('/', '_'))\n",
    "        \n",
    "        models[name] = model\n",
    "    \n",
    "    print(f\"‚úÖ Created {len(models)} models with different initializations\")\n",
    "    return models\n",
    "\n",
    "# Create models for comparison\n",
    "comparison_models = compare_initialization_methods()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_weight_distributions(models):\n",
    "    \"\"\"Analyze initial weight distributions for different initialization methods\"\"\"\n",
    "    \n",
    "    print(\"\\nüìä WEIGHT DISTRIBUTION ANALYSIS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    colors = plt.cm.Set3(np.linspace(0, 1, len(models)))\n",
    "    \n",
    "    for idx, (name, model) in enumerate(models.items()):\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        # Collect all weights from the model\n",
    "        all_weights = []\n",
    "        for layer in model.layers:\n",
    "            if hasattr(layer, 'kernel'):\n",
    "                weights = layer.kernel.numpy().flatten()\n",
    "                all_weights.extend(weights)\n",
    "        \n",
    "        all_weights = np.array(all_weights)\n",
    "        \n",
    "        # Plot histogram\n",
    "        ax.hist(all_weights, bins=50, alpha=0.7, color=colors[idx], \n",
    "                density=True, label=name)\n",
    "        \n",
    "        # Calculate statistics\n",
    "        mean_weight = np.mean(all_weights)\n",
    "        std_weight = np.std(all_weights)\n",
    "        min_weight = np.min(all_weights)\n",
    "        max_weight = np.max(all_weights)\n",
    "        \n",
    "        # Add statistical information\n",
    "        ax.axvline(mean_weight, color='red', linestyle='--', alpha=0.8, label=f'Mean: {mean_weight:.4f}')\n",
    "        ax.axvline(mean_weight + std_weight, color='orange', linestyle=':', alpha=0.6)\n",
    "        ax.axvline(mean_weight - std_weight, color='orange', linestyle=':', alpha=0.6, label=f'¬±1œÉ: {std_weight:.4f}')\n",
    "        \n",
    "        ax.set_title(f'{name}\\nRange: [{min_weight:.3f}, {max_weight:.3f}]', fontsize=10)\n",
    "        ax.set_xlabel('Weight Value')\n",
    "        ax.set_ylabel('Density')\n",
    "        ax.legend(fontsize=8)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Print statistics\n",
    "        print(f\"\\n{name}:\")\n",
    "        print(f\"  Mean: {mean_weight:.6f}\")\n",
    "        print(f\"  Std:  {std_weight:.6f}\")\n",
    "        print(f\"  Range: [{min_weight:.6f}, {max_weight:.6f}]\")\n",
    "        print(f\"  Total weights: {len(all_weights)}\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nüí° DISTRIBUTION INSIGHTS:\")\n",
    "    print(\"‚úÖ Good initialization: Zero mean, appropriate variance\")\n",
    "    print(\"‚ö†Ô∏è Too large: May cause saturation or explosion\")\n",
    "    print(\"‚ö†Ô∏è Too small: May cause vanishing gradients\")\n",
    "\n",
    "# Analyze weight distributions\n",
    "analyze_weight_distributions(comparison_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_gradient_flow_by_initialization(models):\n",
    "    \"\"\"Test gradient flow characteristics for different initializations\"\"\"\n",
    "    \n",
    "    print(\"\\nüåä GRADIENT FLOW ANALYSIS BY INITIALIZATION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Generate test data\n",
    "    X_test = tf.random.normal((100, 784))\n",
    "    y_test = tf.random.uniform((100, 10))\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        print(f\"\\nTesting: {name}\")\n",
    "        \n",
    "        # Calculate gradients\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = model(X_test)\n",
    "            loss = tf.reduce_mean(tf.square(predictions - y_test))\n",
    "        \n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        \n",
    "        # Analyze gradient characteristics\n",
    "        grad_norms = []\n",
    "        for i, grad in enumerate(gradients):\n",
    "            if grad is not None and i % 2 == 0:  # Only weights, skip biases\n",
    "                norm = tf.norm(grad).numpy()\n",
    "                grad_norms.append(norm)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        min_grad = min(grad_norms) if grad_norms else 0\n",
    "        max_grad = max(grad_norms) if grad_norms else 0\n",
    "        mean_grad = np.mean(grad_norms) if grad_norms else 0\n",
    "        std_grad = np.std(grad_norms) if grad_norms else 0\n",
    "        \n",
    "        vanished_layers = sum(1 for g in grad_norms if g < 1e-6)\n",
    "        weak_layers = sum(1 for g in grad_norms if g < 1e-4)\n",
    "        \n",
    "        results[name] = {\n",
    "            'gradient_norms': grad_norms,\n",
    "            'min_gradient': min_grad,\n",
    "            'max_gradient': max_grad,\n",
    "            'mean_gradient': mean_grad,\n",
    "            'std_gradient': std_grad,\n",
    "            'vanished_layers': vanished_layers,\n",
    "            'weak_layers': weak_layers,\n",
    "            'loss': loss.numpy(),\n",
    "            'total_layers': len(grad_norms)\n",
    "        }\n",
    "        \n",
    "        print(f\"  Gradient range: {min_grad:.2e} to {max_grad:.2e}\")\n",
    "        print(f\"  Vanished layers: {vanished_layers}/{len(grad_norms)}\")\n",
    "        print(f\"  Weak layers: {weak_layers}/{len(grad_norms)}\")\n",
    "        print(f\"  Loss: {loss:.6f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test gradient flow\n",
    "gradient_results = test_gradient_flow_by_initialization(comparison_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize gradient flow comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "init_names = list(gradient_results.keys())\n",
    "colors = plt.cm.tab10(np.linspace(0, 1, len(init_names)))\n",
    "\n",
    "# Plot 1: Gradient magnitudes by layer\n",
    "ax1 = axes[0, 0]\n",
    "for i, (name, result) in enumerate(gradient_results.items()):\n",
    "    grad_norms = result['gradient_norms']\n",
    "    layers = list(range(1, len(grad_norms) + 1))\n",
    "    ax1.plot(layers, grad_norms, 'o-', color=colors[i], \n",
    "             label=name.replace(' ', '\\n'), linewidth=2, markersize=6)\n",
    "\n",
    "ax1.set_yscale('log')\n",
    "ax1.axhline(y=1e-6, color='red', linestyle='--', alpha=0.5, label='Vanishing threshold')\n",
    "ax1.axhline(y=1e-4, color='orange', linestyle='--', alpha=0.5, label='Weak threshold')\n",
    "ax1.set_xlabel('Layer Number')\n",
    "ax1.set_ylabel('Gradient Magnitude (log scale)')\n",
    "ax1.set_title('Gradient Flow by Initialization Method')\n",
    "ax1.legend(fontsize=8, loc='upper right')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Vanished layers comparison\n",
    "ax2 = axes[0, 1]\n",
    "vanished_counts = [gradient_results[name]['vanished_layers'] for name in init_names]\n",
    "bars = ax2.bar(range(len(init_names)), vanished_counts, color=colors, alpha=0.7)\n",
    "ax2.set_ylabel('Number of Vanished Layers')\n",
    "ax2.set_title('Vanished Layers by Initialization')\n",
    "ax2.set_xticks(range(len(init_names)))\n",
    "ax2.set_xticklabels([name.replace(' ', '\\n') for name in init_names], \n",
    "                    rotation=45, ha='right', fontsize=8)\n",
    "\n",
    "# Add value labels\n",
    "for bar, value in zip(bars, vanished_counts):\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, value + 0.05, str(value), \n",
    "             ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Plot 3: Gradient statistics\n",
    "ax3 = axes[1, 0]\n",
    "min_grads = [gradient_results[name]['min_gradient'] for name in init_names]\n",
    "max_grads = [gradient_results[name]['max_gradient'] for name in init_names]\n",
    "mean_grads = [gradient_results[name]['mean_gradient'] for name in init_names]\n",
    "\n",
    "x_pos = np.arange(len(init_names))\n",
    "width = 0.25\n",
    "\n",
    "ax3.bar(x_pos - width, min_grads, width, label='Min', alpha=0.7, color='blue')\n",
    "ax3.bar(x_pos, mean_grads, width, label='Mean', alpha=0.7, color='green')\n",
    "ax3.bar(x_pos + width, max_grads, width, label='Max', alpha=0.7, color='red')\n",
    "\n",
    "ax3.set_yscale('log')\n",
    "ax3.set_ylabel('Gradient Magnitude (log scale)')\n",
    "ax3.set_title('Gradient Statistics by Initialization')\n",
    "ax3.set_xticks(x_pos)\n",
    "ax3.set_xticklabels([name.replace(' ', '\\n') for name in init_names], \n",
    "                    rotation=45, ha='right', fontsize=8)\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Health score comparison\n",
    "ax4 = axes[1, 1]\n",
    "\n",
    "# Calculate health scores\n",
    "health_scores = []\n",
    "for name in init_names:\n",
    "    result = gradient_results[name]\n",
    "    score = 0\n",
    "    \n",
    "    # No vanished layers: +3 points\n",
    "    if result['vanished_layers'] == 0:\n",
    "        score += 3\n",
    "    elif result['vanished_layers'] <= 1:\n",
    "        score += 1\n",
    "    \n",
    "    # Good gradient range: +3 points\n",
    "    if result['min_gradient'] > 1e-5:\n",
    "        score += 3\n",
    "    elif result['min_gradient'] > 1e-6:\n",
    "        score += 1\n",
    "    \n",
    "    # No explosion: +2 points\n",
    "    if result['max_gradient'] < 10:\n",
    "        score += 2\n",
    "    elif result['max_gradient'] < 100:\n",
    "        score += 1\n",
    "    \n",
    "    # Stable gradients: +2 points\n",
    "    if result['std_gradient'] < result['mean_gradient']:\n",
    "        score += 2\n",
    "    \n",
    "    health_scores.append(score)\n",
    "\n",
    "bars = ax4.bar(range(len(init_names)), health_scores, color=colors, alpha=0.7)\n",
    "ax4.set_ylabel('Health Score (0-10)')\n",
    "ax4.set_title('Overall Initialization Health Score')\n",
    "ax4.set_xticks(range(len(init_names)))\n",
    "ax4.set_xticklabels([name.replace(' ', '\\n') for name in init_names], \n",
    "                    rotation=45, ha='right', fontsize=8)\n",
    "ax4.set_ylim(0, 10)\n",
    "\n",
    "# Add score labels\n",
    "for bar, score in zip(bars, health_scores):\n",
    "    ax4.text(bar.get_x() + bar.get_width()/2, score + 0.1, f'{score}/10', \n",
    "             ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lsuv_initialization(model, X_sample, target_var=1.0, max_iterations=10):\n",
    "    \"\"\"Layer-Sequential Unit-Variance initialization implementation\"\"\"\n",
    "\n",
    "    print(\"\\nüéØ LSUV INITIALIZATION DEMONSTRATION\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"LSUV: Layer-Sequential Unit-Variance Initialization\")\n",
    "    print(\"Goal: Each layer output has unit variance\")\n",
    "    print()\n",
    "\n",
    "    # Create a fresh model for LSUV\n",
    "    lsuv_model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(128, activation='relu', input_shape=(784,), name='layer1'),\n",
    "        tf.keras.layers.Dense(64, activation='relu', name='layer2'),\n",
    "        tf.keras.layers.Dense(32, activation='relu', name='layer3'),\n",
    "        tf.keras.layers.Dense(16, activation='relu', name='layer4'),\n",
    "        tf.keras.layers.Dense(10, activation='softmax', name='output')\n",
    "    ], name='LSUV_Model')\n",
    "\n",
    "    print(\"üìä LSUV Calibration Process:\")\n",
    "    print(\"=\" * 40)\n",
    "\n",
    "    activation_variances = []\n",
    "    layer_adjustments = []\n",
    "\n",
    "    # Process each layer sequentially\n",
    "    for i, layer in enumerate(lsuv_model.layers[:-1]):  # Skip output layer\n",
    "        if hasattr(layer, 'kernel'):  # Only for Dense layers\n",
    "            print(f\"\\nüîß Calibrating Layer {i+1} ({layer.name})...\")\n",
    "            \n",
    "            layer_variances = []\n",
    "            adjustments = 0\n",
    "\n",
    "            for iteration in range(max_iterations):\n",
    "                # Get current activations up to this layer\n",
    "                temp_model = tf.keras.Sequential(lsuv_model.layers[:i+1])\n",
    "                activations = temp_model(X_sample)\n",
    "\n",
    "                # Calculate variance\n",
    "                current_var = tf.reduce_mean(tf.square(activations)).numpy()\n",
    "                layer_variances.append(current_var)\n",
    "\n",
    "                print(f\"  Iteration {iteration+1}: Variance = {current_var:.6f}\")\n",
    "\n",
    "                # Check convergence\n",
    "                if abs(current_var - target_var) < 0.01:\n",
    "                    print(f\"  ‚úÖ Converged to target variance ({target_var})!\")\n",
    "                    break\n",
    "\n",
    "                # Adjust weights to achieve target variance\n",
    "                if current_var > 0:  # Avoid division by zero\n",
    "                    scale_factor = np.sqrt(target_var / current_var)\n",
    "                    layer.kernel.assign(layer.kernel * scale_factor)\n",
    "                    adjustments += 1\n",
    "                    print(f\"  üîÑ Applied scaling factor: {scale_factor:.4f}\")\n",
    "                else:\n",
    "                    print(f\"  ‚ö†Ô∏è Zero variance detected, skipping adjustment\")\n",
    "                    break\n",
    "\n",
    "            activation_variances.append(layer_variances)\n",
    "            layer_adjustments.append(adjustments)\n",
    "            print(f\"  üìã Total adjustments for Layer {i+1}: {adjustments}\")\n",
    "\n",
    "    print(\"\\n‚úÖ LSUV initialization complete!\")\n",
    "    \n",
    "    # Visualize the calibration process\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Plot 1: Variance evolution\n",
    "    ax1 = axes[0]\n",
    "    for i, variances in enumerate(activation_variances):\n",
    "        iterations = list(range(1, len(variances) + 1))\n",
    "        ax1.plot(iterations, variances, 'o-', label=f'Layer {i+1}', linewidth=2, markersize=6)\n",
    "    \n",
    "    ax1.axhline(y=target_var, color='red', linestyle='--', alpha=0.7, \n",
    "                label=f'Target Variance ({target_var})')\n",
    "    ax1.set_xlabel('Iteration')\n",
    "    ax1.set_ylabel('Activation Variance')\n",
    "    ax1.set_title('LSUV Variance Calibration Process')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Number of adjustments per layer\n",
    "    ax2 = axes[1]\n",
    "    layers = [f'Layer {i+1}' for i in range(len(layer_adjustments))]\n",
    "    bars = ax2.bar(layers, layer_adjustments, color='skyblue', alpha=0.7)\n",
    "    ax2.set_ylabel('Number of Adjustments')\n",
    "    ax2.set_title('LSUV Adjustments per Layer')\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, value in zip(bars, layer_adjustments):\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2, value + 0.05, str(value), \n",
    "                 ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return lsuv_model\n",
    "\n",
    "# Demonstrate LSUV initialization\n",
    "X_sample = tf.random.normal((1000, 784))\n",
    "lsuv_model = lsuv_initialization(None, X_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare LSUV with other methods\n",
    "print(\"\\n‚öîÔ∏è LSUV vs OTHER INITIALIZATION METHODS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Test LSUV model\n",
    "X_test = tf.random.normal((100, 784))\n",
    "y_test = tf.random.uniform((100, 10))\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    predictions = lsuv_model(X_test)\n",
    "    loss = tf.reduce_mean(tf.square(predictions - y_test))\n",
    "\n",
    "gradients = tape.gradient(loss, lsuv_model.trainable_variables)\n",
    "grad_norms = [tf.norm(g).numpy() for i, g in enumerate(gradients) if g is not None and i % 2 == 0]\n",
    "\n",
    "lsuv_result = {\n",
    "    'gradient_norms': grad_norms,\n",
    "    'min_gradient': min(grad_norms),\n",
    "    'max_gradient': max(grad_norms),\n",
    "    'mean_gradient': np.mean(grad_norms),\n",
    "    'vanished_layers': sum(1 for g in grad_norms if g < 1e-6),\n",
    "    'weak_layers': sum(1 for g in grad_norms if g < 1e-4)\n",
    "}\n",
    "\n",
    "# Add LSUV to comparison\n",
    "gradient_results['LSUV'] = lsuv_result\n",
    "\n",
    "# Create final comparison table\n",
    "print(\"\\nüìä FINAL INITIALIZATION COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Method':<25} {'Min Grad':<12} {'Max Grad':<12} {'Mean Grad':<12} {'Vanished':<10} {'Health':<8}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for name, result in gradient_results.items():\n",
    "    # Calculate health score\n",
    "    score = 0\n",
    "    if result['vanished_layers'] == 0: score += 3\n",
    "    if result['min_gradient'] > 1e-5: score += 3\n",
    "    if result['max_gradient'] < 10: score += 2\n",
    "    if result.get('std_gradient', 0) < result['mean_gradient']: score += 2\n",
    "    \n",
    "    health_emoji = \"üü¢\" if score >= 7 else \"üü°\" if score >= 5 else \"üü†\" if score >= 3 else \"üî¥\"\n",
    "    \n",
    "    print(f\"{name:<25} {result['min_gradient']:<12.2e} {result['max_gradient']:<12.2e} \"\n",
    "          f\"{result['mean_gradient']:<12.2e} {result['vanished_layers']:<10} {health_emoji} {score}/10\")\n",
    "\n",
    "print(\"\\nüèÜ WINNER: The method with the highest health score!\")\n",
    "best_method = max(gradient_results.keys(), \n",
    "                 key=lambda x: 3*(gradient_results[x]['vanished_layers']==0) + \n",
    "                              3*(gradient_results[x]['min_gradient']>1e-5) + \n",
    "                              2*(gradient_results[x]['max_gradient']<10))\n",
    "print(f\"üéñÔ∏è Best performing initialization: {best_method}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîç Initialization Strategy Guide\n",
    "\n",
    "### üìä Method Comparison Summary\n",
    "\n",
    "| Method | Best For | Pros | Cons | Health Score |\n",
    "|--------|----------|------|------|-------------|\n",
    "| **He Initialization** | ReLU networks | Prevents vanishing, simple | Only for ReLU | üü¢ 8-9/10 |\n",
    "| **Xavier/Glorot** | Sigmoid/Tanh | Mathematical foundation | Poor for ReLU | üü° 6-7/10 |\n",
    "| **LSUV** | Any activation | Adaptive, robust | Computational overhead | üü¢ 8-10/10 |\n",
    "| **Random Normal (small)** | Shallow networks | Simple | Poor for deep networks | üü† 4-5/10 |\n",
    "| **Random Normal (large)** | None | None | Causes explosions | üî¥ 1-2/10 |\n",
    "\n",
    "### üéØ **Practical Guidelines**\n",
    "\n",
    "#### ‚úÖ **Use He Initialization when:**\n",
    "- Using ReLU or ReLU variants (Leaky ReLU, ELU)\n",
    "- Building deep networks (>5 layers)\n",
    "- Need simple, reliable performance\n",
    "- Computational efficiency is important\n",
    "\n",
    "#### ‚úÖ **Use Xavier/Glorot when:**\n",
    "- Using sigmoid or tanh activations\n",
    "- Building shallow to medium networks (<10 layers)\n",
    "- Following classical approaches\n",
    "\n",
    "#### ‚úÖ **Use LSUV when:**\n",
    "- Building very deep networks (>20 layers)\n",
    "- Using mixed activation functions\n",
    "- Need maximum performance\n",
    "- Can afford initialization overhead\n",
    "\n",
    "#### ‚ùå **Avoid:**\n",
    "- Large random initialization (stddev > 1.0)\n",
    "- Zero initialization (no gradients)\n",
    "- Same initialization for all layer types\n",
    "- Ignoring activation function choice\n",
    "\n",
    "---\n",
    "\n",
    "## üí° Key Mathematical Insights\n",
    "\n",
    "### **Xavier Initialization Formula:**\n",
    "```\n",
    "Variance = 2 / (n_input + n_output)\n",
    "```\n",
    "**Reasoning:** Balances forward and backward pass variance\n",
    "\n",
    "### **He Initialization Formula:**\n",
    "```\n",
    "Variance = 2 / n_input\n",
    "```\n",
    "**Reasoning:** Accounts for ReLU killing half the activations\n",
    "\n",
    "### **LSUV Principle:**\n",
    "```\n",
    "For each layer: Scale weights until Var(activations) = 1\n",
    "```\n",
    "**Reasoning:** Ensures optimal activation magnitude layer by layer\n",
    "\n",
    "---\n",
    "\n",
    "## üß™ Experimental Findings\n",
    "\n",
    "From our experiments:\n",
    "\n",
    "1. **He initialization** consistently produces healthy gradient flow for ReLU networks\n",
    "2. **Large random weights** (stddev=1.0) cause severe gradient problems\n",
    "3. **LSUV** adapts to any architecture but requires more computation\n",
    "4. **Proper initialization** can be more important than architecture choice\n",
    "5. **Activation function** determines optimal initialization strategy\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Next Steps\n",
    "\n",
    "In the next notebook, we'll explore:\n",
    "- **Batch normalization** implementation and theory\n",
    "- **How normalization solves initialization problems**\n",
    "- **LayerNorm vs BatchNorm** comparisons\n",
    "\n",
    "---\n",
    "\n",
    "*This notebook demonstrates Concept 5 of Week 5: Deep Neural Network Architectures*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Week 5: Gradients (Python 3.10)",
   "language": "python",
   "name": "week5-gradients"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}