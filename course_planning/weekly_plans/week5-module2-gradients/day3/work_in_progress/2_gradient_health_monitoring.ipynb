{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üè• Concept 2: Gradient Health Monitoring\n",
    "\n",
    "## Deep Neural Network Architectures - Week 5\n",
    "**Module:** 2 - Optimization and Regularization  \n",
    "**Topic:** Monitoring and Diagnosing Gradient Problems\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Learning Objectives\n",
    "By the end of this notebook, you will:\n",
    "1. **Implement** gradient health monitoring systems\n",
    "2. **Interpret** gradient health metrics and warnings\n",
    "3. **Diagnose** network problems using automated tools\n",
    "4. **Compare** healthy vs. problematic gradient patterns\n",
    "\n",
    "---\n",
    "\n",
    "## üè• The Medical Checkup Analogy\n",
    "\n",
    "Just like doctors use vital signs to monitor patient health, we can monitor \"gradient vital signs\" to assess network health:\n",
    "\n",
    "- **Blood Pressure** ‚Üí Gradient Magnitude Range\n",
    "- **Heart Rate** ‚Üí Gradient Stability\n",
    "- **Temperature** ‚Üí Training Temperature (learning rate effectiveness)\n",
    "- **Oxygen Level** ‚Üí Information Flow Quality\n",
    "\n",
    "---\n",
    "\n",
    "## üíª Code Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.16.2\n",
      "NumPy version: 1.24.4\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def monitor_gradient_health(model, X, y):\n    \"\"\"Monitor gradient health during training\"\"\"\n\n    with tf.GradientTape() as tape:\n        predictions = model(X)\n        loss = tf.reduce_mean(tf.square(predictions - y))\n\n    gradients = tape.gradient(loss, model.trainable_variables)\n\n    # Calculate statistics\n    grad_norms = [tf.norm(g).numpy() for g in gradients if g is not None]\n\n    metrics = {\n        'min_gradient': np.min(grad_norms),\n        'max_gradient': np.max(grad_norms),\n        'mean_gradient': np.mean(grad_norms),\n        'std_gradient': np.std(grad_norms),\n        'vanished_layers': sum(1 for g in grad_norms if g < 1e-6),\n        'weak_layers': sum(1 for g in grad_norms if g < 1e-4),\n        'total_layers': len(grad_norms),\n        'gradient_norms': grad_norms\n    }\n\n    return metrics\n\n# Test the monitoring function\nprint(\"üîß Testing Gradient Health Monitoring System...\")\nprint(\"Creating test data...\")\n\nX_sample = tf.random.normal((100, 10))\ny_sample = tf.random.uniform((100, 1))\n\nprint(f\"Test data shape: {X_sample.shape} ‚Üí {y_sample.shape}\")\nprint(\"‚úÖ Gradient health monitoring system ready!\")"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing gradient health interpretation system...\n",
      "‚úÖ Health interpretation system ready!\n"
     ]
    }
   ],
   "source": [
    "def interpret_gradient_health(health_metrics):\n",
    "    \"\"\"Provide human-readable interpretation of gradient health\"\"\"\n",
    "\n",
    "    print(\"\\nüè• GRADIENT HEALTH DIAGNOSIS:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Header with basic stats\n",
    "    print(f\"Total Layers Analyzed: {health_metrics['total_layers']}\")\n",
    "    print(f\"Loss Computed Successfully: ‚úÖ\")\n",
    "    print()\n",
    "\n",
    "    # Vanishing gradient assessment\n",
    "    vanished = health_metrics['vanished_layers']\n",
    "    weak = health_metrics['weak_layers']\n",
    "    \n",
    "    print(\"üîç VANISHING GRADIENT ANALYSIS:\")\n",
    "    if vanished > 0:\n",
    "        print(f\"üö® CRITICAL: {vanished} layers have vanished gradients!\")\n",
    "        print(\"   üìù Recommendation: Switch to ReLU activation\")\n",
    "        print(\"   üìù Alternative: Check weight initialization\")\n",
    "    elif weak > health_metrics['total_layers'] // 2:\n",
    "        print(f\"‚ö†Ô∏è WARNING: {weak} layers have weak gradients\")\n",
    "        print(\"   üìù Recommendation: Consider ReLU or better initialization\")\n",
    "    else:\n",
    "        print(\"‚úÖ GOOD: No significant vanishing gradient problems detected\")\n",
    "    \n",
    "    print()\n",
    "\n",
    "    # Exploding gradient assessment\n",
    "    max_grad = health_metrics['max_gradient']\n",
    "    print(\"üí• EXPLODING GRADIENT ANALYSIS:\")\n",
    "    if max_grad > 100:\n",
    "        print(\"üö® CRITICAL: Gradient explosion detected!\")\n",
    "        print(\"   üìù Recommendation: Apply gradient clipping\")\n",
    "        print(\"   üìù Alternative: Reduce learning rate\")\n",
    "    elif max_grad > 10:\n",
    "        print(\"‚ö†Ô∏è WARNING: Large gradients detected\")\n",
    "        print(\"   üìù Recommendation: Monitor closely, consider gradient clipping\")\n",
    "    else:\n",
    "        print(\"‚úÖ GOOD: No gradient explosion detected\")\n",
    "    \n",
    "    print()\n",
    "\n",
    "    # Overall network stability\n",
    "    min_grad = health_metrics['min_gradient']\n",
    "    gradient_ratio = max_grad / (min_grad + 1e-10)  # Avoid division by zero\n",
    "    \n",
    "    print(\"‚öñÔ∏è NETWORK STABILITY ANALYSIS:\")\n",
    "    if gradient_ratio > 100000:\n",
    "        print(\"üö® CRITICAL: Very large gradient range - training may be unstable\")\n",
    "        print(\"   üìù Recommendation: Improve initialization or add normalization\")\n",
    "    elif gradient_ratio > 10000:\n",
    "        print(\"‚ö†Ô∏è WARNING: Large gradient range detected\")\n",
    "        print(\"   üìù Recommendation: Monitor training stability\")\n",
    "    else:\n",
    "        print(\"‚úÖ GOOD: Gradient range is reasonable\")\n",
    "    \n",
    "    print()\n",
    "\n",
    "    # Detailed statistics\n",
    "    print(\"üìä DETAILED STATISTICS:\")\n",
    "    print(f\"   Min gradient: {min_grad:.2e}\")\n",
    "    print(f\"   Max gradient: {max_grad:.2e}\")\n",
    "    print(f\"   Mean gradient: {health_metrics['mean_gradient']:.2e}\")\n",
    "    print(f\"   Std gradient: {health_metrics['std_gradient']:.2e}\")\n",
    "    print(f\"   Gradient range ratio: {gradient_ratio:.1e}\")\n",
    "    print(f\"   Vanished layers: {vanished}/{health_metrics['total_layers']}\")\n",
    "    print(f\"   Weak layers: {weak}/{health_metrics['total_layers']}\")\n",
    "    \n",
    "    # Overall health score\n",
    "    health_score = 0\n",
    "    if vanished == 0: health_score += 3\n",
    "    elif vanished < health_metrics['total_layers'] // 3: health_score += 1\n",
    "    \n",
    "    if max_grad < 10: health_score += 3\n",
    "    elif max_grad < 100: health_score += 1\n",
    "    \n",
    "    if gradient_ratio < 10000: health_score += 2\n",
    "    elif gradient_ratio < 100000: health_score += 1\n",
    "    \n",
    "    print()\n",
    "    print(\"üéØ OVERALL HEALTH ASSESSMENT:\")\n",
    "    if health_score >= 7:\n",
    "        print(\"üü¢ EXCELLENT: Network gradients are healthy\")\n",
    "    elif health_score >= 5:\n",
    "        print(\"üü° MODERATE: Some issues detected, but manageable\")\n",
    "    elif health_score >= 3:\n",
    "        print(\"üü† POOR: Significant gradient problems detected\")\n",
    "    else:\n",
    "        print(\"üî¥ CRITICAL: Severe gradient problems - network may not train\")\n",
    "    \n",
    "    print(f\"Health Score: {health_score}/8\")\n",
    "\n",
    "# Test the interpretation function\n",
    "print(\"üß™ Testing gradient health interpretation system...\")\n",
    "print(\"‚úÖ Health interpretation system ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèóÔ∏è Creating three different networks for comparison...\n",
      "‚úÖ Three networks created:\n",
      "   1. Sigmoid Network (vanishing gradients expected)\n",
      "   2. ReLU Network (healthy gradients expected)\n",
      "   3. Exploding Network (exploding gradients expected)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rameshbabu/micromamba/envs/week5-gradients/lib/python3.10/site-packages/keras/src/layers/core/dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Create three different networks to compare\n",
    "print(\"üèóÔ∏è Creating three different networks for comparison...\")\n",
    "\n",
    "# Network 1: Problematic sigmoid network\n",
    "sigmoid_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='sigmoid', input_shape=(10,)),\n",
    "    tf.keras.layers.Dense(64, activation='sigmoid'),\n",
    "    tf.keras.layers.Dense(64, activation='sigmoid'),\n",
    "    tf.keras.layers.Dense(64, activation='sigmoid'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "], name='SigmoidNetwork')\n",
    "\n",
    "# Network 2: Healthy ReLU network\n",
    "relu_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu', input_shape=(10,)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "], name='ReLUNetwork')\n",
    "\n",
    "# Network 3: Exploding gradient network (bad initialization)\n",
    "exploding_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='linear', input_shape=(10,),\n",
    "                         kernel_initializer=tf.keras.initializers.RandomNormal(stddev=2.0)),\n",
    "    tf.keras.layers.Dense(64, activation='linear',\n",
    "                         kernel_initializer=tf.keras.initializers.RandomNormal(stddev=2.0)),\n",
    "    tf.keras.layers.Dense(64, activation='linear',\n",
    "                         kernel_initializer=tf.keras.initializers.RandomNormal(stddev=2.0)),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "], name='ExplodingNetwork')\n",
    "\n",
    "print(\"‚úÖ Three networks created:\")\n",
    "print(\"   1. Sigmoid Network (vanishing gradients expected)\")\n",
    "print(\"   2. ReLU Network (healthy gradients expected)\")\n",
    "print(\"   3. Exploding Network (exploding gradients expected)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ANALYZING: Sigmoid Network (Problematic)\n",
      "============================================================\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'keras._tf_keras.keras.losses' has no attribute 'mean_squared_error'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m60\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Monitor gradient health\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m health \u001b[38;5;241m=\u001b[39m \u001b[43mmonitor_gradient_health\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_sample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_sample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m health_reports[name] \u001b[38;5;241m=\u001b[39m health\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Interpret results\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 6\u001b[0m, in \u001b[0;36mmonitor_gradient_health\u001b[0;34m(model, X, y)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[1;32m      5\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m model(X)\n\u001b[0;32m----> 6\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlosses\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean_squared_error\u001b[49m(y, predictions)\n\u001b[1;32m      7\u001b[0m     loss \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mreduce_mean(loss)\n\u001b[1;32m      9\u001b[0m gradients \u001b[38;5;241m=\u001b[39m tape\u001b[38;5;241m.\u001b[39mgradient(loss, model\u001b[38;5;241m.\u001b[39mtrainable_variables)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'keras._tf_keras.keras.losses' has no attribute 'mean_squared_error'"
     ]
    }
   ],
   "source": [
    "# Analyze each network\n",
    "networks = [\n",
    "    (sigmoid_model, \"Sigmoid Network (Problematic)\"),\n",
    "    (relu_model, \"ReLU Network (Healthy)\"),\n",
    "    (exploding_model, \"Exploding Network (Dangerous)\")\n",
    "]\n",
    "\n",
    "health_reports = {}\n",
    "\n",
    "for model, name in networks:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ANALYZING: {name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Monitor gradient health\n",
    "    health = monitor_gradient_health(model, X_sample, y_sample)\n",
    "    health_reports[name] = health\n",
    "    \n",
    "    # Interpret results\n",
    "    interpret_gradient_health(health)\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the comparison\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# Color scheme for different health levels\n",
    "def get_color(grad_norm):\n",
    "    if grad_norm < 1e-6:\n",
    "        return 'red'      # Vanished\n",
    "    elif grad_norm < 1e-4:\n",
    "        return 'orange'   # Weak\n",
    "    elif grad_norm > 10:\n",
    "        return 'purple'   # Exploding\n",
    "    else:\n",
    "        return 'green'    # Healthy\n",
    "\n",
    "network_names = list(health_reports.keys())\n",
    "\n",
    "# Plot gradient magnitudes for each network\n",
    "for i, (name, health) in enumerate(health_reports.items()):\n",
    "    # Top row: Gradient magnitudes\n",
    "    ax1 = axes[0, i]\n",
    "    grad_norms = health['gradient_norms']\n",
    "    layers = list(range(1, len(grad_norms) + 1))\n",
    "    colors = [get_color(g) for g in grad_norms]\n",
    "    \n",
    "    bars = ax1.bar(layers, grad_norms, color=colors, alpha=0.7)\n",
    "    ax1.set_yscale('log')\n",
    "    ax1.axhline(y=1e-6, color='red', linestyle='--', alpha=0.5, label='Vanished')\n",
    "    ax1.axhline(y=1e-4, color='orange', linestyle='--', alpha=0.5, label='Weak')\n",
    "    ax1.axhline(y=10, color='purple', linestyle='--', alpha=0.5, label='Exploding')\n",
    "    ax1.set_xlabel('Layer')\n",
    "    ax1.set_ylabel('Gradient Magnitude')\n",
    "    ax1.set_title(f'{name.split(\"(\")[0].strip()}\\nGradient Magnitudes')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Bottom row: Health metrics\n",
    "    ax2 = axes[1, i]\n",
    "    metrics = ['Min', 'Max', 'Mean', 'Std']\n",
    "    values = [health['min_gradient'], health['max_gradient'], \n",
    "              health['mean_gradient'], health['std_gradient']]\n",
    "    \n",
    "    bars2 = ax2.bar(metrics, values, color=['blue', 'red', 'green', 'orange'], alpha=0.7)\n",
    "    ax2.set_yscale('log')\n",
    "    ax2.set_ylabel('Gradient Value (log)')\n",
    "    ax2.set_title('Health Metrics Summary')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, value in zip(bars2, values):\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2, value, f'{value:.1e}', \n",
    "                ha='center', va='bottom', rotation=45, fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comprehensive comparison table\n",
    "print(\"üìä COMPREHENSIVE NETWORK COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Headers\n",
    "print(f\"{'Metric':<20} {'Sigmoid':<15} {'ReLU':<15} {'Exploding':<15} {'Ideal Range':<15}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Comparison data\n",
    "metrics_comparison = {\n",
    "    'Min Gradient': ('min_gradient', '1e-4 to 1e0'),\n",
    "    'Max Gradient': ('max_gradient', '1e-4 to 1e1'),\n",
    "    'Mean Gradient': ('mean_gradient', '1e-3 to 1e0'),\n",
    "    'Std Gradient': ('std_gradient', '< Mean'),\n",
    "    'Vanished Layers': ('vanished_layers', '0'),\n",
    "    'Weak Layers': ('weak_layers', '0-1'),\n",
    "    'Total Layers': ('total_layers', 'N/A')\n",
    "}\n",
    "\n",
    "for metric_name, (key, ideal) in metrics_comparison.items():\n",
    "    sigmoid_val = health_reports['Sigmoid Network (Problematic)'][key]\n",
    "    relu_val = health_reports['ReLU Network (Healthy)'][key]\n",
    "    exploding_val = health_reports['Exploding Network (Dangerous)'][key]\n",
    "    \n",
    "    if isinstance(sigmoid_val, float) and sigmoid_val < 1e-2:\n",
    "        sigmoid_str = f\"{sigmoid_val:.1e}\"\n",
    "        relu_str = f\"{relu_val:.1e}\"\n",
    "        exploding_str = f\"{exploding_val:.1e}\"\n",
    "    else:\n",
    "        sigmoid_str = f\"{sigmoid_val}\"\n",
    "        relu_str = f\"{relu_val}\"\n",
    "        exploding_str = f\"{exploding_val}\"\n",
    "    \n",
    "    print(f\"{metric_name:<20} {sigmoid_str:<15} {relu_str:<15} {exploding_str:<15} {ideal:<15}\")\n",
    "\n",
    "print(\"-\" * 80)\n",
    "print(\"\\nüéØ SUMMARY CONCLUSIONS:\")\n",
    "print(\"‚úÖ ReLU Network: Healthy gradient flow, good for training\")\n",
    "print(\"üö® Sigmoid Network: Severe vanishing gradients, poor training\")\n",
    "print(\"‚ö†Ô∏è Exploding Network: Unstable gradients, needs clipping\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîç Key Diagnostic Indicators\n",
    "\n",
    "### üü¢ Healthy Network Signs\n",
    "- **Gradient range:** 1e-4 to 1e0\n",
    "- **Vanished layers:** 0\n",
    "- **Weak layers:** 0-1\n",
    "- **Stability:** Consistent gradient magnitudes\n",
    "\n",
    "### üü° Warning Signs\n",
    "- **Gradient spread:** Very large range (>10,000x)\n",
    "- **Some weak layers:** 2-3 layers with small gradients\n",
    "- **High variance:** Unstable gradient magnitudes\n",
    "\n",
    "### üî¥ Critical Problems\n",
    "- **Vanished gradients:** Any layer < 1e-6\n",
    "- **Exploding gradients:** Any layer > 100\n",
    "- **Complete failure:** All gradients < 1e-5\n",
    "\n",
    "---\n",
    "\n",
    "## üí° Automated Monitoring Best Practices\n",
    "\n",
    "1. **Regular Checkups:** Monitor gradients every few epochs\n",
    "2. **Early Detection:** Catch problems before they worsen\n",
    "3. **Automated Alerts:** Set thresholds for automatic warnings\n",
    "4. **Historical Tracking:** Monitor trends over time\n",
    "5. **Multi-Metric Analysis:** Don't rely on single indicators\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Next Steps\n",
    "\n",
    "In the next notebook, we'll explore:\n",
    "- **Gradient explosion detection** techniques\n",
    "- **Automatic explosion prevention**\n",
    "- **Advanced monitoring strategies**\n",
    "\n",
    "---\n",
    "\n",
    "*This notebook demonstrates Concept 2 of Week 5: Deep Neural Network Architectures*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Week 5: Gradients (Python 3.10)",
   "language": "python",
   "name": "week5-gradients"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}