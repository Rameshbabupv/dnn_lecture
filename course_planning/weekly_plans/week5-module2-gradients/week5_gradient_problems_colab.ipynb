{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 5: Vanishing & Exploding Gradients - Interactive Notebook\n",
    "**Module 2: Optimization and Regularization**\n",
    "**Deep Neural Network Architectures (21CSE558T)**\n",
    "\n",
    "This notebook provides interactive demonstrations of gradient problems and their solutions.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/your-repo/blob/main/week5_gradient_problems_colab.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (if running on Colab)\n",
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "    print(\"Running on Google Colab\")\n",
    "    !pip install -q tensorflow matplotlib numpy scikit-learn seaborn\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better visualizations\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU Available: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Understanding Gradient Flow\n",
    "### 1.1 Visualizing the Chain Rule in Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_chain_rule():\n",
    "    \"\"\"Visualize how gradients flow through layers via chain rule\"\"\"\n",
    "    \n",
    "    # Create figure\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Simulate gradient flow through layers\n",
    "    num_layers = 10\n",
    "    \n",
    "    # Sigmoid activation gradients (max 0.25)\n",
    "    sigmoid_grads = [0.25 ** i for i in range(1, num_layers + 1)]\n",
    "    \n",
    "    # ReLU activation gradients (can be 0 or 1)\n",
    "    relu_grads = [1.0 * (0.9 ** i) for i in range(1, num_layers + 1)]  # Slight decay\n",
    "    \n",
    "    # Plot 1: Gradient magnitude through layers\n",
    "    layers = list(range(1, num_layers + 1))\n",
    "    ax1.plot(layers, sigmoid_grads, 'r-o', label='Sigmoid', linewidth=2, markersize=8)\n",
    "    ax1.plot(layers, relu_grads, 'b-s', label='ReLU', linewidth=2, markersize=8)\n",
    "    ax1.set_yscale('log')\n",
    "    ax1.set_xlabel('Layer Depth', fontsize=12)\n",
    "    ax1.set_ylabel('Gradient Magnitude (log scale)', fontsize=12)\n",
    "    ax1.set_title('Gradient Flow Through Deep Networks', fontsize=14, fontweight='bold')\n",
    "    ax1.legend(fontsize=11)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add annotations\n",
    "    ax1.annotate('Vanishing!', xy=(8, sigmoid_grads[7]), xytext=(6, 1e-6),\n",
    "                arrowprops=dict(arrowstyle='->', color='red', lw=2),\n",
    "                fontsize=11, color='red', fontweight='bold')\n",
    "    \n",
    "    # Plot 2: Cumulative gradient decay\n",
    "    sigmoid_cumulative = np.cumprod([0.25] * num_layers)\n",
    "    relu_cumulative = np.cumprod([0.9] * num_layers)\n",
    "    \n",
    "    ax2.fill_between(layers, 0, sigmoid_cumulative, alpha=0.3, color='red', label='Sigmoid Area')\n",
    "    ax2.fill_between(layers, 0, relu_cumulative, alpha=0.3, color='blue', label='ReLU Area')\n",
    "    ax2.set_yscale('log')\n",
    "    ax2.set_xlabel('Layer Depth', fontsize=12)\n",
    "    ax2.set_ylabel('Cumulative Gradient Product (log scale)', fontsize=12)\n",
    "    ax2.set_title('Cumulative Gradient Decay', fontsize=14, fontweight='bold')\n",
    "    ax2.legend(fontsize=11)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print analysis\n",
    "    print(\"\\nðŸ“Š GRADIENT FLOW ANALYSIS:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"After {num_layers} layers:\")\n",
    "    print(f\"  Sigmoid gradient: {sigmoid_grads[-1]:.2e}\")\n",
    "    print(f\"  ReLU gradient: {relu_grads[-1]:.4f}\")\n",
    "    print(f\"  Ratio (ReLU/Sigmoid): {relu_grads[-1]/sigmoid_grads[-1]:.0f}x better\")\n",
    "\n",
    "visualize_chain_rule()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Vanishing Gradients Problem\n",
    "### 2.1 Create and Analyze a Deep Network with Sigmoid Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class GradientAnalyzer:\n    \"\"\"Tool for analyzing gradient flow in neural networks\"\"\"\n    \n    def __init__(self):\n        self.gradient_history = []\n        self.loss_history = []\n        self.model_counter = 0  # Add counter for unique model names\n    \n    def create_deep_network(self, depth=10, activation='sigmoid', width=64):\n        \"\"\"Create a deep neural network with unique layer names\"\"\"\n        # Clear any existing models to avoid naming conflicts\n        tf.keras.backend.clear_session()\n        \n        # Generate unique suffix for this model\n        self.model_counter += 1\n        suffix = f\"_{activation}_{self.model_counter}\"\n        \n        model = tf.keras.Sequential()\n        \n        # Input layer with unique name\n        model.add(tf.keras.layers.Dense(width, activation=activation, \n                                       input_shape=(10,),\n                                       name=f'input_layer{suffix}'))\n        \n        # Hidden layers with unique names\n        for i in range(depth - 2):\n            model.add(tf.keras.layers.Dense(width, activation=activation,\n                                           name=f'hidden_{i+1}{suffix}'))\n        \n        # Output layer with unique name\n        model.add(tf.keras.layers.Dense(1, activation='sigmoid',\n                                       name=f'output_layer{suffix}'))\n        \n        return model\n    \n    def analyze_gradients(self, model, X, y):\n        \"\"\"Analyze gradient magnitudes for each layer\"\"\"\n        with tf.GradientTape() as tape:\n            y_pred = model(X, training=True)\n            loss = tf.keras.losses.binary_crossentropy(y, y_pred)\n            loss = tf.reduce_mean(loss)\n        \n        # Get gradients\n        gradients = tape.gradient(loss, model.trainable_variables)\n        \n        # Calculate gradient statistics\n        gradient_stats = []\n        for i, (grad, weight) in enumerate(zip(gradients, model.trainable_variables)):\n            if 'kernel' in weight.name:  # Only analyze weight gradients\n                grad_norm = tf.norm(grad).numpy()\n                grad_mean = tf.reduce_mean(tf.abs(grad)).numpy()\n                grad_std = tf.math.reduce_std(grad).numpy()\n                \n                layer_name = weight.name.split('/')[0]\n                gradient_stats.append({\n                    'layer': layer_name,\n                    'norm': grad_norm,\n                    'mean': grad_mean,\n                    'std': grad_std,\n                    'shape': grad.shape\n                })\n        \n        return gradient_stats, loss.numpy()\n\n# Create analyzer\nanalyzer = GradientAnalyzer()\n\n# Generate sample data\nX_sample = tf.random.normal((100, 10))\ny_sample = tf.random.uniform((100, 1))\n\nprint(\"ðŸ” Analyzing gradient flow in deep networks...\\n\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Visualize Vanishing Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def visualize_vanishing_gradients():\n    \"\"\"Compare gradient flow in networks with different activations\"\"\"\n    \n    # Create networks\n    sigmoid_model = analyzer.create_deep_network(depth=10, activation='sigmoid')\n    tanh_model = analyzer.create_deep_network(depth=10, activation='tanh')\n    relu_model = analyzer.create_deep_network(depth=10, activation='relu')\n    \n    # Analyze gradients\n    sigmoid_stats, _ = analyzer.analyze_gradients(sigmoid_model, X_sample, y_sample)\n    tanh_stats, _ = analyzer.analyze_gradients(tanh_model, X_sample, y_sample)\n    relu_stats, _ = analyzer.analyze_gradients(relu_model, X_sample, y_sample)\n    \n    # Create visualization\n    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n    \n    # Plot 1: Gradient norms comparison\n    ax = axes[0, 0]\n    layer_indices = range(len(sigmoid_stats))\n    \n    sigmoid_norms = [s['norm'] for s in sigmoid_stats]\n    tanh_norms = [s['norm'] for s in tanh_stats]\n    relu_norms = [s['norm'] for s in relu_stats]\n    \n    ax.semilogy(layer_indices, sigmoid_norms, 'r-o', label='Sigmoid', linewidth=2, markersize=8)\n    ax.semilogy(layer_indices, tanh_norms, 'g-^', label='Tanh', linewidth=2, markersize=8)\n    ax.semilogy(layer_indices, relu_norms, 'b-s', label='ReLU', linewidth=2, markersize=8)\n    \n    ax.set_xlabel('Layer Index (Input â†’ Output)', fontsize=12)\n    ax.set_ylabel('Gradient Norm (log scale)', fontsize=12)\n    ax.set_title('Gradient Norm by Layer', fontsize=14, fontweight='bold')\n    ax.legend(fontsize=11)\n    ax.grid(True, alpha=0.3)\n    \n    # Plot 2: Gradient mean absolute values\n    ax = axes[0, 1]\n    sigmoid_means = [s['mean'] for s in sigmoid_stats]\n    tanh_means = [s['mean'] for s in tanh_stats]\n    relu_means = [s['mean'] for s in relu_stats]\n    \n    x = np.arange(len(sigmoid_stats))\n    width = 0.25\n    \n    ax.bar(x - width, sigmoid_means, width, label='Sigmoid', color='red', alpha=0.7)\n    ax.bar(x, tanh_means, width, label='Tanh', color='green', alpha=0.7)\n    ax.bar(x + width, relu_means, width, label='ReLU', color='blue', alpha=0.7)\n    \n    ax.set_xlabel('Layer Index', fontsize=12)\n    ax.set_ylabel('Mean Absolute Gradient', fontsize=12)\n    ax.set_title('Mean Gradient Magnitude by Layer', fontsize=14, fontweight='bold')\n    ax.legend(fontsize=11)\n    ax.set_yscale('log')\n    \n    # Plot 3: Gradient distribution heatmap for Sigmoid (FIXED)\n    ax = axes[1, 0]\n    \n    # Collect gradient values for each layer with consistent shape\n    gradient_samples = []\n    sample_size = 100  # Fixed sample size for all layers\n    \n    for var in sigmoid_model.trainable_variables:\n        if 'kernel' in var.name:\n            with tf.GradientTape() as tape:\n                y_pred = sigmoid_model(X_sample[:10], training=True)\n                loss = tf.reduce_mean(tf.keras.losses.binary_crossentropy(y_sample[:10], y_pred))\n            grads = tape.gradient(loss, var)\n            \n            # Flatten and take exactly sample_size values\n            flat_grads = tf.reshape(grads, [-1]).numpy()\n            \n            # Ensure we have exactly sample_size values (pad with zeros if needed)\n            if len(flat_grads) >= sample_size:\n                sampled_grads = flat_grads[:sample_size]\n            else:\n                # Pad with zeros if we have fewer gradients\n                sampled_grads = np.pad(flat_grads, (0, sample_size - len(flat_grads)), 'constant')\n            \n            gradient_samples.append(sampled_grads)\n    \n    # Create heatmap with consistent shape\n    if gradient_samples:\n        gradient_matrix = np.array(gradient_samples)\n        im = ax.imshow(gradient_matrix, aspect='auto', cmap='coolwarm', \n                       vmin=-0.01, vmax=0.01)\n        ax.set_xlabel('Gradient Values (sampled)', fontsize=12)\n        ax.set_ylabel('Layer Index', fontsize=12)\n        ax.set_title('Gradient Distribution - Sigmoid Network', fontsize=14, fontweight='bold')\n        plt.colorbar(im, ax=ax)\n    else:\n        ax.text(0.5, 0.5, 'No gradient data available', ha='center', va='center')\n        ax.set_title('Gradient Distribution - Sigmoid Network', fontsize=14, fontweight='bold')\n    \n    # Plot 4: Gradient ratio (last/first layer)\n    ax = axes[1, 1]\n    \n    # Ensure we have gradients to calculate ratios\n    if sigmoid_norms and tanh_norms and relu_norms:\n        ratios = {\n            'Sigmoid': sigmoid_norms[-1] / sigmoid_norms[0] if sigmoid_norms[0] != 0 else 0,\n            'Tanh': tanh_norms[-1] / tanh_norms[0] if tanh_norms[0] != 0 else 0,\n            'ReLU': relu_norms[-1] / relu_norms[0] if relu_norms[0] != 0 else 0\n        }\n        \n        colors = ['red', 'green', 'blue']\n        bars = ax.bar(ratios.keys(), ratios.values(), color=colors, alpha=0.7)\n        ax.set_ylabel('Gradient Ratio (Last Layer / First Layer)', fontsize=12)\n        ax.set_title('Gradient Decay Across Network', fontsize=14, fontweight='bold')\n        ax.set_yscale('log')\n        \n        # Add value labels on bars\n        for bar, (name, value) in zip(bars, ratios.items()):\n            if value > 0:  # Only add label if value is positive\n                height = bar.get_height()\n                ax.text(bar.get_x() + bar.get_width()/2., height,\n                        f'{value:.2e}', ha='center', va='bottom', fontsize=10)\n    else:\n        ax.text(0.5, 0.5, 'Insufficient data for ratios', ha='center', va='center')\n        ax.set_title('Gradient Decay Across Network', fontsize=14, fontweight='bold')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # Print detailed analysis\n    print(\"\\nðŸ“ˆ VANISHING GRADIENT ANALYSIS:\")\n    print(\"=\" * 60)\n    \n    if sigmoid_norms and tanh_norms and relu_norms:\n        print(f\"{'Activation':<12} {'First Layer Norm':<20} {'Last Layer Norm':<20} {'Ratio':<15}\")\n        print(\"-\" * 60)\n        print(f\"{'Sigmoid':<12} {sigmoid_norms[0]:<20.6f} {sigmoid_norms[-1]:<20.6e} {sigmoid_norms[-1]/sigmoid_norms[0] if sigmoid_norms[0] != 0 else 0:<15.2e}\")\n        print(f\"{'Tanh':<12} {tanh_norms[0]:<20.6f} {tanh_norms[-1]:<20.6e} {tanh_norms[-1]/tanh_norms[0] if tanh_norms[0] != 0 else 0:<15.2e}\")\n        print(f\"{'ReLU':<12} {relu_norms[0]:<20.6f} {relu_norms[-1]:<20.6e} {relu_norms[-1]/relu_norms[0] if relu_norms[0] != 0 else 0:<15.2e}\")\n    else:\n        print(\"Insufficient gradient data for analysis\")\n\nvisualize_vanishing_gradients()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Exploding Gradients Problem\n",
    "### 3.1 Detection and Visualization of Gradient Explosion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientExplosionDetector:\n",
    "    \"\"\"Detect and visualize gradient explosion in neural networks\"\"\"\n",
    "    \n",
    "    def __init__(self, explosion_threshold=100.0):\n",
    "        self.explosion_threshold = explosion_threshold\n",
    "        self.gradient_history = []\n",
    "        self.explosion_events = []\n",
    "    \n",
    "    def create_unstable_network(self):\n",
    "        \"\"\"Create a network prone to gradient explosion\"\"\"\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(64, activation='linear',\n",
    "                                kernel_initializer=tf.keras.initializers.RandomNormal(stddev=2.0),\n",
    "                                input_shape=(10,)),\n",
    "            tf.keras.layers.Dense(64, activation='relu',\n",
    "                                kernel_initializer=tf.keras.initializers.RandomNormal(stddev=2.0)),\n",
    "            tf.keras.layers.Dense(64, activation='relu',\n",
    "                                kernel_initializer=tf.keras.initializers.RandomNormal(stddev=2.0)),\n",
    "            tf.keras.layers.Dense(1)\n",
    "        ])\n",
    "        return model\n",
    "    \n",
    "    def detect_explosion(self, model, X, y, epochs=20):\n",
    "        \"\"\"Train model and detect gradient explosions\"\"\"\n",
    "        optimizer = tf.keras.optimizers.SGD(learning_rate=0.1)  # High learning rate\n",
    "        \n",
    "        self.gradient_history = []\n",
    "        self.explosion_events = []\n",
    "        loss_history = []\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            with tf.GradientTape() as tape:\n",
    "                y_pred = model(X, training=True)\n",
    "                loss = tf.reduce_mean(tf.square(y_pred - y))\n",
    "            \n",
    "            # Calculate gradients\n",
    "            gradients = tape.gradient(loss, model.trainable_variables)\n",
    "            \n",
    "            # Calculate total gradient norm\n",
    "            total_norm = tf.reduce_sum([tf.norm(g) for g in gradients if g is not None]).numpy()\n",
    "            \n",
    "            # Store history\n",
    "            self.gradient_history.append(total_norm)\n",
    "            loss_history.append(loss.numpy())\n",
    "            \n",
    "            # Detect explosion\n",
    "            if total_norm > self.explosion_threshold:\n",
    "                self.explosion_events.append(epoch)\n",
    "                print(f\"âš ï¸ EXPLOSION DETECTED at epoch {epoch}: Gradient norm = {total_norm:.2f}\")\n",
    "            \n",
    "            # Check for NaN\n",
    "            if np.isnan(total_norm) or np.isinf(total_norm):\n",
    "                print(f\"ðŸ’¥ CRITICAL: NaN/Inf detected at epoch {epoch}!\")\n",
    "                break\n",
    "            \n",
    "            # Apply gradients (this might cause issues!)\n",
    "            try:\n",
    "                optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "            except:\n",
    "                print(f\"âŒ Training failed at epoch {epoch}\")\n",
    "                break\n",
    "        \n",
    "        return self.gradient_history, loss_history\n",
    "    \n",
    "    def visualize_explosion(self):\n",
    "        \"\"\"Visualize gradient explosion detection results\"\"\"\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        epochs = range(len(self.gradient_history))\n",
    "        \n",
    "        # Plot 1: Gradient norm over time\n",
    "        ax = axes[0, 0]\n",
    "        ax.plot(epochs, self.gradient_history, 'b-', linewidth=2, label='Gradient Norm')\n",
    "        ax.axhline(y=self.explosion_threshold, color='r', linestyle='--', \n",
    "                  linewidth=2, label=f'Explosion Threshold ({self.explosion_threshold})')\n",
    "        \n",
    "        # Mark explosion events\n",
    "        for event in self.explosion_events:\n",
    "            ax.scatter(event, self.gradient_history[event], color='red', s=100, \n",
    "                      marker='x', linewidths=3, zorder=5)\n",
    "        \n",
    "        ax.set_xlabel('Epoch', fontsize=12)\n",
    "        ax.set_ylabel('Total Gradient Norm', fontsize=12)\n",
    "        ax.set_title('Gradient Explosion Detection', fontsize=14, fontweight='bold')\n",
    "        ax.legend(fontsize=11)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 2: Log scale gradient norm\n",
    "        ax = axes[0, 1]\n",
    "        ax.semilogy(epochs, self.gradient_history, 'g-', linewidth=2)\n",
    "        ax.axhline(y=self.explosion_threshold, color='r', linestyle='--', linewidth=2)\n",
    "        ax.set_xlabel('Epoch', fontsize=12)\n",
    "        ax.set_ylabel('Total Gradient Norm (log scale)', fontsize=12)\n",
    "        ax.set_title('Gradient Norm - Log Scale View', fontsize=14, fontweight='bold')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 3: Gradient distribution histogram\n",
    "        ax = axes[1, 0]\n",
    "        ax.hist(self.gradient_history, bins=30, color='purple', alpha=0.7, edgecolor='black')\n",
    "        ax.axvline(x=self.explosion_threshold, color='r', linestyle='--', linewidth=2)\n",
    "        ax.set_xlabel('Gradient Norm', fontsize=12)\n",
    "        ax.set_ylabel('Frequency', fontsize=12)\n",
    "        ax.set_title('Distribution of Gradient Norms', fontsize=14, fontweight='bold')\n",
    "        \n",
    "        # Plot 4: Explosion event analysis\n",
    "        ax = axes[1, 1]\n",
    "        if self.explosion_events:\n",
    "            explosion_magnitudes = [self.gradient_history[e] for e in self.explosion_events]\n",
    "            ax.bar(range(len(self.explosion_events)), explosion_magnitudes, \n",
    "                  color='red', alpha=0.7)\n",
    "            ax.set_xlabel('Explosion Event Index', fontsize=12)\n",
    "            ax.set_ylabel('Gradient Magnitude', fontsize=12)\n",
    "            ax.set_title(f'Explosion Events ({len(self.explosion_events)} detected)', \n",
    "                        fontsize=14, fontweight='bold')\n",
    "        else:\n",
    "            ax.text(0.5, 0.5, 'No explosions detected', \n",
    "                   ha='center', va='center', fontsize=14)\n",
    "            ax.set_title('Explosion Events', fontsize=14, fontweight='bold')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Demonstrate gradient explosion\n",
    "print(\"ðŸ”¥ Demonstrating Gradient Explosion...\\n\")\n",
    "\n",
    "detector = GradientExplosionDetector(explosion_threshold=50.0)\n",
    "\n",
    "# Create unstable network\n",
    "unstable_model = detector.create_unstable_network()\n",
    "\n",
    "# Generate data\n",
    "X_train = tf.random.normal((100, 10))\n",
    "y_train = tf.random.normal((100, 1))\n",
    "\n",
    "# Detect explosions\n",
    "grad_history, loss_history = detector.detect_explosion(unstable_model, X_train, y_train, epochs=20)\n",
    "\n",
    "# Visualize results\n",
    "detector.visualize_explosion()\n",
    "\n",
    "# Print summary\n",
    "print(\"\\nðŸ“Š EXPLOSION DETECTION SUMMARY:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Total epochs: {len(grad_history)}\")\n",
    "print(f\"Explosions detected: {len(detector.explosion_events)}\")\n",
    "if detector.explosion_events:\n",
    "    print(f\"First explosion at epoch: {detector.explosion_events[0]}\")\n",
    "    print(f\"Max gradient norm: {max(grad_history):.2f}\")\n",
    "print(f\"Mean gradient norm: {np.mean(grad_history):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Gradient Clipping Solution\n",
    "### 4.1 Implement and Visualize Gradient Clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_gradient_clipping():\n",
    "    \"\"\"Compare training with and without gradient clipping\"\"\"\n",
    "    \n",
    "    # Create two identical unstable models\n",
    "    tf.random.set_seed(42)\n",
    "    model_no_clip = detector.create_unstable_network()\n",
    "    \n",
    "    tf.random.set_seed(42)\n",
    "    model_with_clip = detector.create_unstable_network()\n",
    "    \n",
    "    # Training setup\n",
    "    optimizer_no_clip = tf.keras.optimizers.SGD(learning_rate=0.1)\n",
    "    optimizer_with_clip = tf.keras.optimizers.SGD(learning_rate=0.1, clipnorm=1.0)\n",
    "    \n",
    "    # Training data\n",
    "    X = tf.random.normal((100, 10))\n",
    "    y = tf.random.normal((100, 1))\n",
    "    \n",
    "    # Training history\n",
    "    history_no_clip = {'gradients': [], 'loss': []}\n",
    "    history_with_clip = {'gradients': [], 'loss': []}\n",
    "    \n",
    "    epochs = 30\n",
    "    \n",
    "    print(\"Training WITHOUT gradient clipping...\")\n",
    "    for epoch in range(epochs):\n",
    "        # No clipping\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model_no_clip(X, training=True)\n",
    "            loss = tf.reduce_mean(tf.square(y_pred - y))\n",
    "        \n",
    "        grads = tape.gradient(loss, model_no_clip.trainable_variables)\n",
    "        grad_norm = tf.reduce_sum([tf.norm(g) for g in grads if g is not None]).numpy()\n",
    "        \n",
    "        history_no_clip['gradients'].append(grad_norm)\n",
    "        history_no_clip['loss'].append(loss.numpy())\n",
    "        \n",
    "        if not np.isnan(grad_norm) and not np.isinf(grad_norm):\n",
    "            optimizer_no_clip.apply_gradients(zip(grads, model_no_clip.trainable_variables))\n",
    "        else:\n",
    "            print(f\"  âŒ NaN/Inf at epoch {epoch}\")\n",
    "            break\n",
    "    \n",
    "    print(\"\\nTraining WITH gradient clipping (max_norm=1.0)...\")\n",
    "    for epoch in range(epochs):\n",
    "        # With clipping\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model_with_clip(X, training=True)\n",
    "            loss = tf.reduce_mean(tf.square(y_pred - y))\n",
    "        \n",
    "        grads = tape.gradient(loss, model_with_clip.trainable_variables)\n",
    "        \n",
    "        # Manual clipping for visualization\n",
    "        clipped_grads = []\n",
    "        for g in grads:\n",
    "            if g is not None:\n",
    "                clipped_grads.append(tf.clip_by_norm(g, 1.0))\n",
    "            else:\n",
    "                clipped_grads.append(g)\n",
    "        \n",
    "        grad_norm = tf.reduce_sum([tf.norm(g) for g in clipped_grads if g is not None]).numpy()\n",
    "        \n",
    "        history_with_clip['gradients'].append(grad_norm)\n",
    "        history_with_clip['loss'].append(loss.numpy())\n",
    "        \n",
    "        optimizer_with_clip.apply_gradients(zip(clipped_grads, model_with_clip.trainable_variables))\n",
    "    \n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Plot 1: Gradient norms comparison\n",
    "    ax = axes[0, 0]\n",
    "    ax.plot(history_no_clip['gradients'], 'r-', label='No Clipping', linewidth=2, alpha=0.7)\n",
    "    ax.plot(history_with_clip['gradients'], 'b-', label='With Clipping', linewidth=2)\n",
    "    ax.set_xlabel('Epoch', fontsize=12)\n",
    "    ax.set_ylabel('Gradient Norm', fontsize=12)\n",
    "    ax.set_title('Gradient Norm Comparison', fontsize=14, fontweight='bold')\n",
    "    ax.legend(fontsize=11)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Loss comparison\n",
    "    ax = axes[0, 1]\n",
    "    ax.plot(history_no_clip['loss'], 'r-', label='No Clipping', linewidth=2, alpha=0.7)\n",
    "    ax.plot(history_with_clip['loss'], 'b-', label='With Clipping', linewidth=2)\n",
    "    ax.set_xlabel('Epoch', fontsize=12)\n",
    "    ax.set_ylabel('Loss', fontsize=12)\n",
    "    ax.set_title('Loss Comparison', fontsize=14, fontweight='bold')\n",
    "    ax.legend(fontsize=11)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Gradient stability (variance)\n",
    "    ax = axes[1, 0]\n",
    "    window = 5\n",
    "    no_clip_var = [np.var(history_no_clip['gradients'][max(0,i-window):i+1]) \n",
    "                   for i in range(len(history_no_clip['gradients']))]\n",
    "    clip_var = [np.var(history_with_clip['gradients'][max(0,i-window):i+1]) \n",
    "                for i in range(len(history_with_clip['gradients']))]\n",
    "    \n",
    "    ax.plot(no_clip_var, 'r-', label='No Clipping', linewidth=2, alpha=0.7)\n",
    "    ax.plot(clip_var, 'b-', label='With Clipping', linewidth=2)\n",
    "    ax.set_xlabel('Epoch', fontsize=12)\n",
    "    ax.set_ylabel('Gradient Variance (5-epoch window)', fontsize=12)\n",
    "    ax.set_title('Training Stability', fontsize=14, fontweight='bold')\n",
    "    ax.legend(fontsize=11)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 4: Clipping effect visualization\n",
    "    ax = axes[1, 1]\n",
    "    clipping_effect = [min(g, 1.0) / g if g > 0 else 1.0 \n",
    "                      for g in history_no_clip['gradients']]\n",
    "    ax.fill_between(range(len(clipping_effect)), 0, clipping_effect, \n",
    "                    alpha=0.5, color='green')\n",
    "    ax.axhline(y=1.0, color='r', linestyle='--', linewidth=2)\n",
    "    ax.set_xlabel('Epoch', fontsize=12)\n",
    "    ax.set_ylabel('Clipping Factor (1.0 = no clipping)', fontsize=12)\n",
    "    ax.set_title('Gradient Clipping Effect', fontsize=14, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print analysis\n",
    "    print(\"\\nðŸ“Š GRADIENT CLIPPING ANALYSIS:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"{'Metric':<30} {'No Clipping':<15} {'With Clipping':<15}\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"{'Max Gradient Norm':<30} {max(history_no_clip['gradients']):<15.2f} {max(history_with_clip['gradients']):<15.2f}\")\n",
    "    print(f\"{'Mean Gradient Norm':<30} {np.mean(history_no_clip['gradients']):<15.2f} {np.mean(history_with_clip['gradients']):<15.2f}\")\n",
    "    print(f\"{'Gradient Std Dev':<30} {np.std(history_no_clip['gradients']):<15.2f} {np.std(history_with_clip['gradients']):<15.2f}\")\n",
    "    print(f\"{'Final Loss':<30} {history_no_clip['loss'][-1]:<15.4f} {history_with_clip['loss'][-1]:<15.4f}\")\n",
    "\n",
    "demonstrate_gradient_clipping()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Solutions Summary\n",
    "### 5.1 Comprehensive Solutions for Gradient Problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_solution_comparison():\n",
    "    \"\"\"Compare different solutions for gradient problems\"\"\"\n",
    "    \n",
    "    # Generate dataset\n",
    "    X, y = make_classification(n_samples=1000, n_features=20, n_informative=15,\n",
    "                              n_redundant=5, random_state=42)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Define different model configurations\n",
    "    models = {\n",
    "        'Problematic (Sigmoid)': {\n",
    "            'model': tf.keras.Sequential([\n",
    "                tf.keras.layers.Dense(128, activation='sigmoid', input_shape=(20,)),\n",
    "                tf.keras.layers.Dense(64, activation='sigmoid'),\n",
    "                tf.keras.layers.Dense(32, activation='sigmoid'),\n",
    "                tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "            ]),\n",
    "            'optimizer': tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "        },\n",
    "        'ReLU + He Init': {\n",
    "            'model': tf.keras.Sequential([\n",
    "                tf.keras.layers.Dense(128, activation='relu', \n",
    "                                    kernel_initializer='he_normal', input_shape=(20,)),\n",
    "                tf.keras.layers.Dense(64, activation='relu', \n",
    "                                    kernel_initializer='he_normal'),\n",
    "                tf.keras.layers.Dense(32, activation='relu', \n",
    "                                    kernel_initializer='he_normal'),\n",
    "                tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "            ]),\n",
    "            'optimizer': tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "        },\n",
    "        'BatchNorm + Dropout': {\n",
    "            'model': tf.keras.Sequential([\n",
    "                tf.keras.layers.Dense(128, activation='relu', input_shape=(20,)),\n",
    "                tf.keras.layers.BatchNormalization(),\n",
    "                tf.keras.layers.Dropout(0.3),\n",
    "                tf.keras.layers.Dense(64, activation='relu'),\n",
    "                tf.keras.layers.BatchNormalization(),\n",
    "                tf.keras.layers.Dropout(0.2),\n",
    "                tf.keras.layers.Dense(32, activation='relu'),\n",
    "                tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "            ]),\n",
    "            'optimizer': tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "        },\n",
    "        'Complete Solution': {\n",
    "            'model': tf.keras.Sequential([\n",
    "                tf.keras.layers.Dense(128, activation='relu', \n",
    "                                    kernel_initializer='he_normal', input_shape=(20,)),\n",
    "                tf.keras.layers.BatchNormalization(),\n",
    "                tf.keras.layers.Dropout(0.3),\n",
    "                tf.keras.layers.Dense(64, activation='relu', \n",
    "                                    kernel_initializer='he_normal'),\n",
    "                tf.keras.layers.BatchNormalization(),\n",
    "                tf.keras.layers.Dropout(0.2),\n",
    "                tf.keras.layers.Dense(32, activation='relu'),\n",
    "                tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "            ]),\n",
    "            'optimizer': tf.keras.optimizers.Adam(learning_rate=0.001, clipnorm=1.0)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Train and evaluate each model\n",
    "    results = {}\n",
    "    histories = {}\n",
    "    \n",
    "    for name, config in models.items():\n",
    "        print(f\"\\nTraining {name}...\")\n",
    "        model = config['model']\n",
    "        model.compile(optimizer=config['optimizer'],\n",
    "                     loss='binary_crossentropy',\n",
    "                     metrics=['accuracy'])\n",
    "        \n",
    "        history = model.fit(X_train, y_train,\n",
    "                          validation_split=0.2,\n",
    "                          epochs=50,\n",
    "                          batch_size=32,\n",
    "                          verbose=0)\n",
    "        \n",
    "        test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "        \n",
    "        results[name] = {\n",
    "            'test_loss': test_loss,\n",
    "            'test_accuracy': test_acc,\n",
    "            'final_train_loss': history.history['loss'][-1],\n",
    "            'final_val_loss': history.history['val_loss'][-1]\n",
    "        }\n",
    "        histories[name] = history.history\n",
    "        \n",
    "        print(f\"  Test Accuracy: {test_acc:.4f}\")\n",
    "    \n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    colors = ['red', 'blue', 'green', 'purple']\n",
    "    \n",
    "    # Plot training loss\n",
    "    ax = axes[0, 0]\n",
    "    for (name, history), color in zip(histories.items(), colors):\n",
    "        ax.plot(history['loss'], label=name, color=color, linewidth=2, alpha=0.8)\n",
    "    ax.set_xlabel('Epoch', fontsize=12)\n",
    "    ax.set_ylabel('Training Loss', fontsize=12)\n",
    "    ax.set_title('Training Loss Comparison', fontsize=14, fontweight='bold')\n",
    "    ax.legend(fontsize=9)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot validation loss\n",
    "    ax = axes[0, 1]\n",
    "    for (name, history), color in zip(histories.items(), colors):\n",
    "        ax.plot(history['val_loss'], label=name, color=color, linewidth=2, alpha=0.8)\n",
    "    ax.set_xlabel('Epoch', fontsize=12)\n",
    "    ax.set_ylabel('Validation Loss', fontsize=12)\n",
    "    ax.set_title('Validation Loss Comparison', fontsize=14, fontweight='bold')\n",
    "    ax.legend(fontsize=9)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot test accuracy comparison\n",
    "    ax = axes[1, 0]\n",
    "    names = list(results.keys())\n",
    "    test_accs = [results[n]['test_accuracy'] for n in names]\n",
    "    bars = ax.bar(range(len(names)), test_accs, color=colors, alpha=0.7)\n",
    "    ax.set_xticks(range(len(names)))\n",
    "    ax.set_xticklabels(names, rotation=45, ha='right')\n",
    "    ax.set_ylabel('Test Accuracy', fontsize=12)\n",
    "    ax.set_title('Final Test Accuracy', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, acc in zip(bars, test_accs):\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.01,\n",
    "               f'{acc:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    # Plot convergence speed\n",
    "    ax = axes[1, 1]\n",
    "    for (name, history), color in zip(histories.items(), colors):\n",
    "        # Find epoch where validation loss stabilizes\n",
    "        val_losses = history['val_loss']\n",
    "        convergence_epoch = None\n",
    "        for i in range(10, len(val_losses)):\n",
    "            if np.std(val_losses[i-5:i]) < 0.01:\n",
    "                convergence_epoch = i\n",
    "                break\n",
    "        \n",
    "        if convergence_epoch:\n",
    "            ax.bar(name, convergence_epoch, color=color, alpha=0.7)\n",
    "    \n",
    "    ax.set_ylabel('Epochs to Convergence', fontsize=12)\n",
    "    ax.set_title('Convergence Speed', fontsize=14, fontweight='bold')\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary table\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SOLUTION COMPARISON SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"{'Model':<25} {'Test Loss':<12} {'Test Acc':<12} {'Train-Val Gap':<15}\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    for name, res in results.items():\n",
    "        gap = res['final_val_loss'] - res['final_train_loss']\n",
    "        print(f\"{name:<25} {res['test_loss']:<12.4f} {res['test_accuracy']:<12.4f} {gap:<15.4f}\")\n",
    "\n",
    "create_solution_comparison()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Interactive Exercise\n",
    "### Build Your Own Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Students implement their own solution\n",
    "def student_solution():\n",
    "    \"\"\"\n",
    "    Exercise: Create a neural network that addresses gradient problems\n",
    "    \n",
    "    Requirements:\n",
    "    1. Use appropriate activation functions\n",
    "    2. Implement proper weight initialization\n",
    "    3. Add regularization (BatchNorm or Dropout)\n",
    "    4. Use gradient clipping if needed\n",
    "    5. Choose an appropriate optimizer\n",
    "    \"\"\"\n",
    "    \n",
    "    # Your implementation here\n",
    "    model = tf.keras.Sequential([\n",
    "        # TODO: Add layers with proper configuration\n",
    "        tf.keras.layers.Dense(128, activation='relu', \n",
    "                            kernel_initializer='he_normal',\n",
    "                            input_shape=(20,)),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        \n",
    "        tf.keras.layers.Dense(64, activation='relu',\n",
    "                            kernel_initializer='he_normal'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        \n",
    "        tf.keras.layers.Dense(32, activation='relu'),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    # TODO: Choose optimizer with appropriate settings\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001, clipnorm=1.0)\n",
    "    \n",
    "    model.compile(optimizer=optimizer,\n",
    "                 loss='binary_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Test student solution\n",
    "print(\"Testing student solution...\")\n",
    "student_model = student_solution()\n",
    "print(\"\\nModel architecture:\")\n",
    "student_model.summary()\n",
    "\n",
    "print(\"\\nâœ… Solution created successfully!\")\n",
    "print(\"\\nKey features of a good solution:\")\n",
    "print(\"- ReLU activation (avoids vanishing gradients)\")\n",
    "print(\"- He initialization (proper weight scaling)\")\n",
    "print(\"- Batch normalization (stabilizes training)\")\n",
    "print(\"- Dropout (prevents overfitting)\")\n",
    "print(\"- Adam optimizer with gradient clipping\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Key Takeaways\n",
    "\n",
    "### ðŸŽ¯ What We Detected:\n",
    "1. **Vanishing Gradients**: Gradient norms approaching zero in deep sigmoid networks\n",
    "2. **Exploding Gradients**: Gradient norms exceeding safe thresholds\n",
    "3. **Training Instability**: High variance in gradient norms\n",
    "4. **Convergence Issues**: Slow or failed convergence\n",
    "\n",
    "### ðŸ“Š What We Visualized:\n",
    "1. **Gradient Flow**: Layer-wise gradient magnitude progression\n",
    "2. **Explosion Events**: Epochs where gradients exceeded thresholds\n",
    "3. **Clipping Effects**: Comparison of clipped vs unclipped gradients\n",
    "4. **Solution Effectiveness**: Performance comparison of different techniques\n",
    "\n",
    "### âœ… Solutions Implemented:\n",
    "1. **ReLU Activation**: Maintains gradient flow\n",
    "2. **He/Xavier Initialization**: Proper weight scaling\n",
    "3. **Batch Normalization**: Stabilizes distributions\n",
    "4. **Gradient Clipping**: Prevents explosion\n",
    "5. **Dropout**: Regularization\n",
    "6. **Adaptive Optimizers**: Adam, RMSprop\n",
    "\n",
    "### ðŸ“ Best Practices:\n",
    "- Start with ReLU for hidden layers\n",
    "- Use He initialization with ReLU\n",
    "- Add BatchNorm after dense layers\n",
    "- Apply gradient clipping (norm=1.0-5.0)\n",
    "- Monitor gradient norms during training\n",
    "- Use adaptive optimizers (Adam) for faster convergence"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}