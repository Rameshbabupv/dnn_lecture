{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸŽ¯ Concept 6: Gradient Solutions Summary\n",
    "\n",
    "## Deep Neural Network Architectures - Week 5\n",
    "**Module:** 2 - Optimization and Regularization  \n",
    "**Topic:** Complete Solution Arsenal for Gradient Problems\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“‹ Learning Objectives\n",
    "By the end of this notebook, you will:\n",
    "1. **Integrate** all gradient problem solutions into a unified framework\n",
    "2. **Implement** gradient clipping and advanced optimization\n",
    "3. **Design** robust neural networks that avoid gradient problems\n",
    "4. **Apply** modern techniques like batch normalization and residual connections\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ¥ The Complete Medical Analogy\n",
    "\n",
    "**The Patient:** Deep Neural Network  \n",
    "**The Disease:** Gradient Problems  \n",
    "**The Treatment Plan:** Multi-layered approach\n",
    "\n",
    "1. **Prevention (Initialization & Architecture):** Healthy lifestyle\n",
    "2. **Early Detection (Monitoring):** Regular checkups\n",
    "3. **Treatment (Clipping & Normalization):** Medicine when needed\n",
    "4. **Long-term Care (Advanced Techniques):** Ongoing health management\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ’» Complete Solution Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import deque\n",
    "import time\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (14, 10)\n",
    "\n",
    "print(\"ðŸš€ Complete Gradient Solutions Toolkit Loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_clipping_demo():\n",
    "    \"\"\"Comprehensive gradient clipping demonstration\"\"\"\n",
    "    \n",
    "    print(\"âœ‚ï¸ GRADIENT CLIPPING COMPREHENSIVE DEMO\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Simulate various gradient scenarios\n",
    "    gradient_scenarios = {\n",
    "        'Normal Gradients': [tf.constant([0.1, -0.2, 0.3]), tf.constant([0.05, -0.1])],\n",
    "        'Large Gradients': [tf.constant([5.0, -8.0, 12.0]), tf.constant([15.0, -20.0])],\n",
    "        'Explosive Gradients': [tf.constant([100.0, -150.0, 200.0]), tf.constant([300.0, -250.0])],\n",
    "        'Mixed Gradients': [tf.constant([0.01, -50.0, 0.1]), tf.constant([100.0, -0.05])]\n",
    "    }\n",
    "\n",
    "    print(\"ðŸ“Š CLIPPING STRATEGIES COMPARISON:\")\n",
    "    print()\n",
    "\n",
    "    for scenario_name, gradients in gradient_scenarios.items():\n",
    "        print(f\"\\nðŸ” Scenario: {scenario_name}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Calculate original norm\n",
    "        total_norm = tf.sqrt(sum(tf.reduce_sum(tf.square(g)) for g in gradients))\n",
    "        print(f\"Original total norm: {total_norm:.4f}\")\n",
    "        \n",
    "        # Strategy 1: Clip by global norm\n",
    "        clipped_by_norm = tf.clip_by_global_norm(gradients, clip_norm=5.0)\n",
    "        clipped_grads, clipped_norm = clipped_by_norm\n",
    "        print(f\"After global norm clipping (5.0): {clipped_norm:.4f}\")\n",
    "        \n",
    "        # Strategy 2: Clip by value\n",
    "        clipped_by_value = [tf.clip_by_value(g, -10.0, 10.0) for g in gradients]\n",
    "        value_norm = tf.sqrt(sum(tf.reduce_sum(tf.square(g)) for g in clipped_by_value))\n",
    "        print(f\"After value clipping (Â±10.0): {value_norm:.4f}\")\n",
    "        \n",
    "        # Strategy 3: Adaptive clipping\n",
    "        adaptive_threshold = min(10.0, max(1.0, total_norm * 0.1))  # 10% of current norm\n",
    "        adaptive_clipped = tf.clip_by_global_norm(gradients, clip_norm=adaptive_threshold)\n",
    "        adaptive_grads, adaptive_norm = adaptive_clipped\n",
    "        print(f\"After adaptive clipping ({adaptive_threshold:.2f}): {adaptive_norm:.4f}\")\n",
    "        \n",
    "        # Show individual gradient changes\n",
    "        print(\"Individual gradient analysis:\")\n",
    "        for i, (orig, clipped) in enumerate(zip(gradients, clipped_grads)):\n",
    "            orig_norm = tf.norm(orig)\n",
    "            clip_norm = tf.norm(clipped)\n",
    "            reduction = orig_norm / (clip_norm + 1e-10)\n",
    "            print(f\"  Gradient {i+1}: {orig_norm:.3f} â†’ {clip_norm:.3f} (reduction: {reduction:.2f}x)\")\n",
    "\n",
    "# Run gradient clipping demo\n",
    "gradient_clipping_demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptiveGradientClipper:\n",
    "    \"\"\"Advanced adaptive gradient clipping system\"\"\"\n",
    "    \n",
    "    def __init__(self, percentile=95, history_length=100, min_clip=0.1, max_clip=10.0):\n",
    "        self.percentile = percentile\n",
    "        self.history_length = history_length\n",
    "        self.min_clip = min_clip\n",
    "        self.max_clip = max_clip\n",
    "        self.gradient_history = deque(maxlen=history_length)\n",
    "        self.clip_history = deque(maxlen=history_length)\n",
    "        self.total_clips = 0\n",
    "        self.total_checks = 0\n",
    "\n",
    "    def adaptive_clip(self, gradients, verbose=False):\n",
    "        \"\"\"Apply adaptive gradient clipping based on historical data\"\"\"\n",
    "        \n",
    "        self.total_checks += 1\n",
    "        \n",
    "        # Calculate current gradient norm\n",
    "        current_norm = tf.sqrt(sum(tf.reduce_sum(tf.square(g)) for g in gradients if g is not None))\n",
    "        current_norm_val = current_norm.numpy()\n",
    "        \n",
    "        # Update history\n",
    "        self.gradient_history.append(current_norm_val)\n",
    "        \n",
    "        # Determine adaptive threshold\n",
    "        if len(self.gradient_history) >= 10:\n",
    "            # Use percentile of recent history\n",
    "            threshold = np.percentile(list(self.gradient_history), self.percentile)\n",
    "            # Clamp to reasonable bounds\n",
    "            threshold = max(self.min_clip, min(self.max_clip, threshold))\n",
    "        else:\n",
    "            # Use default threshold for initial steps\n",
    "            threshold = 1.0\n",
    "        \n",
    "        # Apply clipping if necessary\n",
    "        if current_norm_val > threshold:\n",
    "            clipped_gradients, clipped_norm = tf.clip_by_global_norm(gradients, threshold)\n",
    "            was_clipped = True\n",
    "            self.total_clips += 1\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"ðŸ”¥ Clipped! {current_norm_val:.3f} â†’ {clipped_norm:.3f} (threshold: {threshold:.3f})\")\n",
    "        else:\n",
    "            clipped_gradients = gradients\n",
    "            clipped_norm = current_norm_val\n",
    "            was_clipped = False\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"âœ… No clipping needed: {current_norm_val:.3f} (threshold: {threshold:.3f})\")\n",
    "        \n",
    "        # Record clipping event\n",
    "        self.clip_history.append(was_clipped)\n",
    "        \n",
    "        return clipped_gradients, {\n",
    "            'was_clipped': was_clipped,\n",
    "            'original_norm': current_norm_val,\n",
    "            'clipped_norm': clipped_norm,\n",
    "            'threshold': threshold,\n",
    "            'clip_rate': self.total_clips / self.total_checks if self.total_checks > 0 else 0\n",
    "        }\n",
    "\n",
    "    def get_statistics(self):\n",
    "        \"\"\"Get clipping statistics\"\"\"\n",
    "        recent_clips = sum(self.clip_history) if self.clip_history else 0\n",
    "        recent_rate = recent_clips / len(self.clip_history) if self.clip_history else 0\n",
    "        \n",
    "        return {\n",
    "            'total_checks': self.total_checks,\n",
    "            'total_clips': self.total_clips,\n",
    "            'overall_clip_rate': self.total_clips / self.total_checks if self.total_checks > 0 else 0,\n",
    "            'recent_clip_rate': recent_rate,\n",
    "            'avg_gradient_norm': np.mean(self.gradient_history) if self.gradient_history else 0,\n",
    "            'current_threshold': np.percentile(list(self.gradient_history), self.percentile) if len(self.gradient_history) >= 10 else 1.0\n",
    "        }\n",
    "\n",
    "# Test adaptive clipper\n",
    "print(\"\\nðŸ§ª TESTING ADAPTIVE GRADIENT CLIPPER\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "clipper = AdaptiveGradientClipper(percentile=90, history_length=50)\n",
    "\n",
    "# Simulate training with varying gradient magnitudes\n",
    "simulation_steps = 20\n",
    "clip_results = []\n",
    "\n",
    "for step in range(simulation_steps):\n",
    "    # Simulate different gradient patterns\n",
    "    if step < 5:\n",
    "        # Normal gradients initially\n",
    "        grad_scale = 0.5\n",
    "    elif step < 10:\n",
    "        # Gradual increase\n",
    "        grad_scale = 1.0 + (step - 5) * 0.5\n",
    "    elif step < 15:\n",
    "        # Explosion period\n",
    "        grad_scale = 5.0 + np.random.exponential(2.0)\n",
    "    else:\n",
    "        # Recovery period\n",
    "        grad_scale = max(0.5, 3.0 - (step - 15) * 0.5)\n",
    "    \n",
    "    # Generate synthetic gradients\n",
    "    synthetic_gradients = [\n",
    "        tf.constant([grad_scale * np.random.randn(), -grad_scale * np.random.randn()]),\n",
    "        tf.constant([grad_scale * np.random.randn()])\n",
    "    ]\n",
    "    \n",
    "    # Apply adaptive clipping\n",
    "    clipped_grads, clip_info = clipper.adaptive_clip(synthetic_gradients, verbose=(step % 5 == 0))\n",
    "    clip_results.append(clip_info)\n",
    "\n",
    "# Show final statistics\n",
    "final_stats = clipper.get_statistics()\n",
    "print(f\"\\nðŸ“Š ADAPTIVE CLIPPING STATISTICS:\")\n",
    "print(f\"Total steps: {final_stats['total_checks']}\")\n",
    "print(f\"Total clips: {final_stats['total_clips']}\")\n",
    "print(f\"Overall clip rate: {final_stats['overall_clip_rate']:.1%}\")\n",
    "print(f\"Recent clip rate: {final_stats['recent_clip_rate']:.1%}\")\n",
    "print(f\"Average gradient norm: {final_stats['avg_gradient_norm']:.3f}\")\n",
    "print(f\"Current adaptive threshold: {final_stats['current_threshold']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_optimizers():\n",
    "    \"\"\"Compare modern optimization algorithms for gradient problems\"\"\"\n",
    "    \n",
    "    print(\"\\nðŸš€ MODERN OPTIMIZERS COMPARISON\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Create a problematic network (prone to gradient issues)\n",
    "    def create_test_network():\n",
    "        return tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(128, activation='relu', input_shape=(20,)),\n",
    "            tf.keras.layers.Dense(64, activation='relu'),\n",
    "            tf.keras.layers.Dense(32, activation='relu'),\n",
    "            tf.keras.layers.Dense(16, activation='relu'),\n",
    "            tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "\n",
    "    # Define optimizers to compare\n",
    "    optimizers = {\n",
    "        'SGD': tf.keras.optimizers.SGD(learning_rate=0.01),\n",
    "        'SGD + Momentum': tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9),\n",
    "        'Adam': tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "        'AdamW': tf.keras.optimizers.AdamW(learning_rate=0.001, weight_decay=0.01),\n",
    "        'RMSprop': tf.keras.optimizers.RMSprop(learning_rate=0.001),\n",
    "        'Adagrad': tf.keras.optimizers.Adagrad(learning_rate=0.01)\n",
    "    }\n",
    "\n",
    "    # Generate test data\n",
    "    X_train = tf.random.normal((1000, 20))\n",
    "    y_train = tf.random.uniform((1000, 1))\n",
    "    X_val = tf.random.normal((200, 20))\n",
    "    y_val = tf.random.uniform((200, 1))\n",
    "\n",
    "    optimizer_results = {}\n",
    "    \n",
    "    print(\"\\nðŸ§ª Testing optimizers with gradient monitoring...\")\n",
    "    \n",
    "    for name, optimizer in optimizers.items():\n",
    "        print(f\"\\n--- Testing {name} ---\")\n",
    "        \n",
    "        # Create fresh model for each optimizer\n",
    "        model = create_test_network()\n",
    "        model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "        \n",
    "        # Monitor gradients during training\n",
    "        gradient_norms = []\n",
    "        losses = []\n",
    "        \n",
    "        # Custom training loop for gradient monitoring\n",
    "        for epoch in range(5):  # Quick test\n",
    "            with tf.GradientTape() as tape:\n",
    "                predictions = model(X_train)\n",
    "                loss = tf.reduce_mean(tf.square(predictions - y_train))\n",
    "            \n",
    "            gradients = tape.gradient(loss, model.trainable_variables)\n",
    "            \n",
    "            # Calculate gradient norm\n",
    "            grad_norm = tf.sqrt(sum(tf.reduce_sum(tf.square(g)) for g in gradients if g is not None))\n",
    "            gradient_norms.append(grad_norm.numpy())\n",
    "            losses.append(loss.numpy())\n",
    "            \n",
    "            # Apply optimizer\n",
    "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        \n",
    "        # Evaluate final performance\n",
    "        final_loss = model.evaluate(X_val, y_val, verbose=0)[0]\n",
    "        \n",
    "        optimizer_results[name] = {\n",
    "            'gradient_norms': gradient_norms,\n",
    "            'losses': losses,\n",
    "            'final_loss': final_loss,\n",
    "            'avg_gradient_norm': np.mean(gradient_norms),\n",
    "            'gradient_stability': np.std(gradient_norms),\n",
    "            'convergence_speed': losses[0] - losses[-1]  # Loss reduction\n",
    "        }\n",
    "        \n",
    "        print(f\"  Final loss: {final_loss:.6f}\")\n",
    "        print(f\"  Avg gradient norm: {np.mean(gradient_norms):.4f}\")\n",
    "        print(f\"  Gradient stability (std): {np.std(gradient_norms):.4f}\")\n",
    "    \n",
    "    return optimizer_results\n",
    "\n",
    "# Run optimizer comparison\n",
    "optimizer_results = compare_optimizers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize optimizer comparison results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "optimizer_names = list(optimizer_results.keys())\n",
    "colors = plt.cm.tab10(np.linspace(0, 1, len(optimizer_names)))\n",
    "\n",
    "# Plot 1: Loss curves\n",
    "ax1 = axes[0, 0]\n",
    "for i, (name, result) in enumerate(optimizer_results.items()):\n",
    "    epochs = list(range(1, len(result['losses']) + 1))\n",
    "    ax1.plot(epochs, result['losses'], 'o-', color=colors[i], \n",
    "             label=name, linewidth=2, markersize=6)\n",
    "\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Training Loss')\n",
    "ax1.set_title('Loss Convergence by Optimizer')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_yscale('log')\n",
    "\n",
    "# Plot 2: Gradient norm evolution\n",
    "ax2 = axes[0, 1]\n",
    "for i, (name, result) in enumerate(optimizer_results.items()):\n",
    "    epochs = list(range(1, len(result['gradient_norms']) + 1))\n",
    "    ax2.plot(epochs, result['gradient_norms'], 'o-', color=colors[i], \n",
    "             label=name, linewidth=2, markersize=6)\n",
    "\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Gradient Norm')\n",
    "ax2.set_title('Gradient Norm Evolution')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_yscale('log')\n",
    "\n",
    "# Plot 3: Final performance comparison\n",
    "ax3 = axes[1, 0]\n",
    "final_losses = [optimizer_results[name]['final_loss'] for name in optimizer_names]\n",
    "bars = ax3.bar(range(len(optimizer_names)), final_losses, color=colors, alpha=0.7)\n",
    "ax3.set_ylabel('Final Validation Loss')\n",
    "ax3.set_title('Final Performance Comparison')\n",
    "ax3.set_xticks(range(len(optimizer_names)))\n",
    "ax3.set_xticklabels(optimizer_names, rotation=45, ha='right')\n",
    "ax3.set_yscale('log')\n",
    "\n",
    "# Add value labels\n",
    "for bar, value in zip(bars, final_losses):\n",
    "    ax3.text(bar.get_x() + bar.get_width()/2, value, f'{value:.4f}', \n",
    "             ha='center', va='bottom', rotation=45, fontsize=8)\n",
    "\n",
    "# Plot 4: Gradient stability comparison\n",
    "ax4 = axes[1, 1]\n",
    "avg_grad_norms = [optimizer_results[name]['avg_gradient_norm'] for name in optimizer_names]\n",
    "grad_stabilities = [optimizer_results[name]['gradient_stability'] for name in optimizer_names]\n",
    "\n",
    "scatter = ax4.scatter(avg_grad_norms, grad_stabilities, c=colors, s=100, alpha=0.7)\n",
    "for i, name in enumerate(optimizer_names):\n",
    "    ax4.annotate(name, (avg_grad_norms[i], grad_stabilities[i]), \n",
    "                xytext=(5, 5), textcoords='offset points', fontsize=9)\n",
    "\n",
    "ax4.set_xlabel('Average Gradient Norm')\n",
    "ax4.set_ylabel('Gradient Stability (Std Dev)')\n",
    "ax4.set_title('Gradient Stability vs Magnitude')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print optimizer ranking\n",
    "print(\"\\nðŸ† OPTIMIZER PERFORMANCE RANKING\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Calculate overall score (lower loss + lower gradient instability = better)\n",
    "scores = []\n",
    "for name in optimizer_names:\n",
    "    result = optimizer_results[name]\n",
    "    # Normalize metrics (lower is better)\n",
    "    loss_score = result['final_loss']\n",
    "    stability_score = result['gradient_stability']\n",
    "    convergence_score = -result['convergence_speed']  # Negative because higher convergence is better\n",
    "    \n",
    "    overall_score = loss_score + stability_score + convergence_score\n",
    "    scores.append((name, overall_score, result))\n",
    "\n",
    "# Sort by overall score\n",
    "scores.sort(key=lambda x: x[1])\n",
    "\n",
    "print(f\"{'Rank':<6} {'Optimizer':<15} {'Final Loss':<12} {'Gradient Stability':<18} {'Overall Score':<15}\")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "for rank, (name, score, result) in enumerate(scores, 1):\n",
    "    emoji = \"ðŸ¥‡\" if rank == 1 else \"ðŸ¥ˆ\" if rank == 2 else \"ðŸ¥‰\" if rank == 3 else \"ðŸ“Š\"\n",
    "    print(f\"{emoji} {rank:<4} {name:<15} {result['final_loss']:<12.6f} {result['gradient_stability']:<18.6f} {score:<15.6f}\")\n",
    "\n",
    "print(f\"\\nðŸŽ–ï¸ WINNER: {scores[0][0]} - Best overall performance!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_robust_network():\n",
    "    \"\"\"Create a network using all gradient problem solutions\"\"\"\n",
    "    \n",
    "    print(\"\\nðŸ—ï¸ BUILDING GRADIENT-ROBUST NETWORK\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"Incorporating all learned techniques:\")\n",
    "    print(\"âœ… He initialization for ReLU\")\n",
    "    print(\"âœ… Batch normalization\")\n",
    "    print(\"âœ… Dropout for regularization\")\n",
    "    print(\"âœ… Residual connections\")\n",
    "    print(\"âœ… Gradient clipping\")\n",
    "    print(\"âœ… Adam optimizer\")\n",
    "    print()\n",
    "    \n",
    "    # Custom residual block\n",
    "    class ResidualBlock(tf.keras.layers.Layer):\n",
    "        def __init__(self, units, dropout_rate=0.1):\n",
    "            super(ResidualBlock, self).__init__()\n",
    "            self.dense1 = tf.keras.layers.Dense(units, activation='relu',\n",
    "                                               kernel_initializer='he_uniform')\n",
    "            self.bn1 = tf.keras.layers.BatchNormalization()\n",
    "            self.dropout1 = tf.keras.layers.Dropout(dropout_rate)\n",
    "            \n",
    "            self.dense2 = tf.keras.layers.Dense(units,\n",
    "                                               kernel_initializer='he_uniform')\n",
    "            self.bn2 = tf.keras.layers.BatchNormalization()\n",
    "            self.dropout2 = tf.keras.layers.Dropout(dropout_rate)\n",
    "            \n",
    "            self.activation = tf.keras.layers.ReLU()\n",
    "\n",
    "        def call(self, x, training=False):\n",
    "            # First sub-layer\n",
    "            residual = x\n",
    "            x = self.dense1(x)\n",
    "            x = self.bn1(x, training=training)\n",
    "            x = self.activation(x)\n",
    "            x = self.dropout1(x, training=training)\n",
    "            \n",
    "            # Second sub-layer\n",
    "            x = self.dense2(x)\n",
    "            x = self.bn2(x, training=training)\n",
    "            \n",
    "            # Residual connection\n",
    "            x = x + residual  # Skip connection\n",
    "            x = self.activation(x)\n",
    "            x = self.dropout2(x, training=training)\n",
    "            \n",
    "            return x\n",
    "    \n",
    "    # Build the robust network\n",
    "    model = tf.keras.Sequential([\n",
    "        # Input layer with batch norm\n",
    "        tf.keras.layers.Dense(128, activation='relu', input_shape=(100,),\n",
    "                             kernel_initializer='he_uniform', name='input_dense'),\n",
    "        tf.keras.layers.BatchNormalization(name='input_bn'),\n",
    "        tf.keras.layers.Dropout(0.1, name='input_dropout'),\n",
    "        \n",
    "        # Residual blocks\n",
    "        ResidualBlock(128, dropout_rate=0.1),\n",
    "        ResidualBlock(128, dropout_rate=0.1),\n",
    "        ResidualBlock(128, dropout_rate=0.1),\n",
    "        \n",
    "        # Output layers\n",
    "        tf.keras.layers.Dense(64, activation='relu',\n",
    "                             kernel_initializer='he_uniform', name='output_dense1'),\n",
    "        tf.keras.layers.BatchNormalization(name='output_bn'),\n",
    "        tf.keras.layers.Dropout(0.2, name='output_dropout'),\n",
    "        \n",
    "        tf.keras.layers.Dense(10, activation='softmax',\n",
    "                             kernel_initializer='glorot_uniform', name='final_output')\n",
    "    ], name='RobustGradientNetwork')\n",
    "    \n",
    "    # Compile with Adam and gradient clipping\n",
    "    optimizer = tf.keras.optimizers.Adam(\n",
    "        learning_rate=0.001,\n",
    "        clipnorm=1.0  # Built-in gradient clipping\n",
    "    )\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    print(\"ðŸ Robust network created successfully!\")\n",
    "    print(f\"Total parameters: {model.count_params():,}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create and test the robust network\n",
    "robust_model = create_robust_network()\n",
    "\n",
    "# Display model summary\n",
    "print(\"\\nðŸ“Š MODEL ARCHITECTURE SUMMARY:\")\n",
    "robust_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the robust network's gradient health\n",
    "def test_robust_network_gradients(model):\n",
    "    \"\"\"Test gradient health of the robust network\"\"\"\n",
    "    \n",
    "    print(\"\\nðŸ§ª TESTING ROBUST NETWORK GRADIENT HEALTH\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Generate test data\n",
    "    X_test = tf.random.normal((100, 100))\n",
    "    y_test = tf.keras.utils.to_categorical(np.random.randint(0, 10, 100), 10)\n",
    "    \n",
    "    # Monitor gradients over several steps\n",
    "    gradient_health_over_time = []\n",
    "    \n",
    "    for step in range(10):\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = model(X_test, training=True)\n",
    "            loss = tf.keras.losses.categorical_crossentropy(y_test, predictions)\n",
    "            loss = tf.reduce_mean(loss)\n",
    "        \n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        \n",
    "        # Analyze gradient health\n",
    "        grad_norms = []\n",
    "        for grad in gradients:\n",
    "            if grad is not None:\n",
    "                norm = tf.norm(grad).numpy()\n",
    "                if np.isfinite(norm):\n",
    "                    grad_norms.append(norm)\n",
    "        \n",
    "        if grad_norms:\n",
    "            health_metrics = {\n",
    "                'step': step + 1,\n",
    "                'min_gradient': min(grad_norms),\n",
    "                'max_gradient': max(grad_norms),\n",
    "                'mean_gradient': np.mean(grad_norms),\n",
    "                'std_gradient': np.std(grad_norms),\n",
    "                'vanished_layers': sum(1 for g in grad_norms if g < 1e-6),\n",
    "                'weak_layers': sum(1 for g in grad_norms if g < 1e-4),\n",
    "                'exploding_layers': sum(1 for g in grad_norms if g > 10),\n",
    "                'total_layers': len(grad_norms),\n",
    "                'loss': loss.numpy()\n",
    "            }\n",
    "            \n",
    "            gradient_health_over_time.append(health_metrics)\n",
    "        \n",
    "        # Apply gradients (simulate training step)\n",
    "        model.optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    \n",
    "    # Analyze results\n",
    "    print(\"\\nðŸ“Š GRADIENT HEALTH ANALYSIS:\")\n",
    "    print(f\"{'Step':<6} {'Loss':<10} {'Min Grad':<12} {'Max Grad':<12} {'Vanished':<10} {'Exploding':<12}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for metrics in gradient_health_over_time:\n",
    "        print(f\"{metrics['step']:<6} {metrics['loss']:<10.6f} {metrics['min_gradient']:<12.2e} \"\n",
    "              f\"{metrics['max_gradient']:<12.2e} {metrics['vanished_layers']:<10} {metrics['exploding_layers']:<12}\")\n",
    "    \n",
    "    # Overall health assessment\n",
    "    total_vanished = sum(m['vanished_layers'] for m in gradient_health_over_time)\n",
    "    total_exploding = sum(m['exploding_layers'] for m in gradient_health_over_time)\n",
    "    avg_gradient_range = np.mean([m['max_gradient'] / (m['min_gradient'] + 1e-10) \n",
    "                                  for m in gradient_health_over_time])\n",
    "    \n",
    "    print(\"\\nðŸŽ¯ OVERALL HEALTH ASSESSMENT:\")\n",
    "    print(f\"Total vanished gradient incidents: {total_vanished}\")\n",
    "    print(f\"Total exploding gradient incidents: {total_exploding}\")\n",
    "    print(f\"Average gradient range ratio: {avg_gradient_range:.2f}\")\n",
    "    \n",
    "    if total_vanished == 0 and total_exploding == 0:\n",
    "        print(\"ðŸŸ¢ EXCELLENT: No gradient problems detected!\")\n",
    "    elif total_vanished <= 2 and total_exploding <= 1:\n",
    "        print(\"ðŸŸ¡ GOOD: Minor gradient issues, well controlled\")\n",
    "    else:\n",
    "        print(\"ðŸŸ  NEEDS IMPROVEMENT: Some gradient problems persist\")\n",
    "    \n",
    "    return gradient_health_over_time\n",
    "\n",
    "# Test the robust network\n",
    "robust_health = test_robust_network_gradients(robust_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create final visualization comparing all approaches\n",
    "def create_final_comparison():\n",
    "    \"\"\"Create final comparison of all gradient solutions\"\"\"\n",
    "    \n",
    "    print(\"\\nðŸŽ¨ FINAL SOLUTIONS COMPARISON\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Solution categories and their effectiveness\n",
    "    solutions = {\n",
    "        'ReLU Activation': {'effectiveness': 9, 'complexity': 1, 'cost': 1},\n",
    "        'He Initialization': {'effectiveness': 8, 'complexity': 2, 'cost': 1},\n",
    "        'Batch Normalization': {'effectiveness': 9, 'complexity': 3, 'cost': 2},\n",
    "        'Gradient Clipping': {'effectiveness': 7, 'complexity': 2, 'cost': 1},\n",
    "        'Adam Optimizer': {'effectiveness': 8, 'complexity': 2, 'cost': 1},\n",
    "        'Residual Connections': {'effectiveness': 9, 'complexity': 4, 'cost': 2},\n",
    "        'LSUV Initialization': {'effectiveness': 8, 'complexity': 5, 'cost': 3},\n",
    "        'Attention Mechanisms': {'effectiveness': 9, 'complexity': 8, 'cost': 4}\n",
    "    }\n",
    "    \n",
    "    # Create comprehensive comparison chart\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    solution_names = list(solutions.keys())\n",
    "    effectiveness = [solutions[name]['effectiveness'] for name in solution_names]\n",
    "    complexity = [solutions[name]['complexity'] for name in solution_names]\n",
    "    cost = [solutions[name]['cost'] for name in solution_names]\n",
    "    \n",
    "    # Plot 1: Effectiveness comparison\n",
    "    ax1 = axes[0, 0]\n",
    "    bars1 = ax1.barh(solution_names, effectiveness, color='green', alpha=0.7)\n",
    "    ax1.set_xlabel('Effectiveness Score (1-10)')\n",
    "    ax1.set_title('Solution Effectiveness for Gradient Problems')\n",
    "    ax1.set_xlim(0, 10)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, value in zip(bars1, effectiveness):\n",
    "        ax1.text(value + 0.1, bar.get_y() + bar.get_height()/2, str(value), \n",
    "                 va='center', fontweight='bold')\n",
    "    \n",
    "    # Plot 2: Implementation complexity\n",
    "    ax2 = axes[0, 1]\n",
    "    bars2 = ax2.barh(solution_names, complexity, color='orange', alpha=0.7)\n",
    "    ax2.set_xlabel('Implementation Complexity (1-10)')\n",
    "    ax2.set_title('Implementation Complexity')\n",
    "    ax2.set_xlim(0, 10)\n",
    "    \n",
    "    for bar, value in zip(bars2, complexity):\n",
    "        ax2.text(value + 0.1, bar.get_y() + bar.get_height()/2, str(value), \n",
    "                 va='center', fontweight='bold')\n",
    "    \n",
    "    # Plot 3: Effectiveness vs Complexity scatter\n",
    "    ax3 = axes[1, 0]\n",
    "    colors = plt.cm.viridis(np.array(cost) / max(cost))\n",
    "    scatter = ax3.scatter(complexity, effectiveness, c=colors, s=100, alpha=0.7)\n",
    "    \n",
    "    for i, name in enumerate(solution_names):\n",
    "        ax3.annotate(name.replace(' ', '\\n'), (complexity[i], effectiveness[i]), \n",
    "                    xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "    \n",
    "    ax3.set_xlabel('Implementation Complexity')\n",
    "    ax3.set_ylabel('Effectiveness')\n",
    "    ax3.set_title('Effectiveness vs Complexity\\n(Color = Computational Cost)')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 4: Recommended combination for different scenarios\n",
    "    ax4 = axes[1, 1]\n",
    "    scenarios = ['Beginner\\nProject', 'Production\\nSystem', 'Research\\nExperiment', 'Resource\\nConstrained']\n",
    "    recommendations = [\n",
    "        ['ReLU', 'He Init', 'Adam'],  # Beginner\n",
    "        ['ReLU', 'BatchNorm', 'ResNet', 'Clipping'],  # Production\n",
    "        ['All Solutions', 'LSUV', 'Attention'],  # Research\n",
    "        ['ReLU', 'He Init', 'SGD']  # Resource constrained\n",
    "    ]\n",
    "    \n",
    "    # Create recommendation matrix\n",
    "    rec_text = \"\\n\".join([f\"{scenario}: {', '.join(rec)}\" \n",
    "                          for scenario, rec in zip(scenarios, recommendations)])\n",
    "    \n",
    "    ax4.text(0.05, 0.95, \"RECOMMENDED COMBINATIONS:\", \n",
    "             transform=ax4.transAxes, fontsize=12, fontweight='bold', va='top')\n",
    "    ax4.text(0.05, 0.85, rec_text, transform=ax4.transAxes, fontsize=10, va='top')\n",
    "    ax4.set_xlim(0, 1)\n",
    "    ax4.set_ylim(0, 1)\n",
    "    ax4.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print final recommendations\n",
    "    print(\"\\nðŸŽ¯ FINAL GRADIENT SOLUTIONS GUIDE\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    print(\"ðŸ¥‡ ESSENTIAL (Must-have for deep networks):\")\n",
    "    print(\"   â€¢ ReLU activation functions\")\n",
    "    print(\"   â€¢ He initialization\")\n",
    "    print(\"   â€¢ Adam optimizer\")\n",
    "    \n",
    "    print(\"\\nðŸ¥ˆ HIGHLY RECOMMENDED (For robust training):\")\n",
    "    print(\"   â€¢ Batch normalization\")\n",
    "    print(\"   â€¢ Gradient clipping\")\n",
    "    print(\"   â€¢ Proper learning rate scheduling\")\n",
    "    \n",
    "    print(\"\\nðŸ¥‰ ADVANCED (For cutting-edge performance):\")\n",
    "    print(\"   â€¢ Residual connections\")\n",
    "    print(\"   â€¢ LSUV initialization\")\n",
    "    print(\"   â€¢ Attention mechanisms\")\n",
    "    \n",
    "    print(\"\\nðŸ’¡ GOLDEN RULES:\")\n",
    "    print(\"   1. Start simple (ReLU + He + Adam)\")\n",
    "    print(\"   2. Add complexity only when needed\")\n",
    "    print(\"   3. Always monitor gradient health\")\n",
    "    print(\"   4. Test thoroughly before deployment\")\n",
    "    print(\"   5. Document what works for your specific problem\")\n",
    "\n",
    "# Create final comparison\n",
    "create_final_comparison()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸŽ“ Complete Gradient Solutions Framework\n",
    "\n",
    "### ðŸ“š **The Complete Toolkit**\n",
    "\n",
    "We've now covered the entire arsenal of gradient problem solutions:\n",
    "\n",
    "#### **1. Foundation Layer** (Essential)\n",
    "- âœ… **ReLU Activations:** Non-saturating, gradient-preserving\n",
    "- âœ… **He Initialization:** Proper weight scaling for ReLU\n",
    "- âœ… **Adam Optimizer:** Adaptive learning rates with momentum\n",
    "\n",
    "#### **2. Stabilization Layer** (Highly Recommended)\n",
    "- âœ… **Batch Normalization:** Stabilizes training dynamics\n",
    "- âœ… **Gradient Clipping:** Prevents explosion disasters\n",
    "- âœ… **Learning Rate Scheduling:** Adaptive training progression\n",
    "\n",
    "#### **3. Architecture Layer** (Advanced)\n",
    "- âœ… **Residual Connections:** Gradient highways for very deep networks\n",
    "- âœ… **LSUV Initialization:** Layer-wise variance control\n",
    "- âœ… **Attention Mechanisms:** Advanced information flow\n",
    "\n",
    "#### **4. Monitoring Layer** (Critical)\n",
    "- âœ… **Gradient Health Monitoring:** Real-time gradient analysis\n",
    "- âœ… **Adaptive Clipping:** Dynamic threshold adjustment\n",
    "- âœ… **Training Diagnostics:** Comprehensive health metrics\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸš€ **From Crisis to Triumph**\n",
    "\n",
    "### **The Journey We've Taken:**\n",
    "\n",
    "1. **ðŸ•³ï¸ Discovered the Problem:** Vanishing gradients killing deep learning\n",
    "2. **ðŸ’¥ Understood the Opposite:** Exploding gradients destroying training\n",
    "3. **ðŸ” Analyzed the Mathematics:** Chain rule multiplication effects\n",
    "4. **ðŸ§ª Tested Activation Functions:** Why ReLU revolutionized the field\n",
    "5. **âš–ï¸ Mastered Initialization:** Proper weight scaling principles\n",
    "6. **ðŸ› ï¸ Implemented Solutions:** Complete robust network design\n",
    "\n",
    "### **The Modern Deep Learning Stack:**\n",
    "```\n",
    "ðŸ—ï¸ Modern Deep Network\n",
    "â”œâ”€â”€ ReLU Activations (gradient flow)\n",
    "â”œâ”€â”€ He Initialization (proper scaling)\n",
    "â”œâ”€â”€ Batch Normalization (stability)\n",
    "â”œâ”€â”€ Residual Connections (depth enabler)\n",
    "â”œâ”€â”€ Adam Optimizer (adaptive learning)\n",
    "â”œâ”€â”€ Gradient Clipping (safety net)\n",
    "â””â”€â”€ Health Monitoring (diagnostics)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ’¡ **Key Insights for Deep Learning Success**\n",
    "\n",
    "### **ðŸŽ¯ The Golden Triangle:**\n",
    "1. **Proper Initialization** â†’ Healthy start\n",
    "2. **Good Architecture** â†’ Sustainable flow  \n",
    "3. **Smart Optimization** â†’ Efficient learning\n",
    "\n",
    "### **ðŸ”§ Implementation Strategy:**\n",
    "1. **Start Simple:** ReLU + He + Adam\n",
    "2. **Add Stability:** BatchNorm + Clipping\n",
    "3. **Scale Up:** ResNet + Advanced techniques\n",
    "4. **Monitor Always:** Health metrics + diagnostics\n",
    "\n",
    "### **âš ï¸ Common Pitfalls to Avoid:**\n",
    "- Using sigmoid in deep networks\n",
    "- Random weight initialization without scaling\n",
    "- Ignoring gradient monitoring\n",
    "- Adding complexity before mastering basics\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽŠ **Congratulations!**\n",
    "\n",
    "You've mastered one of the most fundamental challenges in deep learning. The gradient problem that once seemed insurmountable now has a complete solution framework. \n",
    "\n",
    "**Remember:** Every breakthrough in deep learningâ€”from AlexNet to GPTâ€”builds on these foundational solutions to gradient problems. You now understand the engineering principles that make modern AI possible.\n",
    "\n",
    "**Your toolkit is complete. Your journey in deep learning has truly begun.** ðŸš€\n",
    "\n",
    "---\n",
    "\n",
    "*This completes the Gradient Problems series for Week 5: Deep Neural Network Architectures*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Week 5: Gradients (Python 3.10)",
   "language": "python",
   "name": "week5-gradients"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}