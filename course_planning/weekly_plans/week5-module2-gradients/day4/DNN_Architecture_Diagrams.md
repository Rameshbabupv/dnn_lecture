# Deep Neural Network Architecture Diagrams

**Course:** 21CSE558T - Deep Neural Network Architectures
**For:** M.Tech Students, SRM University
**Week 5, Day 4 - Module 2: Optimization & Regularization**

---

## 1. Basic Deep Neural Network (Multi-Layer Perceptron)

```
     INPUT LAYER    HIDDEN LAYER 1   HIDDEN LAYER 2   HIDDEN LAYER 3   OUTPUT LAYER
         (4)            (6)              (8)              (6)             (3)

        xâ‚ â—â”€â”€â”€â”€â”€â”€â”€â”€â—â”€â”€â”€â—â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â—â”€â”€â”€â—â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â—â”€â”€â”€â—â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â— yâ‚
           â”‚    â•±   â”‚   â”‚    â•±       â”‚   â”‚    â•±       â”‚   â”‚    â•±       â”‚
        xâ‚‚ â—â”€â”€â”€â•±â”€â”€â”€â”€â—â”€â”€â”€â—â”€â”€â”€â•±â”€â”€â”€â”€â”€â”€â”€â”€â—â”€â”€â”€â—â”€â”€â”€â•±â”€â”€â”€â”€â”€â”€â”€â”€â—â”€â”€â”€â—â”€â”€â”€â•±â”€â”€â”€â”€â”€â”€â”€â”€â— yâ‚‚
           â”‚  â•±     â”‚   â”‚  â•±         â”‚   â”‚  â•±         â”‚   â”‚  â•±         â”‚
        xâ‚ƒ â—â”€â•±â”€â”€â”€â”€â”€â”€â—â”€â”€â”€â—â”€â•±â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â—â”€â”€â”€â—â”€â•±â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â—â”€â”€â”€â—â”€â•±â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â— yâ‚ƒ
           â”‚â•±       â”‚   â”‚â•±           â”‚   â”‚â•±           â”‚   â”‚â•±           â”‚
        xâ‚„ â—â”€â”€â”€â”€â”€â”€â”€â”€â—â”€â”€â”€â—â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â—â”€â”€â”€â—â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â—â”€â”€â”€â—â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â—
           â•²       â•±â”‚   â•²           â•±â”‚   â•²           â•±â”‚   â•²           â•±
            â•²     â•± â”‚    â•²         â•± â”‚    â•²         â•± â”‚    â•²         â•±
             â•²   â•±  â”‚     â•²       â•±  â”‚     â•²       â•±  â”‚     â•²       â•±
              â•² â•±   â”‚      â•²     â•±   â”‚      â•²     â•±   â”‚      â•²     â•±
               â—    â”‚       â—â”€â”€â”€â—    â”‚       â—â”€â”€â”€â—    â”‚       â—â”€â”€â”€â—
               â”‚    â”‚       â”‚   â”‚    â”‚       â”‚   â”‚    â”‚       â”‚
               â—    â”‚       â—â”€â”€â”€â—    â”‚       â—â”€â”€â”€â—    â”‚       â—
               â”‚    â”‚       â”‚   â”‚    â”‚       â”‚   â”‚    â”‚
               â—â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â—â”€â”€â”€â—â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â—â”€â”€â”€â—â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â—

     Features:     128 neurons    256 neurons     128 neurons    Classes:
     â€¢ xâ‚ (age)      ReLU           ReLU            ReLU         â€¢ Class A
     â€¢ xâ‚‚ (income)   Dropout(0.2)   Dropout(0.3)    Dropout(0.2)  â€¢ Class B
     â€¢ xâ‚ƒ (score)    L2 Reg.        L2 Reg.         L2 Reg.      â€¢ Class C
     â€¢ xâ‚„ (rating)

Mathematical Formulation:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  hâ½Ë¡âºÂ¹â¾ = f(Wâ½Ë¡â¾hâ½Ë¡â¾ + bâ½Ë¡â¾)                              â”‚
â”‚                                                             â”‚
â”‚  Where:                                                     â”‚
â”‚  â€¢ hâ½Ë¡â¾ = hidden layer l                                    â”‚
â”‚  â€¢ Wâ½Ë¡â¾ = weight matrix                                     â”‚
â”‚  â€¢ bâ½Ë¡â¾ = bias vector                                       â”‚
â”‚  â€¢ f() = activation function (ReLU, Sigmoid, Tanh)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Components:
- **Forward Propagation**: Data flows left to right
- **Weights (W)**: Connection strengths between neurons
- **Biases (b)**: Threshold adjustments for each neuron
- **Activation Functions**: Introduce non-linearity
- **Regularization**: Prevent overfitting (L2, Dropout)

---

## 2. Convolutional Neural Network (CNN) Architecture

```
INPUT IMAGE â†’ FEATURE EXTRACTION â†’ CLASSIFICATION

   28Ã—28Ã—1        Conv+Pool Layers      Fully Connected
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ â–‘â–‘â–‘â–‘â–‘â–‘ â”‚     â”Œâ”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”
  â”‚ â–‘â–‘â–ˆâ–ˆâ–‘â–‘ â”‚ â†’   â”‚CONV â”‚â†’ â”‚POOL â”‚   â†’   â”‚ FC  â”‚â†’ â”‚DROP â”‚â†’ â”‚ FC  â”‚
  â”‚ â–‘â–ˆâ–ˆâ–‘â–ˆâ–ˆ â”‚     â”‚3Ã—3  â”‚  â”‚2Ã—2  â”‚  â•²    â”‚ 128 â”‚  â”‚ 0.5 â”‚  â”‚ 10  â”‚
  â”‚ â–‘â–‘â–‘â–‘â–‘â–‘ â”‚     â”‚ReLU â”‚  â”‚Max  â”‚   â•²   â”‚     â”‚  â”‚     â”‚  â”‚     â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”˜    â•²  â””â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”˜
                                      â•²      â†“       â†“       â†“
                â”Œâ”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”       â•² â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”
                â”‚CONV â”‚â†’ â”‚POOL â”‚        â†’â”‚ReLU â”‚ â”‚ReLU â”‚ â”‚ Softâ”‚
                â”‚3Ã—3  â”‚  â”‚2Ã—2  â”‚         â”‚     â”‚ â”‚     â”‚ â”‚ max â”‚
                â”‚ReLU â”‚  â”‚Max  â”‚         â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜
                â””â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”˜              â†“       â†“      â†“
                     â†“       â†“            Activation Dropout Classes
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚26Ã—26Ã—32 â”‚  â”‚13Ã—13Ã—32 â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â†“           â†“
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚24Ã—24Ã—64 â”‚  â”‚12Ã—12Ã—64 â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â†“
                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                          â”‚Flatten  â”‚ â†’ 9216 features
                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

CONVOLUTION OPERATION:
    Input        Filter      Output
â”Œâ”€ 1  2  3 â”€â”  â”Œâ”€1  0â”€â”   â”Œâ”€ 4 â”€â”
â”‚  4  5  6   â”‚  â”‚ 0  1 â”‚ = â”‚  8  â”‚  (4Ã—1 + 5Ã—0 + 7Ã—0 + 8Ã—1 = 4+8 = 12)
â””â”€ 7  8  9 â”€â”˜  â””â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”˜

KEY CNN CONCEPTS:
â€¢ Convolution: Feature detection using filters/kernels
â€¢ Pooling: Dimensionality reduction (Max, Average)
â€¢ Parameter Sharing: Same filter applied across input
â€¢ Translation Invariance: Features detected regardless of position
â€¢ Hierarchical Features: Low-level â†’ High-level features
```

---

## 3. Recurrent Neural Network (RNN) & LSTM

### Simple RNN Architecture:
```
TIME STEPS:    t=1      t=2      t=3      t=4      t=5
              â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’

INPUT:         xâ‚   â†’   xâ‚‚   â†’   xâ‚ƒ   â†’   xâ‚„   â†’   xâ‚…
               â”‚        â”‚        â”‚        â”‚        â”‚
               â–¼        â–¼        â–¼        â–¼        â–¼
RNN CELL:    â”Œâ”€â”€â”€â”    â”Œâ”€â”€â”€â”    â”Œâ”€â”€â”€â”    â”Œâ”€â”€â”€â”    â”Œâ”€â”€â”€â”
             â”‚RNNâ”‚ â†’  â”‚RNNâ”‚ â†’  â”‚RNNâ”‚ â†’  â”‚RNNâ”‚ â†’  â”‚RNNâ”‚
             â””â”€â”€â”€â”˜    â””â”€â”€â”€â”˜    â””â”€â”€â”€â”˜    â””â”€â”€â”€â”˜    â””â”€â”€â”€â”˜
               â”‚        â”‚        â”‚        â”‚        â”‚
HIDDEN:       hâ‚   â†’   hâ‚‚   â†’   hâ‚ƒ   â†’   hâ‚„   â†’   hâ‚…
               â”‚        â”‚        â”‚        â”‚        â”‚
OUTPUT:        yâ‚       yâ‚‚       yâ‚ƒ       yâ‚„       yâ‚…

Mathematical Formulation:
â€¢ hâ‚œ = tanh(Wâ‚•â‚•hâ‚œâ‚‹â‚ + Wâ‚“â‚•xâ‚œ + bâ‚•)
â€¢ yâ‚œ = Wâ‚•áµ§hâ‚œ + báµ§
```

### LSTM (Long Short-Term Memory) Cell:
```
                    LSTM CELL ARCHITECTURE
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚                         â”‚
    Câ‚œâ‚‹â‚ â”€â”€â”€â”€â”€â”€â”€â”€â†’ â”‚    â”Œâ”€ Ã— â”€â”  â”Œâ”€ + â”€â”   â”‚ â”€â”€â”€â”€â†’ Câ‚œ
                   â”‚    â”‚  f  â”‚  â”‚     â”‚   â”‚   (Cell State)
    hâ‚œâ‚‹â‚ â”€â”€â”       â”‚    â””â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”˜   â”‚
           â”‚       â”‚        â”‚       â•±     â”‚
    xâ‚œ â”€â”€â”€â”€â”¤â”€â”€â”€â”€â”€â”€â”€â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â•±â”€â”€â”€â”€â”€â”€â”‚â”€â”€â”€â”€â†’ hâ‚œ
           â”‚       â”‚    â”Œâ”€ Ã— â”€â”  â”Œâ”€ Ã— â”€â”   â”‚   (Hidden State)
           â”‚       â”‚    â”‚  i  â”‚  â”‚ tanhâ”‚   â”‚
           â”‚       â”‚    â””â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”˜   â”‚
           â”‚       â”‚        â”‚       â”‚     â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”‚â”€â”€â”€â”
                   â”‚    â”Œâ”€ Ã— â”€â”      â”‚     â”‚   â”‚
                   â”‚    â”‚  o  â”‚      â”‚     â”‚   â”‚
                   â””â”€â”€â”€â”€â””â”€â”€â”€â”€â”€â”˜â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”˜   â”‚
                            â”‚        â”‚         â”‚
                            â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”    â”‚
                            â”‚    â”‚ tanh  â”‚    â”‚
                            â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
                            â”‚        â”‚        â”‚
                            â””â”€â”€â”€â”€â”€â”€â”€â”€Ã—â”€â”€â”€â”€â”€â”€â”€â”€â”˜

GATES:
â€¢ f = Forget Gate    (What to forget from cell state)
â€¢ i = Input Gate     (What new info to store)
â€¢ o = Output Gate    (What to output)
â€¢ tanh = New values  (Candidate values)

EQUATIONS:
â€¢ fâ‚œ = Ïƒ(WfÂ·[hâ‚œâ‚‹â‚,xâ‚œ] + bf)     Forget Gate
â€¢ iâ‚œ = Ïƒ(WiÂ·[hâ‚œâ‚‹â‚,xâ‚œ] + bi)     Input Gate
â€¢ CÌƒâ‚œ = tanh(WCÂ·[hâ‚œâ‚‹â‚,xâ‚œ] + bC)  Candidate
â€¢ Câ‚œ = fâ‚œ * Câ‚œâ‚‹â‚ + iâ‚œ * CÌƒâ‚œ      Cell State
â€¢ oâ‚œ = Ïƒ(WoÂ·[hâ‚œâ‚‹â‚,xâ‚œ] + bo)     Output Gate
â€¢ hâ‚œ = oâ‚œ * tanh(Câ‚œ)            Hidden State
```

---

## 4. Transfer Learning Architecture

```
PRE-TRAINED MODEL              NEW TASK ADAPTATION
(e.g., ResNet-50)

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ INPUT LAYER     â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’â”‚ SAME INPUT      â”‚
â”‚ (224Ã—224Ã—3)     â”‚            â”‚ (224Ã—224Ã—3)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                              â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â„ï¸ FROZEN   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ CONV BLOCK 1    â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ â”‚ CONV BLOCK 1    â”‚
â”‚ (112Ã—112Ã—64)    â”‚             â”‚ (112Ã—112Ã—64)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                              â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â„ï¸ FROZEN   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ CONV BLOCK 2    â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ â”‚ CONV BLOCK 2    â”‚
â”‚ (56Ã—56Ã—128)     â”‚             â”‚ (56Ã—56Ã—128)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                              â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â„ï¸ FROZEN   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ CONV BLOCK 3    â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ â”‚ CONV BLOCK 3    â”‚
â”‚ (28Ã—28Ã—256)     â”‚             â”‚ (28Ã—28Ã—256)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                              â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  ğŸ”¥ FINE-TUNE â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ CONV BLOCK 4    â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ â”‚ CONV BLOCK 4    â”‚
â”‚ (14Ã—14Ã—512)     â”‚             â”‚ (14Ã—14Ã—512)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                              â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  âœ‚ï¸ REMOVE    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ GLOBAL POOL     â”‚    â•±â•±â•±â•±â•±â•±â•±â•±â†’ â”‚ GLOBAL POOL     â”‚
â”‚ (2048)          â”‚              â”‚ (2048)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                              â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  âœ‚ï¸ REMOVE           â”‚
â”‚ DENSE (1000)    â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ImageNet Classesâ”‚              â”‚ ğŸ†• DENSE (512)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚ + ReLU + Dropoutâ”‚
                                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                          â”‚
                                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                 â”‚ ğŸ†• DENSE (256)   â”‚
                                 â”‚ + ReLU + Dropoutâ”‚
                                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                          â”‚
                                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                 â”‚ ğŸ†• OUTPUT (N)    â”‚
                                 â”‚ Your Classes    â”‚
                                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

TRANSFER LEARNING STRATEGIES:

1. FEATURE EXTRACTION          2. FINE-TUNING              3. FULL TRAINING
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚â„ï¸ Freeze all layers  â”‚       â”‚â„ï¸ Freeze early layersâ”‚     â”‚ğŸ”¥ Train everything   â”‚
â”‚ğŸ†• Train classifier   â”‚       â”‚ğŸ”¥ Unfreeze top layersâ”‚     â”‚ğŸ² Random weights     â”‚
â”‚âš¡ Fast & Stable      â”‚       â”‚ğŸ¯ Better performance â”‚     â”‚ğŸ’ª Maximum flexibilityâ”‚
â”‚ğŸ“Š Small datasets     â”‚       â”‚ğŸ“ˆ Medium datasets    â”‚     â”‚ğŸ—ƒï¸ Large datasets     â”‚
â”‚ğŸ¯ 85-90% accuracy    â”‚       â”‚ğŸ¯ 90-95% accuracy    â”‚     â”‚ğŸ¯ Variable accuracy  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

PERFORMANCE COMPARISON:
Method              Time    Data Needed    Typical Accuracy
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Feature Extraction   Fast    Small (100s)     85-90%
Fine-tuning         Medium   Medium (1000s)   90-95%
From Scratch        Slow     Large (10000s)   Variable
```

---

## 5. Architecture Comparison Summary

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ NETWORK     â”‚ BEST FOR     â”‚ KEY FEATURE  â”‚ PARAMETERS  â”‚ APPLICATIONS â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ DNN/MLP     â”‚ Tabular Data â”‚ Fully Conn.  â”‚ High        â”‚ Classificationâ”‚
â”‚             â”‚ Structured   â”‚ Universal    â”‚             â”‚ Regression   â”‚
â”‚             â”‚ Features     â”‚ Approximator â”‚             â”‚ Prediction   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ CNN         â”‚ Images       â”‚ Local Conn.  â”‚ Efficient   â”‚ Computer     â”‚
â”‚             â”‚ Spatial Data â”‚ Convolution  â”‚ Parameters  â”‚ Vision       â”‚
â”‚             â”‚ Grid-like    â”‚ Translation  â”‚             â”‚ Image Class. â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ RNN/LSTM    â”‚ Sequences    â”‚ Memory       â”‚ Moderate    â”‚ NLP          â”‚
â”‚             â”‚ Time Series  â”‚ Temporal     â”‚             â”‚ Speech       â”‚
â”‚             â”‚ Text         â”‚ Dependencies â”‚             â”‚ Time Series  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Transfer    â”‚ Limited Data â”‚ Pre-trained  â”‚ Low         â”‚ Fine-tuning  â”‚
â”‚ Learning    â”‚ New Domains  â”‚ Features     â”‚ Training    â”‚ Domain Adapt â”‚
â”‚             â”‚ Fast Deploy  â”‚ Knowledge    â”‚ Time        â”‚ Quick Deploy â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

CHOOSING THE RIGHT ARCHITECTURE:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Data Type           â†’ Recommended Architecture                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ğŸ“Š Tabular/Numeric  â†’ Deep Neural Network (MLP)                        â”‚
â”‚ ğŸ–¼ï¸ Images            â†’ Convolutional Neural Network (CNN)               â”‚
â”‚ ğŸ“ Text/Sequences    â†’ Recurrent Neural Network (RNN/LSTM)             â”‚
â”‚ ğŸµ Time Series       â†’ RNN/LSTM or 1D CNN                              â”‚
â”‚ ğŸ”„ Limited Data      â†’ Transfer Learning                                â”‚
â”‚ ğŸ¯ Multi-modal       â†’ Ensemble/Hybrid Architectures                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Key Takeaways for Week 5 - Regularization Context

### Overfitting in Different Architectures:

**DNNs**: Use Dropout, L2 regularization, Early stopping
**CNNs**: Data augmentation, Dropout, Batch normalization
**RNNs**: Dropout (input/output), Gradient clipping, Regularization
**Transfer Learning**: Inherently regularized through pre-trained features

### Best Practices:
1. **Start Simple**: Begin with basic architecture, add complexity gradually
2. **Monitor Validation**: Track train vs. validation performance gap
3. **Regularization Mix**: Combine multiple techniques (L2 + Dropout + Early stopping)
4. **Architecture Choice**: Match network type to data characteristics
5. **Transfer First**: Try transfer learning before training from scratch

---

*Created for 21CSE558T Deep Neural Network Architectures*
*M.Tech Course - SRM University*
*Week 5, Day 4: Overfitting & Regularization Tutorial*