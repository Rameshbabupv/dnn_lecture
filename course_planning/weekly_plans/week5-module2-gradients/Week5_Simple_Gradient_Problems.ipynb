{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "title"
   },
   "source": [
    "# üéØ Week 5: Simple Gradient Problems Demo\n",
    "\n",
    "**Deep Neural Network Architectures (21CSE558T)**  \n",
    "**Module 2: Optimization & Regularization**\n",
    "\n",
    "---\n",
    "\n",
    "## üìö What You'll Learn (in 15 minutes!):\n",
    "\n",
    "1. **The Problem**: Why deep networks don't train well\n",
    "2. **The Cause**: Vanishing gradients in sigmoid networks\n",
    "3. **The Solution**: Use ReLU instead!\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/your-repo/blob/main/Week5_Simple_Gradient_Problems.ipynb)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## üîß Setup (Run this first!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imports"
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Set random seed for reproducible results\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"‚úÖ TensorFlow version: {tf.__version__}\")\n",
    "print(\"‚úÖ Ready to explore gradient problems!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "problem"
   },
   "source": [
    "---\n",
    "\n",
    "# üö® Part 1: The Problem\n",
    "\n",
    "## Why do deep networks struggle to train?\n",
    "\n",
    "Let's see what happens to gradients in a deep sigmoid network vs a ReLU network.\n",
    "\n",
    "**We'll check the actual gradient numbers!** üëÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gradient_demo"
   },
   "outputs": [],
   "source": [
    "def check_gradients(activation_type):\n",
    "    \"\"\"Check gradient magnitudes in a deep network\"\"\"\n",
    "    \n",
    "    print(f\"\\nüîç {activation_type.upper()} NETWORK:\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Create a 5-layer network\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(8, activation=activation_type, input_shape=(2,)),\n",
    "        tf.keras.layers.Dense(8, activation=activation_type),\n",
    "        tf.keras.layers.Dense(8, activation=activation_type),\n",
    "        tf.keras.layers.Dense(8, activation=activation_type),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')  # Output layer always sigmoid\n",
    "    ])\n",
    "    \n",
    "    # Simple input and target\n",
    "    X = tf.constant([[1.0, 2.0]])\n",
    "    y_true = tf.constant([[0.8]])\n",
    "    \n",
    "    # Calculate gradients\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(X)\n",
    "        loss = tf.square(y_pred - y_true)\n",
    "    \n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    \n",
    "    # Show gradient magnitudes for each layer\n",
    "    for i, grad in enumerate(gradients):\n",
    "        if i % 2 == 0:  # Only weights (skip biases)\n",
    "            layer_num = i // 2 + 1\n",
    "            grad_magnitude = tf.reduce_mean(tf.abs(grad)).numpy()\n",
    "            \n",
    "            print(f\"Layer {layer_num}: {grad_magnitude:.8f}\", end=\"\")\n",
    "            \n",
    "            # Add interpretation\n",
    "            if grad_magnitude < 0.0001:\n",
    "                print(\" ‚ö†Ô∏è TOO SMALL! (vanishing)\")\n",
    "            elif grad_magnitude > 1.0:\n",
    "                print(\" ‚ö†Ô∏è TOO LARGE! (exploding)\")\n",
    "            else:\n",
    "                print(\" ‚úÖ Good magnitude\")\n",
    "    \n",
    "    return [tf.reduce_mean(tf.abs(grad)).numpy() for i, grad in enumerate(gradients) if i % 2 == 0]\n",
    "\n",
    "# Check both networks\n",
    "print(\"üß™ GRADIENT EXPERIMENT\")\n",
    "print(\"Comparing gradient magnitudes in deep networks...\")\n",
    "\n",
    "sigmoid_grads = check_gradients('sigmoid')\n",
    "relu_grads = check_gradients('relu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "interpretation"
   },
   "source": [
    "### ü§î What do you notice?\n",
    "\n",
    "**Sigmoid Network**: Gradients get smaller and smaller (vanishing!)\n",
    "\n",
    "**ReLU Network**: Gradients stay reasonably sized\n",
    "\n",
    "**Why this matters**: Small gradients = slow/no learning in early layers!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "visual"
   },
   "source": [
    "---\n",
    "\n",
    "# üìä Part 2: Visual Comparison\n",
    "\n",
    "Let's see this gradient problem visually!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "visualize"
   },
   "outputs": [],
   "source": [
    "# Create visualization of gradient flow\n",
    "def visualize_gradient_problem():\n",
    "    \"\"\"Simple visualization of vanishing gradients\"\"\"\n",
    "    \n",
    "    # Simulate gradient decay through layers\n",
    "    layers = np.array([1, 2, 3, 4, 5])\n",
    "    \n",
    "    # Sigmoid: gradients shrink exponentially\n",
    "    sigmoid_gradients = np.array([0.25**i for i in range(5)])\n",
    "    \n",
    "    # ReLU: gradients decay more slowly\n",
    "    relu_gradients = np.array([0.8**i for i in range(5)])\n",
    "    \n",
    "    # Create side-by-side comparison\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Sigmoid plot\n",
    "    ax1.plot(layers, sigmoid_gradients, 'r-o', linewidth=4, markersize=10, label='Gradient Magnitude')\n",
    "    ax1.set_title('üò∞ Sigmoid: Vanishing Gradients', fontsize=16, fontweight='bold')\n",
    "    ax1.set_xlabel('Layer Depth', fontsize=14)\n",
    "    ax1.set_ylabel('Gradient Magnitude', fontsize=14)\n",
    "    ax1.set_yscale('log')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_ylim(1e-6, 1)\n",
    "    \n",
    "    # Add annotations\n",
    "    ax1.annotate('Gets very small!', xy=(5, sigmoid_gradients[-1]), xytext=(3.5, 1e-4),\n",
    "                arrowprops=dict(arrowstyle='->', color='red', lw=2),\n",
    "                fontsize=12, color='red', fontweight='bold')\n",
    "    \n",
    "    # ReLU plot\n",
    "    ax2.plot(layers, relu_gradients, 'b-s', linewidth=4, markersize=10, label='Gradient Magnitude')\n",
    "    ax2.set_title('üòä ReLU: Better Gradient Flow', fontsize=16, fontweight='bold')\n",
    "    ax2.set_xlabel('Layer Depth', fontsize=14)\n",
    "    ax2.set_ylabel('Gradient Magnitude', fontsize=14)\n",
    "    ax2.set_yscale('log')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.set_ylim(1e-6, 1)\n",
    "    \n",
    "    # Add annotations\n",
    "    ax2.annotate('Still reasonable!', xy=(5, relu_gradients[-1]), xytext=(3, 1e-2),\n",
    "                arrowprops=dict(arrowstyle='->', color='blue', lw=2),\n",
    "                fontsize=12, color='blue', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print the numbers\n",
    "    print(\"üìä GRADIENT COMPARISON:\")\n",
    "    print(f\"Sigmoid Layer 5: {sigmoid_gradients[-1]:.8f} (TINY!)\")\n",
    "    print(f\"ReLU Layer 5:    {relu_gradients[-1]:.4f} (Much better!)\")\n",
    "    print(f\"\\nReLU gradients are {relu_gradients[-1]/sigmoid_gradients[-1]:.0f}x larger!\")\n",
    "\n",
    "# Show the visualization\n",
    "visualize_gradient_problem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "why"
   },
   "source": [
    "---\n",
    "\n",
    "# üß† Part 3: Why Does This Happen?\n",
    "\n",
    "## The Mathematical Reason (Simple!)\n",
    "\n",
    "**Sigmoid derivative**: Maximum value is 0.25  \n",
    "**Chain rule**: Multiply derivatives through layers  \n",
    "**Result**: 0.25 √ó 0.25 √ó 0.25 √ó ... = Very small number!\n",
    "\n",
    "**ReLU derivative**: Either 0 or 1 (usually 1)  \n",
    "**Result**: 1 √ó 1 √ó 1 √ó ... = Still 1!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "activation_comparison"
   },
   "outputs": [],
   "source": [
    "# Show activation function derivatives\n",
    "def compare_activations():\n",
    "    \"\"\"Compare sigmoid vs ReLU derivatives\"\"\"\n",
    "    \n",
    "    x = np.linspace(-3, 3, 1000)\n",
    "    \n",
    "    # Sigmoid and its derivative\n",
    "    sigmoid = 1 / (1 + np.exp(-x))\n",
    "    sigmoid_deriv = sigmoid * (1 - sigmoid)\n",
    "    \n",
    "    # ReLU and its derivative  \n",
    "    relu = np.maximum(0, x)\n",
    "    relu_deriv = (x > 0).astype(float)\n",
    "    \n",
    "    # Plot comparison\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Sigmoid function\n",
    "    axes[0,0].plot(x, sigmoid, 'r-', linewidth=3)\n",
    "    axes[0,0].set_title('Sigmoid Function', fontsize=14, fontweight='bold')\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Sigmoid derivative\n",
    "    axes[0,1].plot(x, sigmoid_deriv, 'r--', linewidth=3)\n",
    "    axes[0,1].set_title('Sigmoid Derivative (Max = 0.25)', fontsize=14, fontweight='bold')\n",
    "    axes[0,1].axhline(y=0.25, color='red', linestyle=':', alpha=0.7, label='Max = 0.25')\n",
    "    axes[0,1].legend()\n",
    "    axes[0,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # ReLU function\n",
    "    axes[1,0].plot(x, relu, 'b-', linewidth=3)\n",
    "    axes[1,0].set_title('ReLU Function', fontsize=14, fontweight='bold')\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # ReLU derivative\n",
    "    axes[1,1].plot(x, relu_deriv, 'b--', linewidth=3)\n",
    "    axes[1,1].set_title('ReLU Derivative (0 or 1)', fontsize=14, fontweight='bold')\n",
    "    axes[1,1].axhline(y=1, color='blue', linestyle=':', alpha=0.7, label='Active = 1')\n",
    "    axes[1,1].legend()\n",
    "    axes[1,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"üîç KEY INSIGHT:\")\n",
    "    print(\"üìâ Sigmoid derivative ‚â§ 0.25 (always shrinks gradients)\")\n",
    "    print(\"üìà ReLU derivative = 1 when active (preserves gradients)\")\n",
    "\n",
    "compare_activations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "solution"
   },
   "source": [
    "---\n",
    "\n",
    "# ‚úÖ Part 4: The Simple Solution!\n",
    "\n",
    "## Rule #1: Replace sigmoid with ReLU in hidden layers\n",
    "\n",
    "Let's see how easy the fix is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "solution_demo"
   },
   "outputs": [],
   "source": [
    "print(\"üõ†Ô∏è THE SIMPLE FIX\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"‚ùå BROKEN CODE:\")\n",
    "print(\"\"\"\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='sigmoid'),  # BAD!\n",
    "    tf.keras.layers.Dense(32, activation='sigmoid'),  # BAD!\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')    # Output is OK\n",
    "])\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n‚úÖ FIXED CODE:\")\n",
    "print(\"\"\"\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu'),     # GOOD!\n",
    "    tf.keras.layers.Dense(32, activation='relu'),     # GOOD!\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')    # Output unchanged\n",
    "])\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nüéØ SIMPLE RULES:\")\n",
    "print(\"1. Hidden layers: Use 'relu'\")\n",
    "print(\"2. Output layer: Use 'sigmoid' for binary, 'softmax' for multi-class\")\n",
    "print(\"3. That's it! üéâ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "training_comparison"
   },
   "source": [
    "---\n",
    "\n",
    "# üèãÔ∏è Part 5: Training Comparison\n",
    "\n",
    "Let's train both networks and see the difference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "training_demo"
   },
   "outputs": [],
   "source": [
    "# Generate simple dataset\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Create dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=10, n_classes=2, random_state=42)\n",
    "\n",
    "print(\"üìä Training both networks on same data...\")\n",
    "print(f\"Dataset: {X.shape[0]} samples, {X.shape[1]} features\")\n",
    "\n",
    "# Create both models\n",
    "model_sigmoid = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(32, activation='sigmoid', input_shape=(10,)),\n",
    "    tf.keras.layers.Dense(16, activation='sigmoid'),\n",
    "    tf.keras.layers.Dense(8, activation='sigmoid'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model_relu = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(32, activation='relu', input_shape=(10,)),\n",
    "    tf.keras.layers.Dense(16, activation='relu'), \n",
    "    tf.keras.layers.Dense(8, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile both models\n",
    "for model in [model_sigmoid, model_relu]:\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train both models\n",
    "print(\"\\nüèÉ Training Sigmoid model...\")\n",
    "history_sigmoid = model_sigmoid.fit(X, y, epochs=20, batch_size=32, verbose=0, validation_split=0.2)\n",
    "\n",
    "print(\"üèÉ Training ReLU model...\")\n",
    "history_relu = model_relu.fit(X, y, epochs=20, batch_size=32, verbose=0, validation_split=0.2)\n",
    "\n",
    "print(\"‚úÖ Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "training_results"
   },
   "outputs": [],
   "source": [
    "# Plot training results\n",
    "def plot_training_comparison():\n",
    "    \"\"\"Compare training performance\"\"\"\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Plot loss\n",
    "    ax1.plot(history_sigmoid.history['loss'], 'r-', linewidth=3, label='Sigmoid (Bad)', alpha=0.8)\n",
    "    ax1.plot(history_relu.history['loss'], 'b-', linewidth=3, label='ReLU (Good)')\n",
    "    ax1.set_title('Training Loss Comparison', fontsize=16, fontweight='bold')\n",
    "    ax1.set_xlabel('Epoch', fontsize=14)\n",
    "    ax1.set_ylabel('Loss', fontsize=14)\n",
    "    ax1.legend(fontsize=12)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot accuracy\n",
    "    ax2.plot(history_sigmoid.history['accuracy'], 'r-', linewidth=3, label='Sigmoid (Bad)', alpha=0.8)\n",
    "    ax2.plot(history_relu.history['accuracy'], 'b-', linewidth=3, label='ReLU (Good)')\n",
    "    ax2.set_title('Training Accuracy Comparison', fontsize=16, fontweight='bold')\n",
    "    ax2.set_xlabel('Epoch', fontsize=14)\n",
    "    ax2.set_ylabel('Accuracy', fontsize=14)\n",
    "    ax2.legend(fontsize=12)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print final results\n",
    "    print(\"üèÜ FINAL RESULTS:\")\n",
    "    print(f\"Sigmoid - Loss: {history_sigmoid.history['loss'][-1]:.4f}, Accuracy: {history_sigmoid.history['accuracy'][-1]:.4f}\")\n",
    "    print(f\"ReLU    - Loss: {history_relu.history['loss'][-1]:.4f}, Accuracy: {history_relu.history['accuracy'][-1]:.4f}\")\n",
    "    \n",
    "    if history_relu.history['accuracy'][-1] > history_sigmoid.history['accuracy'][-1]:\n",
    "        print(\"\\nüéâ ReLU network trained better!\")\n",
    "    else:\n",
    "        print(\"\\nü§î Results may vary, but ReLU usually wins with deeper networks!\")\n",
    "\n",
    "plot_training_comparison()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hands_on"
   },
   "source": [
    "---\n",
    "\n",
    "# üéÆ Part 6: Your Turn!\n",
    "\n",
    "## Interactive Exercise\n",
    "\n",
    "Try modifying the network below and see what happens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "student_exercise"
   },
   "outputs": [],
   "source": [
    "# üéØ STUDENT CHALLENGE: Fix this broken network!\n",
    "\n",
    "def create_student_network(activation1='sigmoid', activation2='sigmoid', activation3='sigmoid'):\n",
    "    \"\"\"Create a network with customizable activations\"\"\"\n",
    "    \n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(16, activation=activation1, input_shape=(10,)),\n",
    "        tf.keras.layers.Dense(8, activation=activation2),\n",
    "        tf.keras.layers.Dense(4, activation=activation3), \n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')  # Output layer - don't change!\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "print(\"üéÆ EXPERIMENT TIME!\")\n",
    "print(\"Try different combinations and see the gradient magnitudes:\")\n",
    "print()\n",
    "\n",
    "# Example 1: All sigmoid (broken)\n",
    "print(\"Example 1 - All Sigmoid (Broken):\")\n",
    "model1 = create_student_network('sigmoid', 'sigmoid', 'sigmoid')\n",
    "# Check gradients...\n",
    "X_test = tf.random.normal((1, 10))\n",
    "y_test = tf.constant([[0.5]])\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    pred = model1(X_test)\n",
    "    loss = tf.square(pred - y_test)\n",
    "\n",
    "grads = tape.gradient(loss, model1.trainable_variables)\n",
    "for i, g in enumerate(grads[::2]):  # Only weights\n",
    "    grad_mag = tf.reduce_mean(tf.abs(g)).numpy()\n",
    "    print(f\"  Layer {i+1}: {grad_mag:.8f}\")\n",
    "\n",
    "print(\"\\nüí° YOUR TURN: Try changing the activations below!\")\n",
    "print(\"Hint: Replace 'sigmoid' with 'relu' in hidden layers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "student_solution"
   },
   "outputs": [],
   "source": [
    "# üéØ YOUR SOLUTION: Modify the activations here!\n",
    "\n",
    "print(\"üõ†Ô∏è Your Fixed Network:\")\n",
    "model_fixed = create_student_network(\n",
    "    activation1='relu',    # ‚Üê Change this!\n",
    "    activation2='relu',    # ‚Üê Change this! \n",
    "    activation3='relu'     # ‚Üê Change this!\n",
    ")\n",
    "\n",
    "# Check your gradients\n",
    "with tf.GradientTape() as tape:\n",
    "    pred = model_fixed(X_test)\n",
    "    loss = tf.square(pred - y_test)\n",
    "\n",
    "grads = tape.gradient(loss, model_fixed.trainable_variables)\n",
    "for i, g in enumerate(grads[::2]):  # Only weights\n",
    "    grad_mag = tf.reduce_mean(tf.abs(g)).numpy()\n",
    "    status = \"‚úÖ Good!\" if grad_mag > 0.001 else \"‚ö†Ô∏è Still small\"\n",
    "    print(f\"  Layer {i+1}: {grad_mag:.8f} {status}\")\n",
    "\n",
    "print(\"\\nüéâ Great job! You've learned to fix vanishing gradients!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "summary"
   },
   "source": [
    "---\n",
    "\n",
    "# üéì Summary: What You Learned\n",
    "\n",
    "## üîç **The Problem**\n",
    "- Deep sigmoid networks have **vanishing gradients**\n",
    "- Gradients become too small to train early layers\n",
    "- Networks learn slowly or not at all\n",
    "\n",
    "## üß† **The Cause**  \n",
    "- Sigmoid derivative ‚â§ 0.25\n",
    "- Chain rule: 0.25 √ó 0.25 √ó 0.25 = tiny number\n",
    "- ReLU derivative = 1 (when active)\n",
    "\n",
    "## ‚úÖ **The Solution**\n",
    "- **Use ReLU in hidden layers** (not sigmoid)\n",
    "- Keep sigmoid/softmax for output layer\n",
    "- That's it! One simple change fixes everything\n",
    "\n",
    "## üí° **Key Takeaway**\n",
    "**\"Replace sigmoid with ReLU in hidden layers\"** - Remember this rule!\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ **Next Steps**\n",
    "\n",
    "1. **Practice**: Try this on your own datasets\n",
    "2. **Experiment**: Test with different network depths\n",
    "3. **Learn more**: Explore other activation functions (LeakyReLU, ELU, etc.)\n",
    "\n",
    "## üìö **Additional Reading**\n",
    "- [Understanding ReLU](https://en.wikipedia.org/wiki/Rectifier_(neural_networks))\n",
    "- [Activation Functions Explained](https://www.tensorflow.org/api_docs/python/tf/keras/activations)\n",
    "\n",
    "---\n",
    "\n",
    "### üéâ **Congratulations!** \n",
    "You now understand one of the most important concepts in deep learning!\n",
    "\n",
    "*Remember: Deep learning can be simple when you understand the core concepts.* üòä"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}