{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß™ Concept 4: Activation Functions Analysis\n",
    "\n",
    "## Deep Neural Network Architectures - Week 5\n",
    "**Module:** 2 - Optimization and Regularization  \n",
    "**Topic:** Comparing Activation Functions and Their Impact on Gradients\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Learning Objectives\n",
    "By the end of this notebook, you will:\n",
    "1. **Analyze** mathematical properties of different activation functions\n",
    "2. **Compare** gradient flow characteristics across activations\n",
    "3. **Understand** why ReLU revolutionized deep learning\n",
    "4. **Choose** appropriate activations for different scenarios\n",
    "\n",
    "---\n",
    "\n",
    "## üö∞ The Water Flow Analogy\n",
    "\n",
    "Think of activation functions as different types of pipes in a water system:\n",
    "\n",
    "**Sigmoid Pipe:**\n",
    "- **Adjustable valve** that reduces flow\n",
    "- **Maximum flow:** 25% of input pressure\n",
    "- **Multiple pipes:** Flow reduces to almost nothing\n",
    "\n",
    "**ReLU Pipe:**\n",
    "- **Check valve:** Either fully open (100% flow) or fully closed (0% flow)\n",
    "- **No reduction:** When open, full pressure passes through\n",
    "- **Result:** Strong flow maintained through many pipes\n",
    "\n",
    "---\n",
    "\n",
    "## üíª Mathematical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"Set2\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sigmoid():\n",
    "    \"\"\"Comprehensive analysis of sigmoid activation function\"\"\"\n",
    "    \n",
    "    print(\"üîç SIGMOID FUNCTION ANALYSIS\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    x = np.linspace(-6, 6, 1000)\n",
    "    sigmoid = 1 / (1 + np.exp(-x))\n",
    "    sigmoid_deriv = sigmoid * (1 - sigmoid)\n",
    "\n",
    "    # Create comprehensive visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "    # Plot 1: Sigmoid function\n",
    "    axes[0, 0].plot(x, sigmoid, 'b-', linewidth=3, label='œÉ(x) = 1/(1 + e‚ÅªÀ£)')\n",
    "    axes[0, 0].axhline(y=0.5, color='gray', linestyle='--', alpha=0.5)\n",
    "    axes[0, 0].axvline(x=0, color='gray', linestyle='--', alpha=0.5)\n",
    "    axes[0, 0].set_title('Sigmoid Function', fontsize=14, fontweight='bold')\n",
    "    axes[0, 0].set_xlabel('Input (x)')\n",
    "    axes[0, 0].set_ylabel('Output œÉ(x)')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    axes[0, 0].set_ylim(-0.1, 1.1)\n",
    "\n",
    "    # Plot 2: Sigmoid derivative\n",
    "    axes[0, 1].plot(x, sigmoid_deriv, 'r-', linewidth=3, label=\"œÉ'(x) = œÉ(x)(1 - œÉ(x))\")\n",
    "    axes[0, 1].axhline(y=0.25, color='k', linestyle='--', alpha=0.7, label='Maximum = 0.25')\n",
    "    axes[0, 1].axvline(x=0, color='gray', linestyle='--', alpha=0.5)\n",
    "    axes[0, 1].set_title('Sigmoid Derivative\\n(The Gradient Killer!)', fontsize=14, fontweight='bold')\n",
    "    axes[0, 1].set_xlabel('Input (x)')\n",
    "    axes[0, 1].set_ylabel(\"Derivative œÉ'(x)\")\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Gradient reduction through layers\n",
    "    layers = range(1, 11)\n",
    "    gradient_reduction = [0.25**i for i in layers]\n",
    "    axes[1, 0].semilogy(layers, gradient_reduction, 'ro-', linewidth=3, markersize=8)\n",
    "    axes[1, 0].set_title('Gradient Reduction Through Layers', fontsize=14, fontweight='bold')\n",
    "    axes[1, 0].set_xlabel('Layer Depth')\n",
    "    axes[1, 0].set_ylabel('Gradient Magnitude (log scale)')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add annotations for key points\n",
    "    axes[1, 0].annotate(f'Layer 5: {0.25**5:.6f}', xy=(5, 0.25**5), \n",
    "                       xytext=(7, 0.01), arrowprops=dict(arrowstyle='->', color='red'))\n",
    "    axes[1, 0].annotate(f'Layer 10: {0.25**10:.2e}', xy=(10, 0.25**10), \n",
    "                       xytext=(8, 1e-5), arrowprops=dict(arrowstyle='->', color='red'))\n",
    "    \n",
    "    # Plot 4: Saturation zones\n",
    "    saturation_left = sigmoid_deriv[x < -3]\n",
    "    saturation_right = sigmoid_deriv[x > 3]\n",
    "    active_zone = sigmoid_deriv[(-3 <= x) & (x <= 3)]\n",
    "    \n",
    "    x_left = x[x < -3]\n",
    "    x_right = x[x > 3]\n",
    "    x_active = x[(-3 <= x) & (x <= 3)]\n",
    "    \n",
    "    axes[1, 1].fill_between(x_left, 0, saturation_left, alpha=0.3, color='red', label='Left Saturation')\n",
    "    axes[1, 1].fill_between(x_right, 0, saturation_right, alpha=0.3, color='red', label='Right Saturation')\n",
    "    axes[1, 1].fill_between(x_active, 0, active_zone, alpha=0.3, color='green', label='Active Zone')\n",
    "    axes[1, 1].plot(x, sigmoid_deriv, 'b-', linewidth=2)\n",
    "    axes[1, 1].set_title('Sigmoid Saturation Zones', fontsize=14, fontweight='bold')\n",
    "    axes[1, 1].set_xlabel('Input (x)')\n",
    "    axes[1, 1].set_ylabel(\"Derivative œÉ'(x)\")\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Print numerical analysis\n",
    "    print(\"\\nüìä NUMERICAL ANALYSIS:\")\n",
    "    print(f\"Maximum derivative: {np.max(sigmoid_deriv):.6f} (at x = 0)\")\n",
    "    print(f\"Derivative at x = ¬±3: {sigmoid_deriv[np.argmin(np.abs(x-3))]:.6f}\")\n",
    "    print(f\"Derivative at x = ¬±6: {sigmoid_deriv[np.argmin(np.abs(x-6))]:.6f}\")\n",
    "    \n",
    "    print(\"\\nüíÄ GRADIENT DEATH PROGRESSION:\")\n",
    "    for layer in [1, 3, 5, 7, 10]:\n",
    "        reduction = 0.25**layer\n",
    "        percentage = reduction * 100\n",
    "        print(f\"Layer {layer:2d}: {reduction:.2e} ({percentage:.4f}% of original)\")\n",
    "    \n",
    "    print(\"\\nüö® CRITICAL INSIGHT:\")\n",
    "    print(\"After just 5 sigmoid layers, gradient is 1000x smaller!\")\n",
    "    print(\"Early layers receive virtually no learning signal.\")\n",
    "\n",
    "# Run sigmoid analysis\n",
    "analyze_sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_relu_variants():\n",
    "    \"\"\"Compare different ReLU variants and their properties\"\"\"\n",
    "    \n",
    "    print(\"\\nüî• ReLU VARIANTS COMPARISON\")\n",
    "    print(\"=\" * 40)\n",
    "\n",
    "    x = np.linspace(-3, 3, 1000)\n",
    "\n",
    "    # Define activation functions\n",
    "    activations = {\n",
    "        'ReLU': {\n",
    "            'func': np.maximum(0, x),\n",
    "            'derivative': np.where(x > 0, 1, 0),\n",
    "            'description': 'Standard ReLU: f(x) = max(0, x)',\n",
    "            'pros': ['Simple', 'Fast', 'No vanishing gradients'],\n",
    "            'cons': ['Dying ReLU problem', 'Not differentiable at 0']\n",
    "        },\n",
    "        'Leaky ReLU': {\n",
    "            'func': np.where(x > 0, x, 0.01 * x),\n",
    "            'derivative': np.where(x > 0, 1, 0.01),\n",
    "            'description': 'Leaky ReLU: f(x) = max(0.01x, x)',\n",
    "            'pros': ['Fixes dying ReLU', 'Always has gradient'],\n",
    "            'cons': ['Still piecewise linear', 'Hyperparameter (slope)']\n",
    "        },\n",
    "        'ELU': {\n",
    "            'func': np.where(x > 0, x, np.exp(x) - 1),\n",
    "            'derivative': np.where(x > 0, 1, np.exp(x)),\n",
    "            'description': 'ELU: f(x) = x if x>0, else Œ±(eÀ£-1)',\n",
    "            'pros': ['Smooth', 'Negative outputs', 'Self-normalizing'],\n",
    "            'cons': ['Computationally expensive', 'Saturation for large negative']\n",
    "        },\n",
    "        'Swish': {\n",
    "            'func': x * (1 / (1 + np.exp(-x))),\n",
    "            'derivative': (1 / (1 + np.exp(-x))) + x * (1 / (1 + np.exp(-x))) * (1 - (1 / (1 + np.exp(-x)))),\n",
    "            'description': 'Swish: f(x) = x * œÉ(x)',\n",
    "            'pros': ['Smooth', 'Self-gated', 'Unbounded above'],\n",
    "            'cons': ['Computational overhead', 'Can vanish for large negative']\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Create comprehensive comparison plot\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "    colors = ['blue', 'green', 'red', 'purple']\n",
    "\n",
    "    for i, (name, props) in enumerate(activations.items()):\n",
    "        color = colors[i]\n",
    "        \n",
    "        # Top row: Activation functions\n",
    "        axes[0, i].plot(x, props['func'], color=color, linewidth=3, label=name)\n",
    "        axes[0, i].axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "        axes[0, i].axvline(x=0, color='gray', linestyle='--', alpha=0.5)\n",
    "        axes[0, i].set_title(f'{name}\\n{props[\"description\"]}', fontsize=12, fontweight='bold')\n",
    "        axes[0, i].set_xlabel('x')\n",
    "        axes[0, i].set_ylabel(f'{name}(x)')\n",
    "        axes[0, i].grid(True, alpha=0.3)\n",
    "        axes[0, i].legend()\n",
    "        \n",
    "        # Bottom row: Derivatives\n",
    "        axes[1, i].plot(x, props['derivative'], color=color, linewidth=3, label=f\"{name} derivative\")\n",
    "        axes[1, i].axhline(y=1, color='orange', linestyle='--', alpha=0.7, label='Gradient = 1')\n",
    "        axes[1, i].axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "        axes[1, i].axvline(x=0, color='gray', linestyle='--', alpha=0.5)\n",
    "        axes[1, i].set_title(f'{name} Derivative', fontsize=12, fontweight='bold')\n",
    "        axes[1, i].set_xlabel('x')\n",
    "        axes[1, i].set_ylabel(f\"{name}'(x)\")\n",
    "        axes[1, i].grid(True, alpha=0.3)\n",
    "        axes[1, i].legend()\n",
    "        \n",
    "        # Set consistent y-limits for derivatives\n",
    "        axes[1, i].set_ylim(-0.1, 2.0)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print detailed comparison\n",
    "    print(\"\\nüìã DETAILED COMPARISON:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for name, props in activations.items():\n",
    "        print(f\"\\nüîπ {name.upper()}:\")\n",
    "        print(f\"   Description: {props['description']}\")\n",
    "        print(f\"   ‚úÖ Pros: {', '.join(props['pros'])}\")\n",
    "        print(f\"   ‚ùå Cons: {', '.join(props['cons'])}\")\n",
    "        \n",
    "        # Calculate key statistics\n",
    "        max_deriv = np.max(props['derivative'])\n",
    "        min_deriv = np.min(props['derivative'])\n",
    "        mean_deriv = np.mean(props['derivative'])\n",
    "        \n",
    "        print(f\"   üìä Derivative stats: Max={max_deriv:.3f}, Min={min_deriv:.3f}, Mean={mean_deriv:.3f}\")\n",
    "\n",
    "# Run ReLU variants comparison\n",
    "compare_relu_variants()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def practical_activation_comparison():\n",
    "    \"\"\"Compare activations in actual neural networks\"\"\"\n",
    "    \n",
    "    print(\"\\nüß™ PRACTICAL NETWORK COMPARISON\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Create networks with different activations\n",
    "    activations_to_test = ['sigmoid', 'tanh', 'relu', 'elu']\n",
    "    models = {}\n",
    "    \n",
    "    for activation in activations_to_test:\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(64, activation=activation, input_shape=(10,)),\n",
    "            tf.keras.layers.Dense(64, activation=activation),\n",
    "            tf.keras.layers.Dense(64, activation=activation),\n",
    "            tf.keras.layers.Dense(64, activation=activation),\n",
    "            tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "        ], name=f'{activation.title()}Network')\n",
    "        models[activation] = model\n",
    "    \n",
    "    # Test data\n",
    "    X_test = tf.random.normal((100, 10))\n",
    "    y_test = tf.random.uniform((100, 1))\n",
    "    \n",
    "    # Analyze gradient flow for each activation\n",
    "    results = {}\n",
    "    \n",
    "    for activation, model in models.items():\n",
    "        print(f\"\\nAnalyzing {activation.upper()} network...\")\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = model(X_test)\n",
    "            loss = tf.reduce_mean(tf.square(predictions - y_test))\n",
    "        \n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        \n",
    "        # Calculate gradient norms\n",
    "        grad_norms = [tf.norm(g).numpy() for g in gradients if g is not None]\n",
    "        \n",
    "        # Calculate statistics\n",
    "        results[activation] = {\n",
    "            'gradient_norms': grad_norms[::2],  # Only weights, skip biases\n",
    "            'min_gradient': min(grad_norms),\n",
    "            'max_gradient': max(grad_norms),\n",
    "            'mean_gradient': np.mean(grad_norms),\n",
    "            'std_gradient': np.std(grad_norms),\n",
    "            'vanished_layers': sum(1 for g in grad_norms if g < 1e-6),\n",
    "            'weak_layers': sum(1 for g in grad_norms if g < 1e-4),\n",
    "            'loss': loss.numpy()\n",
    "        }\n",
    "        \n",
    "        print(f\"  Gradient range: {results[activation]['min_gradient']:.2e} to {results[activation]['max_gradient']:.2e}\")\n",
    "        print(f\"  Vanished layers: {results[activation]['vanished_layers']}/5\")\n",
    "        print(f\"  Loss: {results[activation]['loss']:.6f}\")\n",
    "    \n",
    "    return results, models\n",
    "\n",
    "# Run practical comparison\n",
    "comparison_results, comparison_models = practical_activation_comparison()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize practical comparison results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "activations = list(comparison_results.keys())\n",
    "colors = ['red', 'orange', 'green', 'blue']\n",
    "\n",
    "# Plot 1: Gradient magnitudes by layer and activation\n",
    "ax1 = axes[0, 0]\n",
    "x_positions = np.arange(5)  # 5 layers\n",
    "width = 0.2\n",
    "\n",
    "for i, (activation, color) in enumerate(zip(activations, colors)):\n",
    "    grad_norms = comparison_results[activation]['gradient_norms']\n",
    "    ax1.bar(x_positions + i * width, grad_norms, width, \n",
    "            label=activation.title(), color=color, alpha=0.7)\n",
    "\n",
    "ax1.set_xlabel('Layer')\n",
    "ax1.set_ylabel('Gradient Magnitude')\n",
    "ax1.set_title('Gradient Magnitudes by Layer and Activation')\n",
    "ax1.set_yscale('log')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_xticks(x_positions + width * 1.5)\n",
    "ax1.set_xticklabels([f'Layer {i+1}' for i in range(5)])\n",
    "\n",
    "# Plot 2: Vanished layers comparison\n",
    "ax2 = axes[0, 1]\n",
    "vanished_counts = [comparison_results[act]['vanished_layers'] for act in activations]\n",
    "bars = ax2.bar(activations, vanished_counts, color=colors, alpha=0.7)\n",
    "ax2.set_ylabel('Number of Vanished Layers')\n",
    "ax2.set_title('Vanished Layers by Activation')\n",
    "ax2.set_ylim(0, 5)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, value in zip(bars, vanished_counts):\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, value + 0.1, str(value), \n",
    "             ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Plot 3: Gradient statistics comparison\n",
    "ax3 = axes[1, 0]\n",
    "metrics = ['min_gradient', 'max_gradient', 'mean_gradient', 'std_gradient']\n",
    "metric_labels = ['Min', 'Max', 'Mean', 'Std']\n",
    "\n",
    "x_pos = np.arange(len(metrics))\n",
    "for i, (activation, color) in enumerate(zip(activations, colors)):\n",
    "    values = [comparison_results[activation][metric] for metric in metrics]\n",
    "    ax3.bar(x_pos + i * width, values, width, \n",
    "            label=activation.title(), color=color, alpha=0.7)\n",
    "\n",
    "ax3.set_xlabel('Metric')\n",
    "ax3.set_ylabel('Gradient Value (log scale)')\n",
    "ax3.set_title('Gradient Statistics Comparison')\n",
    "ax3.set_yscale('log')\n",
    "ax3.legend()\n",
    "ax3.set_xticks(x_pos + width * 1.5)\n",
    "ax3.set_xticklabels(metric_labels)\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Health score comparison\n",
    "ax4 = axes[1, 1]\n",
    "\n",
    "# Calculate health scores (higher is better)\n",
    "health_scores = []\n",
    "for activation in activations:\n",
    "    result = comparison_results[activation]\n",
    "    score = 0\n",
    "    \n",
    "    # No vanished layers: +3 points\n",
    "    if result['vanished_layers'] == 0:\n",
    "        score += 3\n",
    "    elif result['vanished_layers'] <= 1:\n",
    "        score += 1\n",
    "    \n",
    "    # Reasonable gradient range: +2 points\n",
    "    if result['min_gradient'] > 1e-5:\n",
    "        score += 2\n",
    "    elif result['min_gradient'] > 1e-6:\n",
    "        score += 1\n",
    "    \n",
    "    # No explosion: +2 points\n",
    "    if result['max_gradient'] < 10:\n",
    "        score += 2\n",
    "    elif result['max_gradient'] < 100:\n",
    "        score += 1\n",
    "    \n",
    "    # Stable gradients: +1 point\n",
    "    if result['std_gradient'] < result['mean_gradient']:\n",
    "        score += 1\n",
    "    \n",
    "    health_scores.append(score)\n",
    "\n",
    "bars = ax4.bar(activations, health_scores, color=colors, alpha=0.7)\n",
    "ax4.set_ylabel('Health Score (0-8)')\n",
    "ax4.set_title('Overall Gradient Health Score')\n",
    "ax4.set_ylim(0, 8)\n",
    "\n",
    "# Add value labels and health assessment\n",
    "health_labels = ['Critical', 'Poor', 'Fair', 'Good', 'Excellent']\n",
    "for bar, score in zip(bars, health_scores):\n",
    "    ax4.text(bar.get_x() + bar.get_width()/2, score + 0.1, f'{score}/8', \n",
    "             ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    if score >= 7:\n",
    "        health_label = 'Excellent'\n",
    "        color_code = 'üü¢'\n",
    "    elif score >= 5:\n",
    "        health_label = 'Good'\n",
    "        color_code = 'üü°'\n",
    "    elif score >= 3:\n",
    "        health_label = 'Fair'\n",
    "        color_code = 'üü†'\n",
    "    else:\n",
    "        health_label = 'Poor'\n",
    "        color_code = 'üî¥'\n",
    "    \n",
    "    ax4.text(bar.get_x() + bar.get_width()/2, score/2, f'{color_code}\\n{health_label}', \n",
    "             ha='center', va='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create activation function recommendation system\n",
    "def recommend_activation(problem_type, network_depth, computational_budget):\n",
    "    \"\"\"Recommend activation function based on problem characteristics\"\"\"\n",
    "    \n",
    "    print(f\"\\nüéØ ACTIVATION FUNCTION RECOMMENDATION\")\n",
    "    print(f\"Problem Type: {problem_type}\")\n",
    "    print(f\"Network Depth: {network_depth} layers\")\n",
    "    print(f\"Computational Budget: {computational_budget}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    recommendations = []\n",
    "    \n",
    "    # Deep network recommendations\n",
    "    if network_depth > 10:\n",
    "        if computational_budget == 'high':\n",
    "            recommendations.append((\"Swish\", \"Excellent for very deep networks, smooth gradients\", 9))\n",
    "            recommendations.append((\"ELU\", \"Good alternative, self-normalizing properties\", 8))\n",
    "        else:\n",
    "            recommendations.append((\"ReLU\", \"Best balance of performance and speed\", 9))\n",
    "            recommendations.append((\"Leaky ReLU\", \"Fixes dying ReLU in deep networks\", 7))\n",
    "    \n",
    "    # Medium depth networks\n",
    "    elif network_depth > 5:\n",
    "        if problem_type == 'classification':\n",
    "            recommendations.append((\"ReLU\", \"Standard choice for classification\", 9))\n",
    "            recommendations.append((\"ELU\", \"Smooth alternative with good properties\", 8))\n",
    "        elif problem_type == 'regression':\n",
    "            recommendations.append((\"ELU\", \"Smooth, works well for regression\", 9))\n",
    "            recommendations.append((\"ReLU\", \"Fast and effective\", 8))\n",
    "    \n",
    "    # Shallow networks\n",
    "    else:\n",
    "        if computational_budget == 'low':\n",
    "            recommendations.append((\"ReLU\", \"Simple and fast\", 8))\n",
    "            recommendations.append((\"Tanh\", \"Classic choice for shallow networks\", 6))\n",
    "        else:\n",
    "            recommendations.append((\"Swish\", \"Smooth, good for optimization\", 8))\n",
    "            recommendations.append((\"ELU\", \"Good general purpose choice\", 7))\n",
    "    \n",
    "    # Special considerations\n",
    "    if problem_type == 'sequence_modeling':\n",
    "        recommendations.append((\"Tanh\", \"Good for RNNs, bounded output\", 7))\n",
    "        recommendations.append((\"Sigmoid\", \"If you need bounded outputs [0,1]\", 5))\n",
    "    \n",
    "    # Sort by score\n",
    "    recommendations.sort(key=lambda x: x[2], reverse=True)\n",
    "    \n",
    "    print(\"üìã RECOMMENDATIONS (ranked by suitability):\")\n",
    "    print()\n",
    "    \n",
    "    for i, (activation, reason, score) in enumerate(recommendations[:3], 1):\n",
    "        stars = \"‚≠ê\" * min(score, 5)\n",
    "        print(f\"{i}. {activation.upper()} {stars}\")\n",
    "        print(f\"   Reason: {reason}\")\n",
    "        print(f\"   Score: {score}/10\")\n",
    "        print()\n",
    "    \n",
    "    return recommendations[0][0] if recommendations else \"ReLU\"\n",
    "\n",
    "# Test recommendation system\n",
    "scenarios = [\n",
    "    (\"classification\", 15, \"high\"),\n",
    "    (\"regression\", 8, \"medium\"),\n",
    "    (\"classification\", 3, \"low\"),\n",
    "    (\"sequence_modeling\", 6, \"medium\")\n",
    "]\n",
    "\n",
    "print(\"üß™ TESTING RECOMMENDATION SYSTEM\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for problem_type, depth, budget in scenarios:\n",
    "    best_activation = recommend_activation(problem_type, depth, budget)\n",
    "    print(f\"\\n‚û°Ô∏è BEST CHOICE: {best_activation}\\n\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîç Key Insights from Analysis\n",
    "\n",
    "### üìä Mathematical Properties Summary\n",
    "\n",
    "| Activation | Max Derivative | Saturates? | Computational Cost | Gradient Flow |\n",
    "|------------|----------------|------------|--------------------|--------------|\n",
    "| **Sigmoid** | 0.25 | Yes | Low | üî¥ Poor (vanishing) |\n",
    "| **Tanh** | 1.0 | Yes | Low | üü° Fair (still vanishing) |\n",
    "| **ReLU** | 1.0 | No (positive) | Very Low | üü¢ Excellent |\n",
    "| **Leaky ReLU** | 1.0 | No | Very Low | üü¢ Excellent |\n",
    "| **ELU** | 1.0 | Partial | Medium | üü¢ Excellent |\n",
    "| **Swish** | 1.25 | No | High | üü¢ Excellent |\n",
    "\n",
    "### üéØ Practical Guidelines\n",
    "\n",
    "#### ‚úÖ **Use ReLU when:**\n",
    "- Building deep networks (>5 layers)\n",
    "- Computational efficiency is important\n",
    "- You need simple, reliable performance\n",
    "- Classification tasks\n",
    "\n",
    "#### ‚úÖ **Use Leaky ReLU when:**\n",
    "- Experiencing \"dying ReLU\" problem\n",
    "- Need gradients for all neurons\n",
    "- Deep networks with sparse activations\n",
    "\n",
    "#### ‚úÖ **Use ELU when:**\n",
    "- Want smooth activation function\n",
    "- Regression tasks\n",
    "- Self-normalizing networks\n",
    "- Medium computational budget\n",
    "\n",
    "#### ‚úÖ **Use Swish when:**\n",
    "- Maximum performance is critical\n",
    "- High computational budget available\n",
    "- Very deep networks\n",
    "- Research/experimental settings\n",
    "\n",
    "#### ‚ùå **Avoid Sigmoid/Tanh when:**\n",
    "- Building deep networks (>3 layers)\n",
    "- Gradient flow is critical\n",
    "- Training time is important\n",
    "\n",
    "---\n",
    "\n",
    "## üí° The ReLU Revolution\n",
    "\n",
    "### Why ReLU Changed Everything:\n",
    "\n",
    "1. **No Vanishing Gradients:** Derivative is either 0 or 1\n",
    "2. **Computational Efficiency:** Just max(0, x)\n",
    "3. **Sparsity:** Creates sparse representations\n",
    "4. **Unbounded:** No upper saturation\n",
    "5. **Empirical Success:** Enabled deep learning breakthroughs\n",
    "\n",
    "### The Simple Truth:\n",
    "Sometimes the simplest solutions are the most powerful. ReLU's success shows that elegant mathematics often beats complex engineering.\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Next Steps\n",
    "\n",
    "In the next notebook, we'll explore:\n",
    "- **Weight initialization strategies** (Xavier, He, LSUV)\n",
    "- **How initialization affects gradient flow**\n",
    "- **Proper initialization for different activations**\n",
    "\n",
    "---\n",
    "\n",
    "*This notebook demonstrates Concept 4 of Week 5: Deep Neural Network Architectures*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Week 5: Gradients (Python 3.10)",
   "language": "python",
   "name": "week5-gradients"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}