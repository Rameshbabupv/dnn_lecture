# Week 14: YOLO & SSD - Real-Time Object Detection

**Course:** 21CSE558T - Deep Neural Network Architectures
**Module:** 5 - Object Localization and Detection
**Week:** 14 of 15
**Date:** November 2025
**Instructor:** Prof. Ramesh Babu

---

## Overview

Welcome to **Week 14** - where object detection gets REAL! This is the week you've been waiting for: implementing state-of-the-art real-time object detectors that can process video at 30+ frames per second. If Week 13 taught you the fundamentals, Week 14 is where you build production-ready systems.

**What Makes This Week Special?**

Unlike Week 13's foundation-building approach with NumPy, Week 14 puts powerful tools directly in your hands. You'll use YOLOv8 (the latest in the YOLO family) and SSD through high-level frameworks that make deployment surprisingly simple. Within 20 minutes of starting Notebook 02, you'll have a working real-time detector running on your webcam or video files!

**What You'll Build This Week:**

Using the Ultralytics YOLOv8 library and TensorFlow, you'll implement single-shot detectors that process images in milliseconds (not seconds!). You'll understand YOLO's elegant architecture, train models on custom datasets, compare YOLO vs SSD architectures, and achieve impressive results: 50-60% mAP at 30+ FPS. By week's end, you'll have working detectors ready for real applications.

**Why This Matters:**

Week 14 bridges the gap between theory and practice. The skills you gain here - training YOLO models, preparing custom datasets, optimizing for speed vs accuracy - are directly applicable to industry projects. Whether you're building autonomous vehicle systems, surveillance applications, or mobile AR experiences, YOLO and SSD are the go-to architectures powering real-world deployment.

**The Learning Journey:**

You'll progress through 6 hands-on notebooks (totaling ~105 minutes) that take you from understanding YOLO's architecture to training your own custom detector. Each notebook builds practical skills: Notebook 01 explains the theory, Notebook 02 gets you detecting immediately, Notebooks 03-04 teach custom training, and Notebooks 05-06 introduce SSD and comparative analysis. This is hands-on, exciting, and immensely practical!

**Prerequisites Alert:**

This week REQUIRES solid understanding of Week 13 concepts: IoU calculation, Non-Maximum Suppression (NMS), mAP evaluation, and bounding box representations. If you skipped Week 13 or feel shaky on these fundamentals, go back and complete those notebooks first. Week 14 moves fast and assumes you know the building blocks!

---

## Learning Objectives

After completing Week 14, you will be able to:

1. **Understand Single-Shot Detection Paradigm**
   - Explain how YOLO processes images in a single forward pass
   - Compare single-shot detectors (YOLO, SSD) vs two-stage detectors (R-CNN)
   - Describe the speed-accuracy tradeoff in object detection
   - Identify applications where real-time detection is critical (autonomous vehicles, robotics)

2. **Master YOLO Architecture**
   - Understand YOLO's grid-based prediction approach (7√ó7 or 13√ó13 grids)
   - Explain anchor boxes and how YOLO handles multiple objects
   - Describe YOLO's loss function: localization + confidence + classification
   - Trace the evolution from YOLOv1 to YOLOv8 and key improvements

3. **Deploy Pre-trained YOLO Models**
   - Load YOLOv8 pre-trained models (nano, small, medium, large, xlarge)
   - Run inference on images, videos, and webcam streams
   - Interpret detection outputs: bounding boxes, confidence scores, class labels
   - Visualize results with professional-looking annotations

4. **Train YOLO on Custom Datasets**
   - Prepare custom datasets in YOLO format (images + annotation files)
   - Configure training parameters: batch size, learning rate, epochs
   - Monitor training progress: loss curves, validation mAP
   - Fine-tune pre-trained models for specific domains (transfer learning)

5. **Understand and Compare SSD Architecture**
   - Explain SSD's multi-scale feature map approach
   - Compare YOLO vs SSD: architecture, speed, accuracy
   - Understand default boxes (SSD's version of anchor boxes)
   - Choose the right detector for different applications

---

## Prerequisites

**Required Knowledge:**

Before starting Week 14, you MUST be comfortable with:

- **Week 13 Fundamentals (CRITICAL!):** IoU calculation, Non-Maximum Suppression (NMS), mAP evaluation, bounding box formats (XYWH, XYXY)
- **CNNs (Weeks 10-12):** Convolutional layers, feature maps, activation functions, pooling
- **Transfer Learning (Week 12):** Loading pre-trained models, fine-tuning, feature extraction
- **Python Programming:** Functions, loops, file I/O, command-line arguments
- **Deep Learning Frameworks:** Basic TensorFlow/Keras or PyTorch knowledge

**Optional But Helpful:**

- Experience with video processing (OpenCV VideoCapture)
- Understanding of object detection metrics beyond mAP (precision-recall curves)
- Familiarity with YOLO paper (original 2016 publication)
- Knowledge of data augmentation techniques

**Skills Assessment:**

If you can answer these questions confidently, you're ready:
- "What does IoU measure?" ‚Üí Overlap between predicted and ground truth bounding boxes (0.0 to 1.0)
- "What is NMS and why do we need it?" ‚Üí Removes duplicate detections by keeping highest confidence boxes
- "What does mAP@0.5 mean?" ‚Üí Mean Average Precision with IoU threshold of 0.5
- "How does a CNN extract features?" ‚Üí Through hierarchical convolutional layers

**If you need a refresher:**
- **MUST COMPLETE:** Week 13 notebooks (especially 02_iou and 04_evaluation_metrics)
- Review Week 10-11 CNN architectures
- Revisit Week 12 Transfer Learning concepts
- Check Ultralytics YOLOv8 documentation: https://docs.ultralytics.com

---

## Notebook Sequence

This week includes **6 progressive notebooks** (Notebooks 01-06) that build your real-time detection expertise. Each notebook is designed for hands-on learning with immediate visual results. Total estimated time: **100-110 minutes**.

**GPU Recommendation:** Notebooks 01-03, 05-06 can run on CPU. **Notebook 04 (training) strongly recommends GPU** for reasonable training times (30 min with GPU vs 4+ hours on CPU).

---

### **01_yolo_architecture_explained.ipynb**

**Duration:** 15 minutes
**Difficulty:** Intermediate
**Focus:** Architectural understanding
**Hardware:** CPU sufficient

**What You'll Learn:**
- How YOLO divides images into grids (7√ó7, 13√ó13, or 19√ó19)
- Grid cell predictions: bounding boxes + confidence + class probabilities
- Anchor boxes: predefined shapes to handle multiple object scales
- YOLO loss function: balancing localization, objectness, and classification
- Evolution from YOLOv1 (2016) to YOLOv8 (2023) with key improvements

**Key Demonstrations:**
- Visual explanation of grid-based prediction system
- How YOLO assigns objects to grid cells (center point approach)
- Anchor box visualization: different aspect ratios and scales
- Loss function breakdown with mathematical formulas
- Architecture diagram: backbone (feature extraction) + neck (feature fusion) + head (prediction)

**Hands-On Activities:**
- Visualize grid overlays on sample images
- Understand how multiple objects are assigned to cells
- Calculate theoretical number of predictions per image
- Compare YOLOv1 vs YOLOv8 architectural differences

**Expected Output:**
```
YOLO Grid Analysis:
- Input image: 640√ó640 pixels
- Grid size: 20√ó20 cells
- Anchors per cell: 3
- Classes: 80 (COCO dataset)
- Total predictions: 20√ó20√ó3 = 1,200 bounding boxes per image

After NMS: ~5-20 final detections (high confidence only)
```

**Why This Matters:**
Understanding YOLO's grid-based approach is crucial for debugging predictions, tuning hyperparameters, and appreciating why YOLO is so fast (single forward pass). You'll see the elegance of "You Only Look Once" in action!

**Run This:** [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](#)

---

### **02_yolov8_pretrained_detection.ipynb** ‚≠ê KEY NOTEBOOK

**Duration:** 20 minutes
**Difficulty:** Beginner-Intermediate
**Focus:** Hands-on detection with pre-trained models
**Hardware:** CPU sufficient (GPU makes it faster)

**What You'll Learn:**
- Installing Ultralytics library (`pip install ultralytics`)
- Loading YOLOv8 pre-trained models (yolov8n, yolov8s, yolov8m, yolov8l, yolov8x)
- Running inference on images, videos, and live webcam
- Understanding model variants: nano (fastest) to xlarge (most accurate)
- Interpreting detection outputs: boxes, scores, classes
- Visualizing results with professional annotations

**Key Demonstrations:**
- Detect objects in photos (people, cars, animals)
- Process videos frame-by-frame with real-time display
- Run live webcam detection (if available)
- Compare model sizes: yolov8n (6 MB) vs yolov8x (136 MB)
- Speed benchmarking: FPS measurement on your hardware

**Hands-On Activities:**
- Load `yolov8n.pt` (nano model) - downloads automatically
- Run inference on sample images: `model.predict('image.jpg')`
- Filter detections by confidence threshold (e.g., >0.5)
- Visualize detections with bounding boxes and labels
- Try different model variants and compare results

**Code You'll Write:**
```python
from ultralytics import YOLO

# Load pre-trained model (auto-downloads on first use)
model = YOLO('yolov8n.pt')  # nano: fastest, least accurate

# Run inference on image
results = model.predict('sample.jpg', conf=0.5)

# Process results
for result in results:
    boxes = result.boxes  # Bounding boxes
    for box in boxes:
        coords = box.xyxy[0]  # x1, y1, x2, y2
        conf = box.conf[0]    # Confidence score
        cls = box.cls[0]      # Class ID
        print(f"Detected {model.names[int(cls)]} with {conf:.2f} confidence")

# Visualize (annotations drawn automatically)
result.show()
```

**Expected Output:**
```
Detections on 'street_scene.jpg':
- person (0.94)
- car (0.89)
- car (0.87)
- bicycle (0.76)
- traffic light (0.68)

Processing time: 18ms (55 FPS on CPU)
mAP: ~37% on COCO validation set (yolov8n)
```

**Why This Matters:**
This is your first taste of production-ready object detection! In 10 lines of code, you're using a model that powers real commercial applications. The instant gratification of seeing accurate detections builds confidence for the training work ahead.

**Run This:** [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](#)

---

### **03_custom_dataset_preparation.ipynb**

**Duration:** 15 minutes
**Difficulty:** Intermediate
**Focus:** Dataset preparation for custom training
**Hardware:** CPU sufficient

**What You'll Learn:**
- YOLO annotation format: one .txt file per image with normalized coordinates
- Annotation file structure: `<class_id> <x_center> <y_center> <width> <height>`
- Dataset directory structure: `train/`, `val/`, `test/` splits
- Creating YAML configuration files for custom datasets
- Data augmentation strategies for object detection
- Common annotation tools: LabelImg, Roboflow, CVAT

**Key Demonstrations:**
- Convert bounding boxes from COCO format to YOLO format
- Normalize coordinates to [0, 1] range
- Organize dataset with proper directory structure
- Write dataset.yaml configuration file
- Validate annotations with visualization

**Hands-On Activities:**
- Create sample annotations manually
- Implement coordinate conversion functions
- Build dataset.yaml file for a toy dataset
- Visualize annotated images to verify correctness
- Split dataset into train/val/test (80/15/5 or 70/20/10)

**Dataset Structure You'll Create:**
```
custom_dataset/
‚îú‚îÄ‚îÄ images/
‚îÇ   ‚îú‚îÄ‚îÄ train/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ img_001.jpg
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ img_002.jpg
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ   ‚îú‚îÄ‚îÄ val/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ   ‚îî‚îÄ‚îÄ test/
‚îÇ       ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ labels/
‚îÇ   ‚îú‚îÄ‚îÄ train/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ img_001.txt  # Annotations for img_001.jpg
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ img_002.txt
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ   ‚îú‚îÄ‚îÄ val/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ   ‚îî‚îÄ‚îÄ test/
‚îÇ       ‚îî‚îÄ‚îÄ ...
‚îî‚îÄ‚îÄ dataset.yaml

# dataset.yaml content:
path: /path/to/custom_dataset
train: images/train
val: images/val
test: images/test

nc: 3  # Number of classes
names: ['cat', 'dog', 'bird']
```

**Annotation File Format:**
```
# img_001.txt (each line = one object)
0 0.512 0.345 0.123 0.089  # class_id=0 (cat), center=(0.512, 0.345), size=(0.123, 0.089)
1 0.789 0.654 0.098 0.124  # class_id=1 (dog), center=(0.789, 0.654), size=(0.098, 0.124)
```

**Expected Output:**
```
Dataset Statistics:
- Total images: 500
- Train: 400 images (80%)
- Val: 75 images (15%)
- Test: 25 images (5%)
- Classes: 3 (cat, dog, bird)
- Total annotations: 1,247
- Avg objects per image: 2.49

‚úì Dataset validation passed
‚úì All images have corresponding labels
‚úì Coordinate ranges valid [0.0, 1.0]
```

**Why This Matters:**
Dataset preparation is 70% of the work in custom object detection projects! Getting this right is crucial for training success. Poorly formatted data leads to cryptic errors or models that don't learn. Master this once, apply it everywhere.

**Run This:** [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](#)

---

### **04_yolov8_training.ipynb**

**Duration:** 25 minutes (+ training time)
**Difficulty:** Intermediate-Advanced
**Focus:** Custom model training
**Hardware:** ‚ö†Ô∏è **GPU STRONGLY RECOMMENDED** (Google Colab T4 free GPU works great!)

**What You'll Learn:**
- Training YOLOv8 on custom datasets from scratch or via transfer learning
- Setting hyperparameters: epochs, batch size, learning rate, image size
- Monitoring training progress: loss curves, mAP evolution
- Validating trained models on validation set
- Saving and loading trained model weights
- Fine-tuning strategies for limited data

**Key Demonstrations:**
- Train on toy dataset (100-500 images) to see full pipeline
- Monitor training metrics: box_loss, cls_loss, objectness_loss
- Visualize training progress with TensorBoard or built-in plots
- Compare training from scratch vs fine-tuning pre-trained weights
- Evaluate final model: mAP@0.5, mAP@0.5:0.95

**Hands-On Activities:**
- Configure training parameters in YAML or code
- Start training with `model.train()` command
- Monitor loss convergence and mAP improvement
- Run validation after training to check performance
- Save best model checkpoint for deployment

**Code You'll Write:**
```python
from ultralytics import YOLO

# Load pre-trained model for transfer learning (recommended!)
model = YOLO('yolov8n.pt')

# Train on custom dataset
results = model.train(
    data='dataset.yaml',        # Path to dataset config
    epochs=50,                  # Number of training epochs
    batch=16,                   # Batch size (adjust for GPU memory)
    imgsz=640,                  # Image size (640 is standard)
    device='0',                 # GPU device (or 'cpu')
    lr0=0.01,                   # Initial learning rate
    patience=10,                # Early stopping patience
    save=True,                  # Save checkpoints
    project='my_detector',      # Output directory
    name='exp1'                 # Experiment name
)

# Training metrics automatically logged
# Best model saved to: my_detector/exp1/weights/best.pt
```

**Training Process:**
```
Epoch 1/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25/25 [00:45<00:00]
    box_loss: 0.0543, cls_loss: 0.0789, obj_loss: 0.0321
    mAP@0.5: 0.234

Epoch 10/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25/25 [00:42<00:00]
    box_loss: 0.0298, cls_loss: 0.0412, obj_loss: 0.0187
    mAP@0.5: 0.547

Epoch 30/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25/25 [00:41<00:00]
    box_loss: 0.0167, cls_loss: 0.0234, obj_loss: 0.0098
    mAP@0.5: 0.683  ‚Üê Best model (early stopping)

Training complete! Best mAP@0.5: 0.683 at epoch 30
```

**Expected Output:**
```
Training Summary:
- Dataset: 400 train, 75 val images
- Model: YOLOv8n (fine-tuned from COCO)
- Training time: ~25 minutes (GPU) / 4+ hours (CPU)
- Final mAP@0.5: 68.3%
- Final mAP@0.5:0.95: 42.7%
- Inference speed: 18 ms/image (55 FPS)

‚úì Model saved: my_detector/exp1/weights/best.pt
‚úì Training plots: my_detector/exp1/results.png
```

**Why This Matters:**
This is where you become a practitioner, not just a user! Training custom YOLO models is a highly marketable skill. You'll learn to diagnose training issues (overfitting, underfitting), tune hyperparameters, and achieve production-ready results.

**GPU Note:** Training on CPU is possible but painfully slow (4-6 hours for 50 epochs). **Use Google Colab free GPU** (Runtime ‚Üí Change runtime type ‚Üí GPU ‚Üí T4). With GPU, same training takes 20-30 minutes!

**Run This:** [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](#)

---

### **05_ssd_architecture_demo.ipynb**

**Duration:** 15 minutes
**Difficulty:** Intermediate
**Focus:** Alternative single-shot architecture
**Hardware:** CPU sufficient

**What You'll Learn:**
- SSD (Single Shot MultiBox Detector) architecture overview
- Multi-scale feature maps: detecting objects at different scales
- Default boxes (SSD's anchor boxes) and aspect ratios
- How SSD differs from YOLO: architectural choices and tradeoffs
- Running pre-trained SSD models (TensorFlow implementation)

**Key Demonstrations:**
- Visualize SSD's multi-scale feature pyramid
- Show how SSD uses 6 different feature maps (38√ó38, 19√ó19, 10√ó10, 5√ó5, 3√ó3, 1√ó1)
- Default boxes visualization: 4-6 boxes per location with different scales/ratios
- Compare SSD predictions vs YOLO predictions on same images
- Speed and accuracy benchmarks

**Hands-On Activities:**
- Load pre-trained SSD model (e.g., ssd_mobilenet_v2_coco)
- Run inference on test images
- Visualize multi-scale feature maps and default boxes
- Compare detection results with YOLOv8 from Notebook 02
- Analyze where SSD performs better/worse than YOLO

**Architecture Comparison:**
```
YOLO:
- Single-scale grid (19√ó19)
- Anchor boxes at single resolution
- Faster inference (~55 FPS for nano)
- Simpler architecture

SSD:
- Multi-scale feature maps (6 scales)
- Default boxes at multiple resolutions
- Better small object detection
- More complex architecture
- Slower inference (~30-40 FPS)
```

**Expected Output:**
```
SSD MobileNet v2 Results:
- Model size: 67 MB
- Inference time: 28 ms (36 FPS on CPU)
- mAP@0.5: 22% on COCO (lower than YOLO due to speed focus)

Detection comparison on 'crowd_scene.jpg':
- SSD detected: 15 people (including 3 small/distant)
- YOLO detected: 12 people (missed small objects)
- SSD better for: small objects, crowded scenes
- YOLO better for: speed, simple scenes
```

**Why This Matters:**
Real projects require choosing the RIGHT architecture for your constraints. SSD's multi-scale approach excels at small object detection (medical imaging, aerial surveillance), while YOLO wins on speed. Understanding both makes you a versatile engineer.

**Run This:** [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](#)

---

### **06_yolo_vs_ssd_comparison.ipynb**

**Duration:** 15 minutes
**Difficulty:** Intermediate
**Focus:** Comparative analysis and selection criteria
**Hardware:** CPU sufficient

**What You'll Learn:**
- Side-by-side comparison: YOLO vs SSD architecture and performance
- Speed vs accuracy tradeoff analysis
- When to choose YOLO vs SSD for different applications
- Hardware considerations: mobile, edge devices, cloud servers
- Future of object detection: modern variants and improvements

**Key Demonstrations:**
- Benchmark both architectures on same test set
- Measure inference time, mAP, memory usage
- Analyze failure cases: where each struggles
- Visualize speed-accuracy Pareto frontier
- Real-world deployment scenarios

**Hands-On Activities:**
- Run both YOLO and SSD on identical images
- Compare detection outputs visually
- Measure FPS on your hardware (CPU and GPU if available)
- Calculate mAP for both on validation set
- Create decision matrix for architecture selection

**Comparison Table:**
```
Metric               | YOLOv8n  | YOLOv8s  | SSD MobileNet | SSD ResNet50
---------------------|----------|----------|---------------|---------------
Model Size           | 6 MB     | 22 MB    | 67 MB         | 165 MB
Inference (CPU)      | 18 ms    | 35 ms    | 28 ms         | 89 ms
Inference (GPU)      | 2 ms     | 3 ms     | 8 ms          | 15 ms
FPS (CPU)            | 55       | 28       | 36            | 11
FPS (GPU)            | 500      | 333      | 125           | 67
mAP@0.5 (COCO)       | 37%      | 45%      | 22%           | 26%
mAP@0.5:0.95 (COCO)  | 22%      | 28%      | 18%           | 24%
Small Objects        | Medium   | Good     | Excellent     | Excellent
```

**Decision Framework:**
```
Choose YOLO if:
‚úì Speed is critical (real-time video, robotics)
‚úì Limited computational resources (mobile, edge devices)
‚úì Simple deployment preferred
‚úì Most objects are medium-large sized

Choose SSD if:
‚úì Small object detection is important
‚úì Moderate speed acceptable (15-30 FPS)
‚úì Working with crowded scenes
‚úì Multi-scale detection crucial
```

**Expected Output:**
```
Benchmark Results on Test Set (100 images):

YOLOv8n:
- Average FPS: 52 (CPU), 487 (GPU)
- mAP@0.5: 68.3%
- Total inference time: 1.92 sec
- Best at: Speed, medium/large objects

SSD MobileNet:
- Average FPS: 34 (CPU), 118 (GPU)
- mAP@0.5: 61.7%
- Total inference time: 2.94 sec
- Best at: Small objects, detection diversity

Recommendation for our use case: YOLOv8n (speed priority)
```

**Why This Matters:**
No single "best" detector exists - only the best detector for YOUR specific constraints. This comparative analysis teaches you to make data-driven architectural decisions, a critical skill for real-world ML engineering.

**Run This:** [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](#)

---

## Learning Path Diagram

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    WEEK 14 LEARNING JOURNEY                        ‚îÇ
‚îÇ            Real-Time Object Detection with YOLO & SSD              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

                          START HERE
                       (After Week 13!)
                               ‚Üì
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ  [01] YOLO Architecture Explained        ‚îÇ
        ‚îÇ  ‚è± 15 min  ‚îÇ  üéØ Theoretical Foundation ‚îÇ
        ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
        ‚îÇ  Learn:                                  ‚îÇ
        ‚îÇ  ‚úì Grid-based prediction system          ‚îÇ
        ‚îÇ  ‚úì Anchor boxes and cell assignments     ‚îÇ
        ‚îÇ  ‚úì YOLO loss function breakdown          ‚îÇ
        ‚îÇ  ‚úì Evolution: YOLOv1 ‚Üí YOLOv8           ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                               ‚Üì
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ  [02] YOLOv8 Pre-trained Detection  ‚≠ê   ‚îÇ
        ‚îÇ  ‚è± 20 min  ‚îÇ  üéØ Hands-On Detection     ‚îÇ
        ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
        ‚îÇ  Learn:                                  ‚îÇ
        ‚îÇ  ‚úì Load YOLOv8 models (n/s/m/l/x)        ‚îÇ
        ‚îÇ  ‚úì Run inference on images/videos        ‚îÇ
        ‚îÇ  ‚úì Interpret detection outputs           ‚îÇ
        ‚îÇ  ‚úì Real-time webcam detection            ‚îÇ
        ‚îÇ                                          ‚îÇ
        ‚îÇ  Code: model.predict() in 5 lines!       ‚îÇ
        ‚îÇ  THIS IS THE FUN PART! üöÄ                ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                               ‚Üì
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ  [03] Custom Dataset Preparation         ‚îÇ
        ‚îÇ  ‚è± 15 min  ‚îÇ  üéØ Data Engineering       ‚îÇ
        ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
        ‚îÇ  Learn:                                  ‚îÇ
        ‚îÇ  ‚úì YOLO annotation format (normalized)   ‚îÇ
        ‚îÇ  ‚úì Dataset directory structure           ‚îÇ
        ‚îÇ  ‚úì Creating dataset.yaml config          ‚îÇ
        ‚îÇ  ‚úì Coordinate conversion utilities       ‚îÇ
        ‚îÇ                                          ‚îÇ
        ‚îÇ  Build: Complete dataset pipeline        ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                               ‚Üì
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ  [04] YOLOv8 Training  ‚ö° GPU NEEDED      ‚îÇ
        ‚îÇ  ‚è± 25 min + training  ‚îÇ  üéØ Custom Model ‚îÇ
        ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
        ‚îÇ  Learn:                                  ‚îÇ
        ‚îÇ  ‚úì Transfer learning with YOLOv8         ‚îÇ
        ‚îÇ  ‚úì Hyperparameter tuning                 ‚îÇ
        ‚îÇ  ‚úì Monitoring training metrics           ‚îÇ
        ‚îÇ  ‚úì Validation and model saving           ‚îÇ
        ‚îÇ                                          ‚îÇ
        ‚îÇ  Train: Your own detector! üéì            ‚îÇ
        ‚îÇ  Achieve: 60%+ mAP in 30 minutes         ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                               ‚Üì
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ  [05] SSD Architecture Demo              ‚îÇ
        ‚îÇ  ‚è± 15 min  ‚îÇ  üéØ Alternative Approach   ‚îÇ
        ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
        ‚îÇ  Learn:                                  ‚îÇ
        ‚îÇ  ‚úì Multi-scale feature maps (6 levels)   ‚îÇ
        ‚îÇ  ‚úì Default boxes and aspect ratios       ‚îÇ
        ‚îÇ  ‚úì SSD vs YOLO architectural diffs       ‚îÇ
        ‚îÇ  ‚úì Small object detection advantages     ‚îÇ
        ‚îÇ                                          ‚îÇ
        ‚îÇ  Run: Pre-trained SSD models             ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                               ‚Üì
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ  [06] YOLO vs SSD Comparison             ‚îÇ
        ‚îÇ  ‚è± 15 min  ‚îÇ  üéØ Decision Framework     ‚îÇ
        ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
        ‚îÇ  Learn:                                  ‚îÇ
        ‚îÇ  ‚úì Speed vs accuracy tradeoffs           ‚îÇ
        ‚îÇ  ‚úì Benchmark both on same data           ‚îÇ
        ‚îÇ  ‚úì When to choose which architecture     ‚îÇ
        ‚îÇ  ‚úì Real-world deployment considerations  ‚îÇ
        ‚îÇ                                          ‚îÇ
        ‚îÇ  Outcome: Make informed choices!         ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                               ‚Üì
                    ‚úÖ WEEK 14 COMPLETE!
                               ‚Üì
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ         You're now ready for:            ‚îÇ
        ‚îÇ  ‚Üí Week 15: R-CNN (Two-stage detectors)  ‚îÇ
        ‚îÇ  ‚Üí Deploy YOLO in production apps        ‚îÇ
        ‚îÇ  ‚Üí Build custom detection systems        ‚îÇ
        ‚îÇ  ‚Üí Final exam object detection questions ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  üí° KEY INSIGHT: From Theory to Practice!                          ‚îÇ
‚îÇ                                                                    ‚îÇ
‚îÇ  Week 13 taught you WHY (IoU, mAP, fundamentals).                 ‚îÇ
‚îÇ  Week 14 teaches you HOW (deploy YOLO, train models, ship code).  ‚îÇ
‚îÇ  Week 15 will teach you ALTERNATIVES (R-CNN for max accuracy).    ‚îÇ
‚îÇ                                                                    ‚îÇ
‚îÇ  By end of Week 14, you can build production detectors! üöÄ        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## Setup Instructions

### Option 1: Google Colab (Recommended) ‚úÖ

**Why Colab?**
- **Free GPU access** (NVIDIA T4) - essential for Notebook 04 training!
- No installation hassle - Ultralytics and dependencies pre-installed
- Consistent environment for all students
- Easy sharing and collaboration
- Persistent storage with Google Drive integration

**Steps:**
1. **Open any notebook** by clicking the "Open in Colab" badge
2. **Enable GPU** (for Notebook 04 only):
   - Click Runtime ‚Üí Change runtime type
   - Hardware accelerator ‚Üí GPU ‚Üí T4 GPU (free tier)
   - Save
3. **Run cells** using Shift+Enter
4. **Install Ultralytics** (first cell of each notebook):
   ```python
   !pip install ultralytics
   ```

**First-Time Setup (1 minute):**
```python
# Cell 1: Install dependencies
!pip install ultralytics  # YOLOv8 library (includes PyTorch, OpenCV, etc.)

# Cell 2: Verify installation
from ultralytics import YOLO
import torch
print(f"‚úì Ultralytics installed")
print(f"‚úì PyTorch {torch.__version__}")
print(f"‚úì CUDA available: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"  GPU: {torch.cuda.get_device_name(0)}")
```

**Expected Output (with GPU enabled):**
```
‚úì Ultralytics installed
‚úì PyTorch 2.0.1+cu118
‚úì CUDA available: True
  GPU: Tesla T4 (16 GB)
```

**That's it!** First model download happens automatically when you run `YOLO('yolov8n.pt')`.

---

### Option 2: Local Jupyter Notebook (Alternative)

**Requirements:**
- Python 3.8 or higher
- **GPU strongly recommended** for training (NVIDIA with CUDA support)
- 10 GB disk space (for models and datasets)
- Internet connection (for model downloads)

**Installation Steps:**

**Step 1: Create Virtual Environment**
```bash
# Navigate to week14 directory
cd course_planning/weekly_plans/week14-module5-detection-models

# Create virtual environment
python -m venv week14_env

# Activate environment
# On macOS/Linux:
source week14_env/bin/activate
# On Windows:
week14_env\Scripts\activate
```

**Step 2: Install Dependencies**
```bash
# Install from requirements.txt
pip install -r requirements.txt

# OR install manually:
pip install ultralytics torch torchvision opencv-python matplotlib jupyter ipywidgets
```

**Step 3: Verify GPU (if available)**
```bash
# Check CUDA availability
python -c "import torch; print(f'CUDA: {torch.cuda.is_available()}')"

# If False and you have NVIDIA GPU, install CUDA toolkit:
# Visit: https://developer.nvidia.com/cuda-downloads
```

**Step 4: Launch Jupyter**
```bash
jupyter notebook
# OR
jupyter lab

# Browser opens automatically to notebooks/ directory
```

---

### Verification Checklist

Run this in the first cell of any notebook:

```python
# Week 14 Environment Check
import sys
print(f"Python: {sys.version}")

import ultralytics
print(f"‚úì Ultralytics {ultralytics.__version__}")

import torch
print(f"‚úì PyTorch {torch.__version__}")
print(f"‚úì CUDA: {torch.cuda.is_available()}")

import cv2
print(f"‚úì OpenCV {cv2.__version__}")

from ultralytics import YOLO
model = YOLO('yolov8n.pt')  # Downloads on first run (~6 MB)
print(f"‚úì YOLOv8 model loaded successfully")

print("\n‚úÖ Week 14 environment ready!")
print(f"üéØ GPU available: {'YES ‚ö°' if torch.cuda.is_available() else 'NO (CPU only)'}")
```

**Expected Output:**
```
Python: 3.10.x
‚úì Ultralytics 8.0.200
‚úì PyTorch 2.0.1
‚úì CUDA: True
‚úì OpenCV 4.8.1
Downloading yolov8n.pt... 100%
‚úì YOLOv8 model loaded successfully

‚úÖ Week 14 environment ready!
üéØ GPU available: YES ‚ö°
```

---

## Quick Start Guide

### For Students

**If you're new to object detection:**
1. ‚úÖ **MUST COMPLETE Week 13 first** - IoU, NMS, mAP are prerequisites
2. Start with Notebook 01 for architectural understanding
3. Jump to Notebook 02 for hands-on excitement (get detections in 5 minutes!)
4. Complete Notebooks 03-04 to train custom models
5. Explore Notebooks 05-06 for SSD comparison

**If you're experienced with detection:**
1. Skim Notebook 01 as refresher
2. Deep dive into Notebook 02 (YOLOv8 inference)
3. Focus on Notebook 04 (training) - this is where skills grow
4. Use Notebook 06 for comparative analysis

**Time Management:**
- Minimum path (core skills): Notebooks 01, 02, 04 ‚Üí 60 minutes
- Complete path (full understanding): All 6 notebooks ‚Üí 105 minutes
- Recommended: Spread across 2-3 sessions for better retention

**Common Pitfalls to Avoid:**
- ‚ùå Skipping Week 13 fundamentals ‚Üí You'll be lost on IoU, NMS, mAP
- ‚ùå Training on CPU (Notebook 04) ‚Üí 4+ hours wait time. Use Colab GPU!
- ‚ùå Not validating dataset preparation (Notebook 03) ‚Üí Training fails mysteriously
- ‚ùå Using giant batch sizes ‚Üí GPU out-of-memory errors. Start with batch=8 or 16

---

### For Instructors

**Lecture Planning (2-3 hours total):**
- **Session 1 (60 min):** YOLO architecture theory + live Notebook 02 demo
  - Cover YOLO evolution, grid-based approach, loss function
  - Live demo: Load model and detect objects in classroom (webcam!)
  - Students: Complete Notebooks 01-02 as homework

- **Session 2 (90 min):** Custom training workshop (Notebook 04)
  - Review dataset preparation (Notebook 03 pre-requisite)
  - Live training demo: Start training, monitor progress
  - While model trains: Explain hyperparameters, overfitting, validation
  - Students: Train on provided toy dataset during class

- **Session 3 (45 min):** SSD comparison + final discussion
  - Compare YOLO vs SSD architectures
  - Discuss speed-accuracy tradeoffs
  - Student presentations: Show custom trained models
  - Preview Week 15 (R-CNN two-stage detectors)

**Lab Setup Recommendations:**
- Pre-install Ultralytics on lab machines (or use Colab)
- Prepare toy dataset (100-200 images) for quick training demos
- Test GPU availability ahead of time
- Have backup: pre-trained custom model in case training fails during class

**Assessment Ideas:**
- **Quiz:** YOLO architecture components, IoU calculations, mAP interpretation
- **Coding:** Modify Notebook 02 to filter detections by class or confidence
- **Project:** Train detector on student-chosen dataset, report mAP results
- **Presentation:** Compare YOLO vs SSD for specific application domain

---

## Expected Results

After completing Week 14 notebooks, you should achieve:

### Notebook 02 - Pre-trained Detection
```
Performance Metrics:
- Model: YOLOv8n (nano)
- Inference time: 15-25 ms per image (CPU)
- Inference time: 2-5 ms per image (GPU)
- FPS: 40-65 (CPU), 200-500 (GPU)
- mAP@0.5 (COCO val): ~37%
- Detection quality: Excellent for common objects (people, vehicles, animals)
```

### Notebook 04 - Custom Training
```
Training Results (typical toy dataset):
- Dataset: 400 train, 75 val images
- Epochs: 50 (with early stopping)
- Training time: 25-35 minutes (GPU), 4-6 hours (CPU)
- Final mAP@0.5: 60-75% (depends on dataset quality)
- Final mAP@0.5:0.95: 35-50%
- Inference speed: 18-25 ms (CPU)

Success Criteria:
‚úì Training loss decreases steadily
‚úì Validation mAP > 60%
‚úì No severe overfitting (train/val mAP gap < 15%)
‚úì Model detects objects in test images
```

### Notebook 06 - YOLO vs SSD Comparison
```
Benchmark Summary (100-image test set):

YOLOv8n:
- FPS: 52 (CPU), 480 (GPU)
- mAP@0.5: 68%
- Strengths: Speed, simple deployment

SSD MobileNet:
- FPS: 34 (CPU), 115 (GPU)
- mAP@0.5: 62%
- Strengths: Small object detection

Insight: YOLO preferred for most real-time applications
```

**Visual Outputs You'll Create:**
- Annotated images with bounding boxes, labels, confidence scores
- Training curves: loss vs epoch, mAP vs epoch
- Speed-accuracy scatter plots
- Confusion matrices for multi-class detection
- Video demos with real-time detections

---

## Technical Requirements

### Hardware Recommendations

**Minimum (CPU-only):**
- Modern CPU (Intel i5/AMD Ryzen 5 or better)
- 8 GB RAM
- 10 GB free disk space
- Works for: Notebooks 01-03, 05-06
- Notebook 04 training: Very slow (4+ hours)

**Recommended (with GPU):**
- NVIDIA GPU with 4+ GB VRAM (GTX 1650 or better)
- CUDA 11.0+ and cuDNN installed
- 16 GB system RAM
- 20 GB free disk space
- Works for: All notebooks at full speed
- Notebook 04 training: 25-35 minutes

**Cloud Options (Best for most students):**
- **Google Colab (Free GPU):** Tesla T4 (16 GB VRAM) - Perfect for this course!
- **Kaggle Kernels:** Free P100 GPU - Great alternative
- **AWS SageMaker:** Paid but powerful for large projects

### Software Requirements

**Core Dependencies:**
```
Python >= 3.8
ultralytics >= 8.0.0  (includes PyTorch, OpenCV, etc.)
torch >= 1.13.0
torchvision >= 0.14.0
opencv-python >= 4.5.0
matplotlib >= 3.5.0
jupyter >= 1.0.0
```

**Optional (for enhanced features):**
```
tensorboard  (training visualization)
gradio       (web UI for demos)
onnx         (model export for deployment)
```

All dependencies installed automatically via `pip install ultralytics`.

### Model Downloads (Automatic)

When you run `YOLO('yolov8n.pt')`, models auto-download from Ultralytics:
- **yolov8n.pt**: 6 MB (nano - fastest)
- **yolov8s.pt**: 22 MB (small)
- **yolov8m.pt**: 52 MB (medium)
- **yolov8l.pt**: 88 MB (large)
- **yolov8x.pt**: 136 MB (xlarge - most accurate)

First run takes 10-30 seconds for download. Subsequent runs use cached models.

---

## Troubleshooting

### Common Issues and Solutions

**1. CUDA Not Available (Notebook 04)**
```
Problem: torch.cuda.is_available() returns False
Solutions:
- Verify NVIDIA GPU installed: nvidia-smi command
- Install CUDA toolkit: https://developer.nvidia.com/cuda-downloads
- Reinstall PyTorch with CUDA:
  pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118
- OR use Google Colab free GPU (easiest solution!)
```

**2. Out of Memory Error During Training**
```
Problem: RuntimeError: CUDA out of memory
Solutions:
- Reduce batch size: batch=16 ‚Üí batch=8 ‚Üí batch=4
- Reduce image size: imgsz=640 ‚Üí imgsz=416
- Use smaller model: yolov8m ‚Üí yolov8s ‚Üí yolov8n
- Close other applications using GPU
- Restart Python kernel to clear memory
```

**3. Training Loss Not Decreasing**
```
Problem: Loss stays flat or increases
Diagnosis:
- Check dataset annotations (visualize in Notebook 03)
- Verify class IDs match dataset.yaml (0-indexed)
- Ensure images and labels have matching filenames
Solutions:
- Lower learning rate: lr0=0.01 ‚Üí lr0=0.001
- Increase epochs: epochs=50 ‚Üí epochs=100
- Try transfer learning (start from yolov8n.pt, not scratch)
- Add data augmentation
```

**4. Model Download Fails**
```
Problem: Ultralytics can't download models
Solutions:
- Check internet connection
- Manual download: https://github.com/ultralytics/assets/releases
- Place .pt file in ~/.cache/ultralytics/ directory
- Or specify local path: YOLO('/path/to/yolov8n.pt')
```

**5. Inference Very Slow on CPU**
```
Problem: 500+ ms per image (2 FPS)
Solutions:
- Use nano model: yolov8x ‚Üí yolov8n (10x faster)
- Reduce image size: imgsz=640 ‚Üí imgsz=416
- Enable GPU if available
- Accept slower speed for CPU-only setups (normal behavior)
```

**6. Poor Detection Results**
```
Problem: Model misses obvious objects
Diagnosis:
- Check confidence threshold: conf=0.25 (default) might be too high
- Verify object classes in model: print(model.names)
Solutions:
- Lower confidence: results = model.predict(conf=0.1)
- Use larger model: yolov8n ‚Üí yolov8s ‚Üí yolov8m
- Ensure object classes exist in pre-trained model (COCO has 80 classes)
- Train custom model if detecting non-COCO objects
```

---

## Assessment Alignment

Week 14 content aligns with **Final Examination** and course outcomes:

### Course Outcome Mapping

**CO-4: Apply deep learning for image processing applications**
- Notebooks 02, 04: Deploying detection models on images/videos
- Assessment: Train and evaluate custom YOLO detector

**CO-5: Design and implement CNN architectures for computer vision**
- Notebooks 01, 05: Understanding YOLO and SSD architectures
- Assessment: Compare architectural choices and tradeoffs

### Typical Exam Questions

**Theory (20 marks):**
1. Explain YOLO's grid-based prediction approach. How does it differ from R-CNN? (5 marks)
2. Describe YOLO's loss function components and their purposes. (4 marks)
3. Compare YOLO vs SSD: architecture, speed, accuracy. When would you choose each? (6 marks)
4. What are anchor boxes in YOLO? How do they help detect objects of different sizes? (5 marks)

**Practical (30 marks):**
1. Write code to load YOLOv8 and detect objects in a given image. (8 marks)
2. Prepare a custom dataset for YOLO training (annotation format, directory structure). (7 marks)
3. Train a YOLO model on provided dataset and report mAP@0.5. (10 marks)
4. Compare inference speed of yolov8n vs yolov8l on 10 test images. (5 marks)

**Study Focus:**
- YOLO architecture: grid cells, anchor boxes, multi-scale predictions
- Training process: loss function, hyperparameters, transfer learning
- Evaluation metrics: mAP@0.5, mAP@0.5:0.95, FPS
- YOLO vs SSD comparison: when to use each

---

## Learning Outcomes

By completing Week 14, you will have gained:

### Technical Skills

‚úÖ **Deploy pre-trained YOLO models** for object detection on images, videos, webcam
‚úÖ **Understand YOLO architecture** deeply: grid predictions, anchor boxes, loss function
‚úÖ **Prepare custom datasets** in YOLO format with proper annotations and structure
‚úÖ **Train YOLO models** on custom data using transfer learning and hyperparameter tuning
‚úÖ **Evaluate detectors** using mAP, precision-recall curves, speed benchmarks
‚úÖ **Compare architectures** (YOLO vs SSD) and choose appropriately for applications
‚úÖ **Debug detection systems**: visualize predictions, analyze failure cases

### Conceptual Understanding

‚úÖ Single-shot detection paradigm vs two-stage detectors
‚úÖ Speed-accuracy tradeoffs in real-time detection
‚úÖ Multi-scale detection strategies (SSD's feature pyramid)
‚úÖ Transfer learning effectiveness in object detection
‚úÖ Production deployment considerations (model size, FPS, hardware)

### Practical Capabilities

‚úÖ Build production-ready object detectors in 1-2 hours
‚úÖ Train custom detectors achieving 60-70% mAP
‚úÖ Deploy models on edge devices, cloud servers, or mobile apps
‚úÖ Debug and improve model performance systematically
‚úÖ Make informed architecture choices for project constraints

---

## Assignment Ideas

### Beginner Level

**Assignment 1: Pre-trained Model Exploration (30 minutes)**
- Run YOLOv8n on 10 personal photos
- Document: detection accuracy, missed objects, false positives
- Experiment with confidence thresholds (0.25, 0.5, 0.75)
- Compare yolov8n vs yolov8s: which is better for your images?

**Assignment 2: Video Detection (45 minutes)**
- Process a 1-minute video with YOLO
- Count total objects detected per frame
- Create annotated output video
- Measure average FPS and inference time

### Intermediate Level

**Assignment 3: Custom Dataset Training (2-3 hours)**
- Collect 100-200 images for 2-3 custom classes
- Annotate using LabelImg or Roboflow
- Train YOLOv8n for 50 epochs
- Achieve minimum 50% mAP@0.5
- Deliverable: Training report with mAP curves and sample detections

**Assignment 4: YOLO Hyperparameter Tuning (2 hours)**
- Use provided dataset (e.g., 500 images, 5 classes)
- Train with different configurations:
  - Baseline: default hyperparameters
  - Experiment 1: Different learning rates (0.001, 0.01, 0.1)
  - Experiment 2: Different batch sizes (8, 16, 32)
  - Experiment 3: Different image sizes (416, 640, 896)
- Report: Best configuration and mAP achieved

### Advanced Level

**Assignment 5: Real-Time Application Development (4-5 hours)**
- Build real-time detection system using webcam
- Implement one of:
  - People counter for room occupancy
  - Vehicle speed estimator using detection + tracking
  - Safety violation detector (e.g., no helmet, no mask)
- Requirements:
  - Real-time performance (15+ FPS)
  - Visual display with annotations
  - Log detection statistics
- Deliverable: Working application + demo video

**Assignment 6: Multi-Model Comparison (3-4 hours)**
- Compare 4 models on same test set (100 images):
  - YOLOv8n, YOLOv8s, YOLOv8m, SSD MobileNet
- Metrics to measure:
  - mAP@0.5 and mAP@0.5:0.95
  - Inference time (ms) and FPS
  - Model size (MB) and memory usage
  - Per-class AP for each model
- Deliverable: Comprehensive comparison report with visualizations

---

## Additional Resources

### Official Documentation
- **Ultralytics YOLOv8:** https://docs.ultralytics.com
  - Installation, training, inference tutorials
  - Model zoo and performance benchmarks
- **YOLO Paper (Original):** "You Only Look Once: Unified, Real-Time Object Detection" (Redmon et al., 2016)
- **SSD Paper:** "SSD: Single Shot MultiBox Detector" (Liu et al., 2016)

### Video Tutorials
- **YOLOv8 Complete Guide:** https://www.youtube.com/watch?v=... (search "YOLOv8 tutorial")
- **Custom YOLO Training:** Ultralytics official YouTube channel
- **Object Detection Comparison:** Two Minute Papers (visual explanations)

### Datasets for Practice
- **COCO Dataset:** 80 classes, 330K images - industry standard
- **Pascal VOC:** 20 classes, 11K images - smaller, good for learning
- **Roboflow Universe:** 100,000+ public datasets for custom training
- **Open Images:** 600 classes, 9M images - most comprehensive

### Annotation Tools
- **LabelImg:** Desktop tool (Windows/Mac/Linux) - free, simple
- **Roboflow:** Web-based with auto-labeling - free tier available
- **CVAT:** Advanced web tool - supports video annotation
- **Labelbox:** Professional tool - enterprise features

### Interactive Demos
- **Ultralytics HUB:** https://hub.ultralytics.com - train models in browser
- **Roboflow Inference:** Test models on webcam instantly
- **Gradio YOLO Apps:** Community-built web interfaces

### Books and Papers
- **Dive into Deep Learning (d2l.ai):** Chapter on Object Detection
- **Deep Learning for Computer Vision (Rajalingappaa Shanmugamani):** YOLO chapter
- **YOLOv3 Paper:** Best explanations of YOLO improvements

---

## Session Timeline

### Week 14 in Course Schedule

**Context:** Week 14 is the second week of Module 5 (Object Detection), occurring in mid-November.

```
Course Timeline:
‚îú‚îÄ Weeks 1-3:  Module 1 (Intro to DL, MLPs, TensorFlow)
‚îú‚îÄ Weeks 4-6:  Module 2 (Optimization, Regularization)
‚îú‚îÄ Weeks 7-9:  Module 3 (Image Processing, Features)
‚îú‚îÄ Weeks 10-12: Module 4 (CNNs, Transfer Learning)
‚îî‚îÄ Weeks 13-15: Module 5 (Object Detection) ‚Üê YOU ARE HERE
   ‚îú‚îÄ Week 13: Detection Fundamentals (IoU, mAP, NMS)
   ‚îú‚îÄ Week 14: YOLO & SSD (Real-Time Detection) ‚≠ê
   ‚îî‚îÄ Week 15: R-CNN Family (Two-Stage Detectors)
```

**Preparation Before Week 14:**
- Complete Week 13 notebooks (MANDATORY!)
- Review CNN architectures (Weeks 10-11)
- Ensure Python environment ready with GPU access if possible

**During Week 14:**
- Day 1: Notebooks 01-02 (theory + hands-on detection)
- Day 2: Notebooks 03-04 (dataset prep + training)
- Day 3: Notebooks 05-06 (SSD + comparison)

**After Week 14:**
- Week 15: Learn R-CNN (two-stage detectors) for comparison
- Final Exam Prep: Review YOLO architecture, training, evaluation
- Capstone Project: Deploy custom YOLO model for real application

---

## Checklist for Students

Use this checklist to track your Week 14 progress:

### Before Starting Week 14
- [ ] ‚úÖ Completed Week 13 (all 5 notebooks)
- [ ] ‚úÖ Understand IoU, NMS, and mAP calculations
- [ ] ‚úÖ Python environment ready (Google Colab or local Jupyter)
- [ ] ‚úÖ GPU access verified (for Notebook 04) or Colab GPU enabled

### Notebook 01: YOLO Architecture
- [ ] Understand grid-based prediction approach
- [ ] Explain anchor boxes and their purpose
- [ ] Describe YOLO loss function components
- [ ] Trace YOLOv1 ‚Üí YOLOv8 evolution

### Notebook 02: Pre-trained Detection (‚≠ê CRITICAL)
- [ ] Install Ultralytics library successfully
- [ ] Load and run YOLOv8n model
- [ ] Detect objects in at least 5 test images
- [ ] Understand detection output format (boxes, scores, classes)
- [ ] Experiment with different model sizes (n, s, m)

### Notebook 03: Dataset Preparation
- [ ] Understand YOLO annotation format (normalized coordinates)
- [ ] Create sample annotation files manually
- [ ] Build proper dataset directory structure
- [ ] Write dataset.yaml configuration
- [ ] Validate dataset with visualization

### Notebook 04: Custom Training (‚≠ê CRITICAL)
- [ ] Configure training hyperparameters
- [ ] Start training with transfer learning
- [ ] Monitor loss curves and mAP during training
- [ ] Achieve minimum 50% mAP@0.5 on validation set
- [ ] Save and load trained model weights
- [ ] Run inference with custom-trained model

### Notebook 05: SSD Architecture
- [ ] Understand multi-scale feature maps
- [ ] Compare SSD vs YOLO architectures
- [ ] Run pre-trained SSD model
- [ ] Analyze SSD performance on small objects

### Notebook 06: YOLO vs SSD Comparison
- [ ] Benchmark both architectures on same test set
- [ ] Measure FPS and mAP for both
- [ ] Understand speed-accuracy tradeoffs
- [ ] Know when to choose YOLO vs SSD

### Final Week 14 Outcomes
- [ ] Can deploy YOLO models in <5 minutes
- [ ] Trained at least one custom YOLO detector
- [ ] Achieved 50%+ mAP on custom dataset
- [ ] Understand YOLO architecture deeply
- [ ] Can explain YOLO vs SSD vs R-CNN differences
- [ ] Ready for Week 15 (R-CNN family)

---

## Success Metrics

### How to Know You've Mastered Week 14

**Minimum Success (60% Proficiency):**
- ‚úÖ Run pre-trained YOLOv8 and get detections on test images
- ‚úÖ Understand basic YOLO architecture (grids, anchor boxes)
- ‚úÖ Prepare simple dataset in YOLO format
- ‚úÖ Complete one training run (even if mAP is low)

**Good Success (75% Proficiency):**
- ‚úÖ Deploy YOLO on images and videos confidently
- ‚úÖ Explain YOLO loss function and training process
- ‚úÖ Train custom model achieving 60%+ mAP@0.5
- ‚úÖ Compare YOLO vs SSD meaningfully
- ‚úÖ Debug common training issues (overfitting, low mAP)

**Excellent Success (90%+ Proficiency):**
- ‚úÖ Build production-ready detection systems from scratch
- ‚úÖ Train multiple models with hyperparameter tuning
- ‚úÖ Achieve 70%+ mAP on custom datasets
- ‚úÖ Optimize for deployment (speed vs accuracy)
- ‚úÖ Implement real-time detection applications (webcam, video)
- ‚úÖ Contribute to class discussions on architecture tradeoffs

**Mastery Indicators:**
- Can implement YOLO-based application in 1-2 hours without guidance
- Understand subtle architectural differences between YOLO versions
- Make informed decisions on model selection for projects
- Debug and improve model performance systematically
- Teach YOLO concepts to others clearly

---

## Support

### Getting Help

**During Class:**
- Ask instructor during live demos (Notebooks 02, 04)
- Collaborate with classmates on dataset preparation
- Share training results and troubleshoot together

**Office Hours:**
- Bring specific errors (screenshots help!)
- Discuss training convergence issues
- Get feedback on custom dataset quality
- Review mAP scores and improvement strategies

**Online Resources:**
- **Ultralytics Docs:** Most questions answered here
- **GitHub Issues:** https://github.com/ultralytics/ultralytics/issues
- **Stack Overflow:** Tag [yolo] [object-detection]
- **Course Forum:** Post questions for peer/instructor help

**Common Questions:**
1. "Training loss not decreasing" ‚Üí Check dataset, lower learning rate, verify annotations
2. "Out of memory error" ‚Üí Reduce batch size, use smaller model, enable gradient checkpointing
3. "Low mAP despite low loss" ‚Üí Possible overfitting, need more data or augmentation
4. "Detections are terrible" ‚Üí Check confidence threshold, verify classes, try larger model

### Contact Information

**Instructor:** Prof. Ramesh Babu
**Course:** 21CSE558T - Deep Neural Network Architectures
**Office:** [Your office location]
**Email:** [Your email]
**Office Hours:** [Your schedule]

---

## Final Notes

**Week 14 is EXCITING!** üöÄ

This is the week where abstract concepts (CNNs, transfer learning, evaluation metrics) come together into a working system you can deploy TODAY. The instant gratification of Notebook 02 (seeing YOLO detect objects in seconds) is incredibly motivating.

**Don't skip Week 13!** We can't emphasize this enough. Without solid understanding of IoU, NMS, and mAP, Week 14 will feel like magic - impressive but confusing. Week 13 gives you the foundation to understand WHY YOLO works, not just HOW to use it.

**GPU access matters for training.** If you don't have a local GPU, Google Colab's free T4 GPU is perfect for this course. The 30-minute training time vs 4+ hours on CPU makes the difference between learning effectively and getting frustrated.

**Real-time detection is POWERFUL.** Once you complete Week 14, you'll have a highly marketable skill. YOLO powers countless production systems: autonomous vehicles, surveillance, robotics, AR applications. You're learning technology that's actively reshaping industries.

**Enjoy the journey!** Object detection is one of the most visually rewarding areas of deep learning. Seeing those bounding boxes appear around detected objects never gets old. Have fun, experiment, and build something cool!

---

**Ready to detect? Start with Notebook 01! üöÄ**

*Last Updated: November 2025*
*Course Materials by Prof. Ramesh Babu*
*SRM University - M.Tech Deep Learning Course*
