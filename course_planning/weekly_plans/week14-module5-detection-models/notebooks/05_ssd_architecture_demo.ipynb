{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 05: SSD Architecture Demo\n",
    "\n",
    "**Week 14 - Module 5: Object Detection Models**  \n",
    "**Understanding Single Shot MultiBox Detector (SSD)**\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand SSD (Single Shot Detector) architecture\n",
    "- Compare SSD with YOLO conceptually\n",
    "- Use pre-trained SSD for detection\n",
    "- Analyze multi-scale feature detection\n",
    "- Understand default boxes and their role\n",
    "\n",
    "**Estimated Time:** 15 minutes  \n",
    "**Prerequisites:** Basic understanding of CNNs and object detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is SSD?\n",
    "\n",
    "### SSD: Single Shot MultiBox Detector\n",
    "\n",
    "**Published:** December 2016 (same year as YOLOv1)  \n",
    "**Authors:** Liu et al., University of North Carolina  \n",
    "**Key Innovation:** Multi-scale feature maps for detection\n",
    "\n",
    "### Core Principles:\n",
    "1. **Single Shot**: Detect objects in one forward pass (like YOLO)\n",
    "2. **MultiBox**: Multiple default boxes (anchors) at each location\n",
    "3. **Multi-Scale**: Use feature maps from different network layers\n",
    "\n",
    "### Why SSD?\n",
    "- ‚úÖ **Real-time performance**: 59 FPS on SSD300, 22 FPS on SSD512\n",
    "- ‚úÖ **Good accuracy**: 74.3% mAP (SSD300), 76.8% mAP (SSD512)\n",
    "- ‚úÖ **Multi-scale detection**: Better for objects of varying sizes\n",
    "- ‚úÖ **Flexibility**: Works with different backbones (VGG, ResNet, MobileNet)\n",
    "\n",
    "### SSD vs YOLO (Quick Overview):\n",
    "- **YOLO**: 3 detection scales, custom backbone, simpler architecture\n",
    "- **SSD**: 6 detection scales, VGG/ResNet backbone, more complex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SSD vs YOLO: Conceptual Comparison\n",
    "\n",
    "| Feature | YOLO | SSD |\n",
    "|---------|------|-----|\n",
    "| **Architecture** | Custom (Darknet) | VGG16, ResNet, MobileNet |\n",
    "| **Feature Scales** | 3 scales | 6 scales |\n",
    "| **Default Boxes** | 3 anchors per scale | 4-6 boxes per scale |\n",
    "| **Total Anchors** | ~10,647 | 8,732 (SSD300) |\n",
    "| **Input Size** | 640√ó640 (YOLOv8) | 300√ó300 or 512√ó512 |\n",
    "| **Speed (FPS)** | ~45 (YOLOv8n) | 59 (SSD300), 22 (SSD512) |\n",
    "| **mAP** | 37.3% (YOLOv8n) | 74.3% (SSD300), 76.8% (SSD512) |\n",
    "| **First Release** | 2016 (YOLOv1) | 2016 |\n",
    "| **Latest Version** | 2023 (YOLOv8) | SSD with various backbones |\n",
    "| **Popularity** | High (active development) | Moderate (established) |\n",
    "| **Ease of Use** | Very easy (Ultralytics) | Moderate (TensorFlow/PyTorch) |\n",
    "| **Best For** | General object detection | Multi-scale detection |\n",
    "\n",
    "### Key Differences:\n",
    "1. **Multi-Scale Strategy**: SSD uses more feature maps (6 vs 3)\n",
    "2. **Backbone**: SSD flexible (VGG, ResNet), YOLO custom\n",
    "3. **Default Boxes**: SSD has more aspect ratios per location\n",
    "4. **Training**: YOLO easier to train, SSD requires more tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SSD Architecture Diagram\n",
    "\n",
    "```\n",
    "Input Image (300√ó300√ó3)\n",
    "        |\n",
    "        v\n",
    "   VGG-16 Base Network\n",
    "        |\n",
    "        |--- Conv4_3 (38√ó38√ó512) --> Detections (4 boxes/cell = 5,776 boxes)\n",
    "        |\n",
    "        v\n",
    "        |--- FC7 (19√ó19√ó1024) -----> Detections (6 boxes/cell = 2,166 boxes)\n",
    "        |\n",
    "        v\n",
    "   Extra Feature Layers\n",
    "        |\n",
    "        |--- Conv8_2 (10√ó10√ó512) ---> Detections (6 boxes/cell = 600 boxes)\n",
    "        |\n",
    "        |--- Conv9_2 (5√ó5√ó256) -----> Detections (6 boxes/cell = 150 boxes)\n",
    "        |\n",
    "        |--- Conv10_2 (3√ó3√ó256) ----> Detections (4 boxes/cell = 36 boxes)\n",
    "        |\n",
    "        |--- Conv11_2 (1√ó1√ó256) ----> Detections (4 boxes/cell = 4 boxes)\n",
    "        |\n",
    "        v\n",
    "   Total: 8,732 default boxes\n",
    "        |\n",
    "        v\n",
    "   Non-Maximum Suppression (NMS)\n",
    "        |\n",
    "        v\n",
    "   Final Detections\n",
    "```\n",
    "\n",
    "### Layer-by-Layer Breakdown:\n",
    "1. **Conv4_3**: Detects small objects (early features, high resolution)\n",
    "2. **FC7**: Detects medium objects\n",
    "3. **Conv8_2 - Conv11_2**: Detect progressively larger objects\n",
    "\n",
    "### Why Multi-Scale?\n",
    "- **Small objects** (e.g., person far away): Detected in high-resolution layers (38√ó38)\n",
    "- **Large objects** (e.g., car close-up): Detected in low-resolution layers (1√ó1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q tensorflow tensorflow-hub opencv-python matplotlib numpy pillow\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import urllib.request\n",
    "from pathlib import Path\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU available: {tf.test.is_gpu_available()}\")\n",
    "print(\"\\n‚úÖ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pre-trained SSD Model\n",
    "\n",
    "We'll use SSD MobileNet V2 from TensorFlow Hub (trained on COCO dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained SSD MobileNet V2 from TensorFlow Hub\n",
    "print(\"üì• Loading SSD MobileNet V2 model...\")\n",
    "\n",
    "model_url = \"https://tfhub.dev/tensorflow/ssd_mobilenet_v2/2\"\n",
    "detector = hub.load(model_url)\n",
    "\n",
    "print(\"‚úÖ Model loaded successfully!\")\n",
    "print(\"\\nüìä Model Details:\")\n",
    "print(\"  Architecture: SSD MobileNet V2\")\n",
    "print(\"  Trained on: COCO dataset (90 classes)\")\n",
    "print(\"  Input size: 320√ó320 (flexible)\")\n",
    "print(\"  Speed: ~25 FPS on CPU\")\n",
    "\n",
    "# COCO class names (80 classes)\n",
    "COCO_CLASSES = [\n",
    "    'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat',\n",
    "    'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat',\n",
    "    'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack',\n",
    "    'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n",
    "    'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',\n",
    "    'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple',\n",
    "    'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake',\n",
    "    'chair', 'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop',\n",
    "    'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink',\n",
    "    'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n",
    "]\n",
    "\n",
    "print(f\"\\n  Classes: {len(COCO_CLASSES)} (person, car, dog, etc.)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run SSD Detection on Sample Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download sample test images\n",
    "test_images = [\n",
    "    ('https://ultralytics.com/images/bus.jpg', 'bus.jpg'),\n",
    "    ('https://ultralytics.com/images/zidane.jpg', 'zidane.jpg'),\n",
    "]\n",
    "\n",
    "for url, filename in test_images:\n",
    "    if not Path(filename).exists():\n",
    "        urllib.request.urlretrieve(url, filename)\n",
    "        print(f\"‚úÖ Downloaded: {filename}\")\n",
    "\n",
    "print(\"\\nüì∏ Test images ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ssd_detection(image_path, confidence_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Run SSD object detection on an image.\n",
    "    \n",
    "    Args:\n",
    "        image_path: Path to input image\n",
    "        confidence_threshold: Minimum confidence score (0-1)\n",
    "    \n",
    "    Returns:\n",
    "        detections: Dictionary with boxes, scores, classes\n",
    "    \"\"\"\n",
    "    # Load image\n",
    "    image = Image.open(image_path)\n",
    "    image_np = np.array(image)\n",
    "    \n",
    "    # Convert to tensor\n",
    "    input_tensor = tf.convert_to_tensor(image_np)\n",
    "    input_tensor = input_tensor[tf.newaxis, ...]\n",
    "    \n",
    "    # Run detection\n",
    "    detections = detector(input_tensor)\n",
    "    \n",
    "    # Extract results\n",
    "    num_detections = int(detections.pop('num_detections'))\n",
    "    detections = {key: value[0, :num_detections].numpy()\n",
    "                  for key, value in detections.items()}\n",
    "    detections['num_detections'] = num_detections\n",
    "    \n",
    "    # Filter by confidence\n",
    "    indices = detections['detection_scores'] >= confidence_threshold\n",
    "    \n",
    "    return {\n",
    "        'boxes': detections['detection_boxes'][indices],\n",
    "        'scores': detections['detection_scores'][indices],\n",
    "        'classes': detections['detection_classes'][indices].astype(int),\n",
    "        'image': image_np\n",
    "    }\n",
    "\n",
    "def visualize_detections(detections, class_names=COCO_CLASSES):\n",
    "    \"\"\"\n",
    "    Visualize SSD detection results.\n",
    "    \"\"\"\n",
    "    image = detections['image'].copy()\n",
    "    h, w = image.shape[:2]\n",
    "    \n",
    "    # Draw bounding boxes\n",
    "    for box, score, class_id in zip(detections['boxes'], detections['scores'], detections['classes']):\n",
    "        ymin, xmin, ymax, xmax = box\n",
    "        left, right, top, bottom = int(xmin * w), int(xmax * w), int(ymin * h), int(ymax * h)\n",
    "        \n",
    "        # Draw box\n",
    "        cv2.rectangle(image, (left, top), (right, bottom), (0, 255, 0), 2)\n",
    "        \n",
    "        # Draw label\n",
    "        class_name = class_names[class_id - 1] if class_id <= len(class_names) else f'Class {class_id}'\n",
    "        label = f'{class_name}: {score:.2f}'\n",
    "        cv2.putText(image, label, (left, top - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "    \n",
    "    return image\n",
    "\n",
    "# Run detection on test images\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 7))\n",
    "\n",
    "for idx, (_, filename) in enumerate(test_images):\n",
    "    print(f\"\\nüîç Detecting objects in {filename}...\")\n",
    "    \n",
    "    detections = run_ssd_detection(filename, confidence_threshold=0.5)\n",
    "    vis_image = visualize_detections(detections)\n",
    "    \n",
    "    axes[idx].imshow(cv2.cvtColor(vis_image, cv2.COLOR_BGR2RGB))\n",
    "    axes[idx].set_title(f'{filename} - {len(detections[\"boxes\"])} detections', fontsize=12, fontweight='bold')\n",
    "    axes[idx].axis('off')\n",
    "    \n",
    "    # Print detection details\n",
    "    print(f\"  Found {len(detections['boxes'])} objects:\")\n",
    "    for box, score, class_id in zip(detections['boxes'], detections['scores'], detections['classes']):\n",
    "        class_name = COCO_CLASSES[class_id - 1] if class_id <= len(COCO_CLASSES) else f'Class {class_id}'\n",
    "        print(f\"    - {class_name}: {score:.2f}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Scale Detection Visualization\n",
    "\n",
    "Let's understand how SSD detects objects at different scales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize multi-scale detection concept\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('SSD Multi-Scale Feature Maps', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Define feature map sizes for SSD300\n",
    "feature_maps = [\n",
    "    ('Conv4_3', 38, 'Small objects'),\n",
    "    ('FC7', 19, 'Medium objects'),\n",
    "    ('Conv8_2', 10, 'Medium-large objects'),\n",
    "    ('Conv9_2', 5, 'Large objects'),\n",
    "    ('Conv10_2', 3, 'Very large objects'),\n",
    "    ('Conv11_2', 1, 'Huge objects'),\n",
    "]\n",
    "\n",
    "# Create visualization for each feature map\n",
    "for idx, (name, size, description) in enumerate(feature_maps):\n",
    "    ax = axes[idx // 3, idx % 3]\n",
    "    \n",
    "    # Create grid\n",
    "    grid = np.zeros((size, size))\n",
    "    \n",
    "    # Highlight some cells\n",
    "    if size > 1:\n",
    "        grid[size//4:3*size//4, size//4:3*size//4] = 0.5\n",
    "    else:\n",
    "        grid[0, 0] = 0.5\n",
    "    \n",
    "    # Plot\n",
    "    ax.imshow(grid, cmap='viridis', interpolation='nearest')\n",
    "    ax.set_title(f'{name}\\n{size}√ó{size}\\n({description})', fontsize=11, fontweight='bold')\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    \n",
    "    # Add grid lines\n",
    "    for i in range(size + 1):\n",
    "        ax.axhline(i - 0.5, color='white', linewidth=0.5)\n",
    "        ax.axvline(i - 0.5, color='white', linewidth=0.5)\n",
    "    \n",
    "    # Add text\n",
    "    boxes_per_cell = 4 if size in [38, 3, 1] else 6\n",
    "    total_boxes = size * size * boxes_per_cell\n",
    "    ax.text(0.5, -0.15, f'{total_boxes} default boxes', \n",
    "            ha='center', transform=ax.transAxes, fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Feature Map Summary:\")\n",
    "print(\"  Early layers (38√ó38): High resolution ‚Üí Detect small objects\")\n",
    "print(\"  Middle layers (19√ó19, 10√ó10): Medium resolution ‚Üí Detect medium objects\")\n",
    "print(\"  Late layers (5√ó5, 3√ó3, 1√ó1): Low resolution ‚Üí Detect large objects\")\n",
    "print(\"\\n  Total default boxes: 8,732 (across all scales)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default Boxes Explanation\n",
    "\n",
    "### What are Default Boxes?\n",
    "Default boxes (also called **anchor boxes** or **priors**) are predefined bounding boxes:\n",
    "- Fixed sizes and aspect ratios\n",
    "- Placed at each cell in feature maps\n",
    "- Network predicts **offsets** from these boxes\n",
    "\n",
    "### Default Box Configuration:\n",
    "\n",
    "| Feature Map | Size | Boxes/Cell | Aspect Ratios | Total Boxes |\n",
    "|-------------|------|------------|---------------|-------------|\n",
    "| Conv4_3 | 38√ó38 | 4 | 1:1, 1:2, 2:1, 1:1 (extra) | 5,776 |\n",
    "| FC7 | 19√ó19 | 6 | 1:1, 1:2, 2:1, 1:3, 3:1, 1:1 | 2,166 |\n",
    "| Conv8_2 | 10√ó10 | 6 | 1:1, 1:2, 2:1, 1:3, 3:1, 1:1 | 600 |\n",
    "| Conv9_2 | 5√ó5 | 6 | 1:1, 1:2, 2:1, 1:3, 3:1, 1:1 | 150 |\n",
    "| Conv10_2 | 3√ó3 | 4 | 1:1, 1:2, 2:1, 1:1 (extra) | 36 |\n",
    "| Conv11_2 | 1√ó1 | 4 | 1:1, 1:2, 2:1, 1:1 (extra) | 4 |\n",
    "| **Total** | | | | **8,732** |\n",
    "\n",
    "### Why So Many Boxes?\n",
    "- **Coverage**: Ensure at least one box overlaps with every object\n",
    "- **Aspect Ratios**: Match different object shapes (person=tall, car=wide)\n",
    "- **Scales**: Match different object sizes\n",
    "\n",
    "### Training Process:\n",
    "1. Match default boxes to ground truth objects (IoU > 0.5)\n",
    "2. Predict **class scores** for each box\n",
    "3. Predict **offset adjustments** (Œîx, Œîy, Œîw, Œîh)\n",
    "4. Apply Non-Maximum Suppression (NMS) to remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize default boxes at different aspect ratios\n",
    "fig, axes = plt.subplots(1, 5, figsize=(20, 4))\n",
    "fig.suptitle('SSD Default Box Aspect Ratios', fontsize=16, fontweight='bold')\n",
    "\n",
    "aspect_ratios = [\n",
    "    (1, 1, '1:1 (Square)'),\n",
    "    (1, 2, '1:2 (Tall)'),\n",
    "    (2, 1, '2:1 (Wide)'),\n",
    "    (1, 3, '1:3 (Very Tall)'),\n",
    "    (3, 1, '3:1 (Very Wide)'),\n",
    "]\n",
    "\n",
    "for idx, (w, h, label) in enumerate(aspect_ratios):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Draw box\n",
    "    box_w, box_h = w * 0.3, h * 0.3\n",
    "    rect = plt.Rectangle((0.5 - box_w/2, 0.5 - box_h/2), box_w, box_h, \n",
    "                          fill=False, edgecolor='red', linewidth=3)\n",
    "    ax.add_patch(rect)\n",
    "    \n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_title(label, fontsize=12, fontweight='bold')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìè Aspect Ratio Examples:\")\n",
    "print(\"  1:1 - General objects, cars\")\n",
    "print(\"  1:2 - People, bottles\")\n",
    "print(\"  2:1 - Buses, trains\")\n",
    "print(\"  1:3 - Tall buildings, lampposts\")\n",
    "print(\"  3:1 - Horizontal signs, benches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SSD Performance Metrics\n",
    "\n",
    "### Official SSD Results (2016 Paper):\n",
    "\n",
    "| Model | Input Size | mAP (COCO) | FPS (Titan X) |\n",
    "|-------|-----------|------------|---------------|\n",
    "| SSD300 | 300√ó300 | 74.3% | 59 |\n",
    "| SSD512 | 512√ó512 | 76.8% | 22 |\n",
    "\n",
    "### Comparison with Other Detectors (2016):\n",
    "\n",
    "| Model | mAP | FPS | Notes |\n",
    "|-------|-----|-----|-------|\n",
    "| Faster R-CNN | 73.2% | 7 | Two-stage (slow) |\n",
    "| YOLOv1 | 63.4% | 45 | Single-stage (fast) |\n",
    "| SSD300 | 74.3% | 59 | **Best speed/accuracy trade-off** |\n",
    "| SSD512 | 76.8% | 22 | Higher accuracy, slower |\n",
    "\n",
    "### Modern SSD Variants:\n",
    "- **SSD MobileNet**: Lightweight for mobile devices (~25 FPS on CPU)\n",
    "- **SSD ResNet**: Higher accuracy with deeper backbone\n",
    "- **SSDLite**: Optimized for mobile deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SSD Strengths and Weaknesses\n",
    "\n",
    "### Strengths:\n",
    "‚úÖ **Real-time performance**: 59 FPS (SSD300), faster than Faster R-CNN  \n",
    "‚úÖ **Good accuracy**: Competitive with two-stage detectors  \n",
    "‚úÖ **Multi-scale detection**: 6 feature maps ‚Üí better for varying object sizes  \n",
    "‚úÖ **Flexible backbone**: Works with VGG, ResNet, MobileNet  \n",
    "‚úÖ **End-to-end training**: Single network, no region proposals  \n",
    "‚úÖ **Well-established**: Production-ready, TensorFlow/PyTorch support  \n",
    "\n",
    "### Weaknesses:\n",
    "‚ùå **Struggles with small objects**: Despite multi-scale, small objects (<5% image) are challenging  \n",
    "‚ùå **More complex than YOLO**: 8,732 default boxes, harder to tune  \n",
    "‚ùå **Imbalanced training**: Many negative (background) boxes, requires hard negative mining  \n",
    "‚ùå **Fixed input size**: Requires resizing images (300√ó300 or 512√ó512)  \n",
    "‚ùå **Less active development**: Surpassed by YOLO, EfficientDet in recent years  \n",
    "\n",
    "### When to Use SSD:\n",
    "- ‚úÖ Need multi-scale detection (objects of varying sizes)\n",
    "- ‚úÖ Using TensorFlow ecosystem\n",
    "- ‚úÖ Production deployment (stable, well-tested)\n",
    "- ‚úÖ Mobile deployment (SSD MobileNet)\n",
    "\n",
    "### When to Use YOLO Instead:\n",
    "- ‚úÖ Need latest state-of-the-art accuracy\n",
    "- ‚úÖ Easier training and fine-tuning\n",
    "- ‚úÖ Active community and updates\n",
    "- ‚úÖ Simpler architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Compare SSD and YOLO on Same Image\n",
    "\n",
    "Let's run both SSD and YOLOv8 on the same image and compare results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install YOLO for comparison\n",
    "!pip install -q ultralytics\n",
    "\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load YOLOv8\n",
    "yolo_model = YOLO('yolov8n.pt')\n",
    "\n",
    "# Compare on test image\n",
    "test_image = 'bus.jpg'\n",
    "\n",
    "print(\"üîç Running SSD detection...\")\n",
    "ssd_detections = run_ssd_detection(test_image, confidence_threshold=0.5)\n",
    "ssd_vis = visualize_detections(ssd_detections)\n",
    "\n",
    "print(\"üîç Running YOLO detection...\")\n",
    "yolo_results = yolo_model(test_image)\n",
    "yolo_vis = yolo_results[0].plot()\n",
    "\n",
    "# Visualize side-by-side\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 7))\n",
    "\n",
    "axes[0].imshow(cv2.cvtColor(ssd_vis, cv2.COLOR_BGR2RGB))\n",
    "axes[0].set_title(f'SSD MobileNet V2\\n{len(ssd_detections[\"boxes\"])} detections', \n",
    "                  fontsize=14, fontweight='bold')\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(cv2.cvtColor(yolo_vis, cv2.COLOR_BGR2RGB))\n",
    "axes[1].set_title(f'YOLOv8n\\n{len(yolo_results[0].boxes)} detections', \n",
    "                  fontsize=14, fontweight='bold')\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Comparison:\")\n",
    "print(f\"  SSD detections: {len(ssd_detections['boxes'])}\")\n",
    "print(f\"  YOLO detections: {len(yolo_results[0].boxes)}\")\n",
    "print(\"\\n  Both models detect similar objects, but may differ in:\")\n",
    "print(\"    - Confidence scores\")\n",
    "print(\"    - Bounding box precision\")\n",
    "print(\"    - Small object detection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What We Learned:\n",
    "1. ‚úÖ **SSD Architecture**: Multi-scale detection with 6 feature maps\n",
    "2. ‚úÖ **Default Boxes**: 8,732 boxes with multiple aspect ratios\n",
    "3. ‚úÖ **SSD vs YOLO**: Conceptual and architectural differences\n",
    "4. ‚úÖ **Multi-Scale Detection**: How SSD handles objects of varying sizes\n",
    "5. ‚úÖ **Performance**: Real-time speed (59 FPS) with good accuracy (74.3% mAP)\n",
    "\n",
    "### Key Takeaways:\n",
    "- **SSD = Multi-scale + Single-shot**: Combines speed and accuracy\n",
    "- **6 feature maps**: 38√ó38 (small objects) ‚Üí 1√ó1 (large objects)\n",
    "- **8,732 default boxes**: Ensure comprehensive coverage\n",
    "- **Trade-off**: SSD512 more accurate, SSD300 faster\n",
    "- **Production-ready**: TensorFlow/PyTorch support, mobile deployment\n",
    "\n",
    "### SSD vs YOLO Decision:\n",
    "- **Choose SSD**: TensorFlow ecosystem, multi-scale focus, mobile deployment\n",
    "- **Choose YOLO**: Latest accuracy, easier training, active development\n",
    "\n",
    "### Next Steps:\n",
    "- **Notebook 06**: Comprehensive YOLO vs SSD benchmark comparison\n",
    "- **Practice**: Train SSD on custom dataset (similar to YOLOv8 training)\n",
    "\n",
    "---\n",
    "\n",
    "**Great job! You now understand SSD architecture and multi-scale detection! üéâ**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
