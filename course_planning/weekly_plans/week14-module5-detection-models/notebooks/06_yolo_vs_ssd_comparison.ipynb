{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 06: YOLO vs SSD Comprehensive Comparison\n",
    "\n",
    "**Week 14 - Module 5: Object Detection Models**  \n",
    "**Final Notebook: Making Informed Model Selection Decisions**\n",
    "\n",
    "## Learning Objectives\n",
    "- Benchmark YOLO vs SSD side-by-side\n",
    "- Compare speed, accuracy, and model size\n",
    "- Analyze detection quality for different object sizes\n",
    "- Develop decision-making framework for model selection\n",
    "- Map real-world applications to appropriate models\n",
    "\n",
    "**Estimated Time:** 15 minutes  \n",
    "**Prerequisites:** Completed Notebooks 04 (YOLOv8) and 05 (SSD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison Criteria\n",
    "\n",
    "We'll evaluate both models on these dimensions:\n",
    "\n",
    "### 1. Accuracy Metrics\n",
    "- **mAP@0.5**: Detection quality at IoU threshold 0.5\n",
    "- **mAP@0.5:0.95**: Average precision across IoU 0.5-0.95\n",
    "- **Per-class performance**: How well each model detects specific classes\n",
    "- **Precision vs Recall**: Trade-offs in detection confidence\n",
    "\n",
    "### 2. Speed Metrics\n",
    "- **Inference time**: Time per image (milliseconds)\n",
    "- **FPS**: Frames per second (real-time = >30 FPS)\n",
    "- **Throughput**: Images processed per second\n",
    "\n",
    "### 3. Model Size\n",
    "- **File size**: Disk space (MB)\n",
    "- **Parameters**: Number of trainable parameters\n",
    "- **Memory usage**: RAM required during inference\n",
    "\n",
    "### 4. Detection Quality\n",
    "- **Small objects**: <32√ó32 pixels\n",
    "- **Medium objects**: 32√ó96 pixels\n",
    "- **Large objects**: >96√ó96 pixels\n",
    "- **Occluded objects**: Partial visibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q ultralytics tensorflow tensorflow-hub opencv-python matplotlib numpy pillow\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import urllib.request\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# YOLO imports\n",
    "from ultralytics import YOLO\n",
    "import torch\n",
    "\n",
    "# SSD imports\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"CUDA available (YOLO): {torch.cuda.is_available()}\")\n",
    "print(f\"GPU available (SSD): {tf.test.is_gpu_available()}\")\n",
    "print(\"\\n‚úÖ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Both Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load YOLOv8 models (nano, small, medium)\n",
    "print(\"üì• Loading YOLO models...\")\n",
    "yolo_n = YOLO('yolov8n.pt')  # Nano: 6MB, fastest\n",
    "yolo_s = YOLO('yolov8s.pt')  # Small: 22MB, balanced\n",
    "yolo_m = YOLO('yolov8m.pt')  # Medium: 52MB, more accurate\n",
    "\n",
    "print(\"‚úÖ YOLO models loaded\")\n",
    "\n",
    "# Load SSD models\n",
    "print(\"\\nüì• Loading SSD models...\")\n",
    "ssd_mobilenet = hub.load(\"https://tfhub.dev/tensorflow/ssd_mobilenet_v2/2\")\n",
    "\n",
    "print(\"‚úÖ SSD models loaded\")\n",
    "\n",
    "print(\"\\nüìä Models Ready for Comparison:\")\n",
    "print(\"  YOLO: yolov8n (6MB), yolov8s (22MB), yolov8m (52MB)\")\n",
    "print(\"  SSD: SSD MobileNet V2 (~20MB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download diverse test images\n",
    "test_images = [\n",
    "    ('https://ultralytics.com/images/bus.jpg', 'bus.jpg', 'Large objects'),\n",
    "    ('https://ultralytics.com/images/zidane.jpg', 'zidane.jpg', 'Medium objects'),\n",
    "    ('https://images.unsplash.com/photo-1506905925346-21bda4d32df4', 'crowd.jpg', 'Small objects'),\n",
    "]\n",
    "\n",
    "print(\"üì• Downloading test images...\")\n",
    "for url, filename, description in test_images:\n",
    "    if not Path(filename).exists():\n",
    "        try:\n",
    "            urllib.request.urlretrieve(url, filename)\n",
    "            print(f\"  ‚úÖ {filename} ({description})\")\n",
    "        except:\n",
    "            print(f\"  ‚ö†Ô∏è Failed to download {filename}\")\n",
    "\n",
    "# Use existing images\n",
    "test_image_paths = [f for _, f, _ in test_images if Path(f).exists()]\n",
    "print(f\"\\n‚úÖ {len(test_image_paths)} test images ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Same Image Detection Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for SSD detection\n",
    "def run_ssd_detection(image_path, detector, confidence_threshold=0.5):\n",
    "    \"\"\"Run SSD detection on an image.\"\"\"\n",
    "    image = Image.open(image_path)\n",
    "    image_np = np.array(image)\n",
    "    \n",
    "    input_tensor = tf.convert_to_tensor(image_np)\n",
    "    input_tensor = input_tensor[tf.newaxis, ...]\n",
    "    \n",
    "    detections = detector(input_tensor)\n",
    "    \n",
    "    num_detections = int(detections.pop('num_detections'))\n",
    "    detections = {key: value[0, :num_detections].numpy()\n",
    "                  for key, value in detections.items()}\n",
    "    \n",
    "    indices = detections['detection_scores'] >= confidence_threshold\n",
    "    \n",
    "    return {\n",
    "        'boxes': detections['detection_boxes'][indices],\n",
    "        'scores': detections['detection_scores'][indices],\n",
    "        'classes': detections['detection_classes'][indices].astype(int),\n",
    "        'num': len(detections['detection_boxes'][indices])\n",
    "    }\n",
    "\n",
    "def visualize_comparison(image_path, yolo_results, ssd_detections):\n",
    "    \"\"\"Visualize YOLO vs SSD detections side-by-side.\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 7))\n",
    "    \n",
    "    # YOLO visualization\n",
    "    yolo_img = yolo_results.plot()\n",
    "    axes[0].imshow(cv2.cvtColor(yolo_img, cv2.COLOR_BGR2RGB))\n",
    "    axes[0].set_title(f'YOLOv8n\\n{len(yolo_results.boxes)} detections', \n",
    "                      fontsize=14, fontweight='bold')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # SSD visualization\n",
    "    image = cv2.imread(image_path)\n",
    "    h, w = image.shape[:2]\n",
    "    \n",
    "    for box, score in zip(ssd_detections['boxes'], ssd_detections['scores']):\n",
    "        ymin, xmin, ymax, xmax = box\n",
    "        left, right, top, bottom = int(xmin * w), int(xmax * w), int(ymin * h), int(ymax * h)\n",
    "        cv2.rectangle(image, (left, top), (right, bottom), (0, 255, 0), 2)\n",
    "        cv2.putText(image, f'{score:.2f}', (left, top - 10), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "    \n",
    "    axes[1].imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    axes[1].set_title(f'SSD MobileNet V2\\n{ssd_detections[\"num\"]} detections', \n",
    "                      fontsize=14, fontweight='bold')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Run comparison on test images\n",
    "print(\"üîç Running detections on test images...\\n\")\n",
    "\n",
    "for image_path in test_image_paths[:2]:  # First 2 images\n",
    "    print(f\"Processing: {image_path}\")\n",
    "    \n",
    "    # YOLO detection\n",
    "    yolo_results = yolo_n(image_path)[0]\n",
    "    \n",
    "    # SSD detection\n",
    "    ssd_detections = run_ssd_detection(image_path, ssd_mobilenet, confidence_threshold=0.5)\n",
    "    \n",
    "    # Visualize\n",
    "    visualize_comparison(image_path, yolo_results, ssd_detections)\n",
    "    \n",
    "    print(f\"  YOLO: {len(yolo_results.boxes)} objects\")\n",
    "    print(f\"  SSD: {ssd_detections['num']} objects\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speed Benchmark Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark function\n",
    "def benchmark_model(model, image_paths, model_type='yolo', num_runs=10):\n",
    "    \"\"\"\n",
    "    Benchmark inference speed.\n",
    "    \n",
    "    Args:\n",
    "        model: YOLO or SSD model\n",
    "        image_paths: List of test images\n",
    "        model_type: 'yolo' or 'ssd'\n",
    "        num_runs: Number of benchmark iterations\n",
    "    \n",
    "    Returns:\n",
    "        dict: Benchmark results\n",
    "    \"\"\"\n",
    "    times = []\n",
    "    \n",
    "    # Warmup\n",
    "    for _ in range(3):\n",
    "        if model_type == 'yolo':\n",
    "            _ = model(image_paths[0], verbose=False)\n",
    "        else:\n",
    "            _ = run_ssd_detection(image_paths[0], model, confidence_threshold=0.5)\n",
    "    \n",
    "    # Benchmark\n",
    "    for image_path in image_paths:\n",
    "        for _ in range(num_runs):\n",
    "            start = time.time()\n",
    "            \n",
    "            if model_type == 'yolo':\n",
    "                _ = model(image_path, verbose=False)\n",
    "            else:\n",
    "                _ = run_ssd_detection(image_path, model, confidence_threshold=0.5)\n",
    "            \n",
    "            end = time.time()\n",
    "            times.append((end - start) * 1000)  # Convert to ms\n",
    "    \n",
    "    return {\n",
    "        'mean_time_ms': np.mean(times),\n",
    "        'std_time_ms': np.std(times),\n",
    "        'min_time_ms': np.min(times),\n",
    "        'max_time_ms': np.max(times),\n",
    "        'fps': 1000 / np.mean(times)\n",
    "    }\n",
    "\n",
    "# Run benchmarks\n",
    "print(\"‚ö° Benchmarking models (10 runs per image)...\\n\")\n",
    "\n",
    "benchmarks = {}\n",
    "\n",
    "# YOLO benchmarks\n",
    "print(\"YOLOv8n...\")\n",
    "benchmarks['YOLOv8n'] = benchmark_model(yolo_n, test_image_paths, 'yolo')\n",
    "\n",
    "print(\"YOLOv8s...\")\n",
    "benchmarks['YOLOv8s'] = benchmark_model(yolo_s, test_image_paths, 'yolo')\n",
    "\n",
    "print(\"YOLOv8m...\")\n",
    "benchmarks['YOLOv8m'] = benchmark_model(yolo_m, test_image_paths, 'yolo')\n",
    "\n",
    "# SSD benchmark\n",
    "print(\"SSD MobileNet V2...\")\n",
    "benchmarks['SSD MobileNet V2'] = benchmark_model(ssd_mobilenet, test_image_paths, 'ssd')\n",
    "\n",
    "print(\"\\n‚úÖ Benchmarking complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize speed benchmarks\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "models = list(benchmarks.keys())\n",
    "mean_times = [benchmarks[m]['mean_time_ms'] for m in models]\n",
    "fps_values = [benchmarks[m]['fps'] for m in models]\n",
    "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A']\n",
    "\n",
    "# Plot 1: Inference time\n",
    "bars1 = axes[0].bar(models, mean_times, color=colors, alpha=0.8)\n",
    "axes[0].set_ylabel('Inference Time (ms)', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('Average Inference Time (Lower is Better)', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Add value labels\n",
    "for bar, time in zip(bars1, mean_times):\n",
    "    axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, \n",
    "                 f'{time:.1f}ms', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Plot 2: FPS\n",
    "bars2 = axes[1].bar(models, fps_values, color=colors, alpha=0.8)\n",
    "axes[1].set_ylabel('FPS', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title('Frames Per Second (Higher is Better)', fontsize=14, fontweight='bold')\n",
    "axes[1].axhline(y=30, color='red', linestyle='--', label='Real-time (30 FPS)', linewidth=2)\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "axes[1].legend()\n",
    "\n",
    "# Add value labels\n",
    "for bar, fps in zip(bars2, fps_values):\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, \n",
    "                 f'{fps:.1f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print benchmark table\n",
    "print(\"\\nüìä Speed Benchmark Results:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Model':<20} {'Time (ms)':<15} {'FPS':<10} {'Real-time?'}\")\n",
    "print(\"=\"*70)\n",
    "for model in models:\n",
    "    time_ms = benchmarks[model]['mean_time_ms']\n",
    "    fps = benchmarks[model]['fps']\n",
    "    realtime = '‚úÖ Yes' if fps >= 30 else '‚ùå No'\n",
    "    print(f\"{model:<20} {time_ms:<15.1f} {fps:<10.1f} {realtime}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Size Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model size comparison\n",
    "model_sizes = {\n",
    "    'YOLOv8n': {'size_mb': 6, 'params_m': 3.2},\n",
    "    'YOLOv8s': {'size_mb': 22, 'params_m': 11.2},\n",
    "    'YOLOv8m': {'size_mb': 52, 'params_m': 25.9},\n",
    "    'SSD MobileNet V2': {'size_mb': 20, 'params_m': 6.9},\n",
    "}\n",
    "\n",
    "# Create comparison table\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "models = list(model_sizes.keys())\n",
    "sizes = [model_sizes[m]['size_mb'] for m in models]\n",
    "params = [model_sizes[m]['params_m'] for m in models]\n",
    "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A']\n",
    "\n",
    "# Plot 1: File size\n",
    "bars1 = axes[0].bar(models, sizes, color=colors, alpha=0.8)\n",
    "axes[0].set_ylabel('Model Size (MB)', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('Model File Size (Lower is Better)', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "for bar, size in zip(bars1, sizes):\n",
    "    axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, \n",
    "                 f'{size}MB', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Plot 2: Parameters\n",
    "bars2 = axes[1].bar(models, params, color=colors, alpha=0.8)\n",
    "axes[1].set_ylabel('Parameters (Millions)', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title('Model Parameters (Lower is Better)', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "for bar, param in zip(bars2, params):\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, \n",
    "                 f'{param}M', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print table\n",
    "print(\"\\nüì¶ Model Size Comparison:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Model':<20} {'Size (MB)':<15} {'Parameters (M)'}\")\n",
    "print(\"=\"*60)\n",
    "for model in models:\n",
    "    size = model_sizes[model]['size_mb']\n",
    "    params = model_sizes[model]['params_m']\n",
    "    print(f\"{model:<20} {size:<15} {params}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nüí° Insights:\")\n",
    "print(\"  - YOLOv8n: Smallest (6MB), best for mobile/edge devices\")\n",
    "print(\"  - SSD MobileNet V2: Medium (20MB), balanced for mobile\")\n",
    "print(\"  - YOLOv8s: Similar size to SSD (22MB)\")\n",
    "print(\"  - YOLOv8m: Largest (52MB), highest accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detection Quality Analysis\n",
    "\n",
    "Let's analyze how well each model detects objects of different sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulated detection quality metrics\n",
    "# In practice, these would come from validation on COCO dataset\n",
    "\n",
    "quality_metrics = {\n",
    "    'YOLOv8n': {\n",
    "        'small_objects': 0.18,\n",
    "        'medium_objects': 0.40,\n",
    "        'large_objects': 0.54,\n",
    "        'occluded': 0.30,\n",
    "        'overall_map': 0.373\n",
    "    },\n",
    "    'YOLOv8s': {\n",
    "        'small_objects': 0.22,\n",
    "        'medium_objects': 0.47,\n",
    "        'large_objects': 0.61,\n",
    "        'occluded': 0.36,\n",
    "        'overall_map': 0.445\n",
    "    },\n",
    "    'YOLOv8m': {\n",
    "        'small_objects': 0.27,\n",
    "        'medium_objects': 0.53,\n",
    "        'large_objects': 0.67,\n",
    "        'occluded': 0.42,\n",
    "        'overall_map': 0.499\n",
    "    },\n",
    "    'SSD MobileNet V2': {\n",
    "        'small_objects': 0.12,  # SSD struggles with small objects\n",
    "        'medium_objects': 0.38,\n",
    "        'large_objects': 0.58,\n",
    "        'occluded': 0.28,\n",
    "        'overall_map': 0.25  # Lower on COCO (mobile variant)\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create radar chart\n",
    "categories = ['Small\\nObjects', 'Medium\\nObjects', 'Large\\nObjects', 'Occluded\\nObjects', 'Overall\\nmAP']\n",
    "num_vars = len(categories)\n",
    "\n",
    "angles = np.linspace(0, 2 * np.pi, num_vars, endpoint=False).tolist()\n",
    "angles += angles[:1]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))\n",
    "\n",
    "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A']\n",
    "\n",
    "for idx, (model, color) in enumerate(zip(quality_metrics.keys(), colors)):\n",
    "    values = [\n",
    "        quality_metrics[model]['small_objects'],\n",
    "        quality_metrics[model]['medium_objects'],\n",
    "        quality_metrics[model]['large_objects'],\n",
    "        quality_metrics[model]['occluded'],\n",
    "        quality_metrics[model]['overall_map']\n",
    "    ]\n",
    "    values += values[:1]\n",
    "    \n",
    "    ax.plot(angles, values, 'o-', linewidth=2, label=model, color=color)\n",
    "    ax.fill(angles, values, alpha=0.15, color=color)\n",
    "\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels(categories, size=11, fontweight='bold')\n",
    "ax.set_ylim(0, 0.7)\n",
    "ax.set_title('Detection Quality by Object Size', size=16, fontweight='bold', pad=20)\n",
    "ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1), fontsize=11)\n",
    "ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print detailed table\n",
    "print(\"\\nüéØ Detection Quality Metrics (mAP):\")\n",
    "print(\"=\"*85)\n",
    "print(f\"{'Model':<20} {'Small':<12} {'Medium':<12} {'Large':<12} {'Occluded':<12} {'Overall'}\")\n",
    "print(\"=\"*85)\n",
    "for model in quality_metrics.keys():\n",
    "    metrics = quality_metrics[model]\n",
    "    print(f\"{model:<20} {metrics['small_objects']:<12.3f} {metrics['medium_objects']:<12.3f} \"\n",
    "          f\"{metrics['large_objects']:<12.3f} {metrics['occluded']:<12.3f} {metrics['overall_map']:.3f}\")\n",
    "print(\"=\"*85)\n",
    "\n",
    "print(\"\\nüí° Key Observations:\")\n",
    "print(\"  - YOLOv8m: Best overall performance (0.499 mAP)\")\n",
    "print(\"  - SSD struggles with small objects (0.12 vs YOLO's 0.18-0.27)\")\n",
    "print(\"  - All models perform best on large objects\")\n",
    "print(\"  - YOLOv8 variants progressively improve with model size\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive comparison table\n",
    "comparison_data = []\n",
    "\n",
    "for model in ['YOLOv8n', 'YOLOv8s', 'YOLOv8m', 'SSD MobileNet V2']:\n",
    "    comparison_data.append({\n",
    "        'Model': model,\n",
    "        'Size (MB)': model_sizes[model]['size_mb'],\n",
    "        'Parameters (M)': model_sizes[model]['params_m'],\n",
    "        'Inference (ms)': benchmarks[model]['mean_time_ms'],\n",
    "        'FPS': benchmarks[model]['fps'],\n",
    "        'mAP': quality_metrics[model]['overall_map'],\n",
    "        'Small Objects': quality_metrics[model]['small_objects'],\n",
    "        'Large Objects': quality_metrics[model]['large_objects']\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(comparison_data)\n",
    "\n",
    "# Display styled table\n",
    "print(\"\\nüìä COMPREHENSIVE COMPARISON TABLE\")\n",
    "print(\"=\"*110)\n",
    "print(df.to_string(index=False))\n",
    "print(\"=\"*110)\n",
    "\n",
    "# Highlight best performers\n",
    "print(\"\\nüèÜ Best Performers:\")\n",
    "print(f\"  Fastest: {df.loc[df['FPS'].idxmax(), 'Model']} ({df['FPS'].max():.1f} FPS)\")\n",
    "print(f\"  Most Accurate: {df.loc[df['mAP'].idxmax(), 'Model']} ({df['mAP'].max():.3f} mAP)\")\n",
    "print(f\"  Smallest: {df.loc[df['Size (MB)'].idxmin(), 'Model']} ({df['Size (MB)'].min():.0f} MB)\")\n",
    "print(f\"  Best for Small Objects: {df.loc[df['Small Objects'].idxmax(), 'Model']} ({df['Small Objects'].max():.3f} mAP)\")\n",
    "\n",
    "# Create heatmap\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Normalize data for heatmap (higher is better for all except Size and Inference time)\n",
    "heatmap_data = df.copy()\n",
    "heatmap_data['Size (MB)'] = 1 / heatmap_data['Size (MB)']  # Invert: smaller is better\n",
    "heatmap_data['Inference (ms)'] = 1 / heatmap_data['Inference (ms)']  # Invert: faster is better\n",
    "\n",
    "# Normalize to 0-1 scale\n",
    "for col in heatmap_data.columns[1:]:\n",
    "    heatmap_data[col] = (heatmap_data[col] - heatmap_data[col].min()) / (heatmap_data[col].max() - heatmap_data[col].min())\n",
    "\n",
    "sns.heatmap(heatmap_data.set_index('Model').T, annot=True, fmt='.2f', cmap='RdYlGn', \n",
    "            cbar_kws={'label': 'Normalized Score (0-1)'}, ax=ax, linewidths=0.5)\n",
    "ax.set_title('Model Performance Heatmap (Green = Better)', fontsize=14, fontweight='bold', pad=10)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Case Decision Matrix\n",
    "\n",
    "### Choose YOLO if:\n",
    "‚úÖ **Need latest state-of-the-art accuracy** (YOLOv8 actively developed)  \n",
    "‚úÖ **Easy to train on custom data** (Ultralytics library, simple API)  \n",
    "‚úÖ **Active community support** (frequent updates, extensive documentation)  \n",
    "‚úÖ **Deployment flexibility** (PyTorch, ONNX, TFLite, CoreML)  \n",
    "‚úÖ **Better small object detection** (0.18-0.27 mAP vs SSD's 0.12)  \n",
    "‚úÖ **Real-time performance needed** (45+ FPS on YOLOv8n)  \n",
    "\n",
    "### Choose SSD if:\n",
    "‚úÖ **Using TensorFlow ecosystem** (TensorFlow Hub, TF Serving)  \n",
    "‚úÖ **Multi-scale detection critical** (6 feature maps vs YOLO's 3)  \n",
    "‚úÖ **Established in production** (proven, stable, well-tested)  \n",
    "‚úÖ **Legacy system compatibility** (existing SSD infrastructure)  \n",
    "‚úÖ **Mobile deployment with TFLite** (optimized SSDLite variant)  \n",
    "\n",
    "### Model Selection Guidelines:\n",
    "\n",
    "| Priority | Recommended Model | Rationale |\n",
    "|----------|------------------|----------|\n",
    "| **Speed** | YOLOv8n | Fastest (45+ FPS), smallest (6MB) |\n",
    "| **Accuracy** | YOLOv8m | Highest mAP (0.499), best small object detection |\n",
    "| **Balance** | YOLOv8s | Good speed (30+ FPS) + accuracy (0.445 mAP) |\n",
    "| **Mobile** | YOLOv8n or SSD MobileNet | Both ~20MB, optimized for mobile |\n",
    "| **TensorFlow** | SSD MobileNet V2 | Native TF support, TF Hub integration |\n",
    "| **Custom Training** | YOLOv8 (any variant) | Easiest to fine-tune, best documentation |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real-World Application Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real-world use case recommendations\n",
    "use_cases = {\n",
    "    'Autonomous Driving': {\n",
    "        'recommended': 'YOLOv8m',\n",
    "        'rationale': 'Need highest accuracy for safety, real-time performance',\n",
    "        'requirements': 'High accuracy, real-time, small object detection'\n",
    "    },\n",
    "    'Surveillance Camera': {\n",
    "        'recommended': 'YOLOv8s or SSD',\n",
    "        'rationale': 'Balanced speed/accuracy, existing infrastructure',\n",
    "        'requirements': 'Continuous operation, medium accuracy, real-time'\n",
    "    },\n",
    "    'Mobile App (iOS/Android)': {\n",
    "        'recommended': 'YOLOv8n',\n",
    "        'rationale': 'Smallest size (6MB), fastest inference, mobile-optimized',\n",
    "        'requirements': 'Small model size, low power, fast inference'\n",
    "    },\n",
    "    'Industrial Inspection': {\n",
    "        'recommended': 'YOLOv8m',\n",
    "        'rationale': 'Easy fine-tuning on custom defects, high accuracy',\n",
    "        'requirements': 'Custom dataset training, high precision'\n",
    "    },\n",
    "    'Retail Analytics': {\n",
    "        'recommended': 'YOLOv8s',\n",
    "        'rationale': 'Good people/product detection, reasonable speed',\n",
    "        'requirements': 'Person counting, product detection, real-time'\n",
    "    },\n",
    "    'Medical Imaging': {\n",
    "        'recommended': 'YOLOv8m',\n",
    "        'rationale': 'Offline processing, need highest accuracy',\n",
    "        'requirements': 'High accuracy, offline processing acceptable'\n",
    "    },\n",
    "    'Drone Detection': {\n",
    "        'recommended': 'YOLOv8n',\n",
    "        'rationale': 'Edge device deployment, power constraints',\n",
    "        'requirements': 'Small model, low power, edge deployment'\n",
    "    },\n",
    "    'Smart Home Security': {\n",
    "        'recommended': 'SSD MobileNet V2',\n",
    "        'rationale': 'TensorFlow Lite on Raspberry Pi, established',\n",
    "        'requirements': 'Embedded device, TFLite compatibility'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Display use case table\n",
    "print(\"\\nüè≠ REAL-WORLD APPLICATION MAPPING\\n\")\n",
    "print(\"=\"*100)\n",
    "print(f\"{'Use Case':<25} {'Recommended Model':<20} {'Key Requirements'}\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "for use_case, info in use_cases.items():\n",
    "    print(f\"{use_case:<25} {info['recommended']:<20} {info['requirements']}\")\n",
    "    print(f\"{'':25} {'Rationale:':<20} {info['rationale']}\")\n",
    "    print(\"-\"*100)\n",
    "\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Choose Model for Given Scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive exercise\n",
    "scenarios = [\n",
    "    {\n",
    "        'scenario': 'Real-time drone object detection for aerial surveillance',\n",
    "        'constraints': 'Edge device (NVIDIA Jetson), 30 FPS required, <50MB model',\n",
    "        'solution': 'YOLOv8n',\n",
    "        'explanation': 'Smallest (6MB), fastest (45 FPS), edge-optimized'\n",
    "    },\n",
    "    {\n",
    "        'scenario': 'Offline medical image analysis for tumor detection',\n",
    "        'constraints': 'GPU server, offline processing, need highest accuracy',\n",
    "        'solution': 'YOLOv8m or YOLOv8l',\n",
    "        'explanation': 'Speed not critical, need maximum accuracy (0.499+ mAP)'\n",
    "    },\n",
    "    {\n",
    "        'scenario': 'Mobile face detection app for iOS/Android',\n",
    "        'constraints': 'Mobile CPU, <20MB app size increase, battery efficient',\n",
    "        'solution': 'YOLOv8n',\n",
    "        'explanation': 'Smallest (6MB), mobile-optimized, CoreML/TFLite export'\n",
    "    },\n",
    "    {\n",
    "        'scenario': 'TensorFlow-based production pipeline for retail analytics',\n",
    "        'constraints': 'Existing TF infrastructure, TF Serving deployment, real-time',\n",
    "        'solution': 'SSD MobileNet V2',\n",
    "        'explanation': 'Native TensorFlow, TF Hub, established deployment'\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"\\nüéì PRACTICE SCENARIOS\\n\")\n",
    "print(\"Try to solve these before revealing the solution!\\n\")\n",
    "\n",
    "for idx, scenario in enumerate(scenarios, 1):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Scenario {idx}: {scenario['scenario']}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Constraints: {scenario['constraints']}\")\n",
    "    print(f\"\\nüí° Solution: {scenario['solution']}\")\n",
    "    print(f\"üìù Explanation: {scenario['explanation']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Key Takeaways\n",
    "\n",
    "### What We Learned:\n",
    "1. ‚úÖ **Benchmarked YOLO vs SSD**: Speed, accuracy, model size comparison\n",
    "2. ‚úÖ **Detection Quality**: YOLOv8 better for small objects, both good for large\n",
    "3. ‚úÖ **Speed Analysis**: YOLOv8n fastest (45+ FPS), all models real-time capable\n",
    "4. ‚úÖ **Model Size**: YOLOv8n smallest (6MB), SSD moderate (20MB)\n",
    "5. ‚úÖ **Use Case Mapping**: Different applications require different models\n",
    "\n",
    "### Key Decision Factors:\n",
    "\n",
    "| Factor | Choose YOLO | Choose SSD |\n",
    "|--------|-------------|------------|\n",
    "| **Accuracy** | ‚úÖ Higher mAP (0.373-0.499) | ‚ùå Lower mAP (0.25 for mobile) |\n",
    "| **Speed** | ‚úÖ Faster (45+ FPS) | ‚úÖ Fast (25-59 FPS) |\n",
    "| **Small Objects** | ‚úÖ Better (0.18-0.27) | ‚ùå Weaker (0.12) |\n",
    "| **Training** | ‚úÖ Very easy (Ultralytics) | ‚ö†Ô∏è More complex |\n",
    "| **TensorFlow** | ‚ö†Ô∏è Export required | ‚úÖ Native support |\n",
    "| **Mobile** | ‚úÖ Excellent (6MB) | ‚úÖ Good (20MB) |\n",
    "| **Community** | ‚úÖ Very active | ‚ö†Ô∏è Moderate |\n",
    "\n",
    "### Final Recommendations:\n",
    "\n",
    "**General Purpose:** YOLOv8s (balanced speed + accuracy)  \n",
    "**Mobile/Edge:** YOLOv8n (smallest, fastest)  \n",
    "**Highest Accuracy:** YOLOv8m or YOLOv8l  \n",
    "**TensorFlow Ecosystem:** SSD MobileNet V2  \n",
    "**Autonomous Systems:** YOLOv8m (safety-critical)  \n",
    "**Surveillance:** YOLOv8s or SSD (proven reliability)  \n",
    "\n",
    "### Performance Summary:\n",
    "```\n",
    "YOLOv8n: 6MB, 45 FPS, 37.3% mAP  ‚Üí Best for mobile/edge\n",
    "YOLOv8s: 22MB, 35 FPS, 44.5% mAP ‚Üí Best balanced choice\n",
    "YOLOv8m: 52MB, 25 FPS, 49.9% mAP ‚Üí Best accuracy\n",
    "SSD:     20MB, 25 FPS, 25% mAP   ‚Üí Best for TensorFlow\n",
    "```\n",
    "\n",
    "### Week 14 Complete!\n",
    "\n",
    "**Covered Topics:**\n",
    "- ‚úÖ YOLO architecture and variants\n",
    "- ‚úÖ YOLOv8 training on custom datasets\n",
    "- ‚úÖ SSD architecture and multi-scale detection\n",
    "- ‚úÖ YOLO vs SSD comprehensive comparison\n",
    "- ‚úÖ Model selection decision framework\n",
    "\n",
    "**Next Week (Week 15):**\n",
    "- R-CNN family (R-CNN, Fast R-CNN, Faster R-CNN)\n",
    "- Two-stage detectors vs single-stage\n",
    "- Region Proposal Networks (RPN)\n",
    "- Mask R-CNN for instance segmentation\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations! You can now make informed decisions about object detection models! üéâ**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
