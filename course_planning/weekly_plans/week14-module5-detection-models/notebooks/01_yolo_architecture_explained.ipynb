{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLO Architecture Explained\n",
    "\n",
    "**Week 14 - Module 5: Object Detection Models**\n",
    "\n",
    "**Estimated Time:** 15 minutes\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand YOLO's \"You Only Look Once\" paradigm\n",
    "- Learn grid-based detection mechanism\n",
    "- Understand anchor boxes concept\n",
    "- Compare YOLO versions (v1 ‚Üí v8)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What is YOLO?\n",
    "\n",
    "**YOLO (You Only Look Once)** is a revolutionary object detection algorithm that changed how we approach real-time detection.\n",
    "\n",
    "### Key Innovation\n",
    "Unlike previous methods (R-CNN, Fast R-CNN) that:\n",
    "1. Propose regions\n",
    "2. Classify each region separately\n",
    "3. Refine bounding boxes\n",
    "\n",
    "YOLO does everything in **one forward pass** through the network!\n",
    "\n",
    "### History\n",
    "- **YOLOv1 (2015)**: Joseph Redmon et al. - First real-time detector\n",
    "- **YOLOv2/YOLO9000 (2016)**: Added batch normalization, anchor boxes\n",
    "- **YOLOv3 (2018)**: Multi-scale predictions, better small object detection\n",
    "- **YOLOv4 (2020)**: CSPDarknet53 backbone, improved accuracy\n",
    "- **YOLOv5 (2020)**: PyTorch implementation, user-friendly\n",
    "- **YOLOv8 (2023)**: Ultralytics, state-of-the-art performance\n",
    "\n",
    "### Why \"You Only Look Once\"?\n",
    "The network looks at the entire image once and predicts all bounding boxes and class probabilities simultaneously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The Core Idea: Grid-Based Detection\n",
    "\n",
    "### How YOLO Works\n",
    "\n",
    "```\n",
    "INPUT IMAGE (e.g., 416√ó416)\n",
    "         |\n",
    "         v\n",
    "    CNN BACKBONE\n",
    "         |\n",
    "         v\n",
    "DIVIDE INTO S√óS GRID (e.g., 13√ó13)\n",
    "         |\n",
    "         v\n",
    "EACH CELL PREDICTS B BOUNDING BOXES\n",
    "         |\n",
    "         v\n",
    "OUTPUT: Grid of predictions\n",
    "```\n",
    "\n",
    "### Grid Example (7√ó7 grid)\n",
    "```\n",
    "+---+---+---+---+---+---+---+\n",
    "|   |   |   |   |   |   |   |\n",
    "+---+---+---+---+---+---+---+\n",
    "|   |   | DOG  |   |   |   |\n",
    "|   |   |[‚Ä¢]  |   |   |   |  <- Center of dog in this cell\n",
    "+---+---+---+---+---+---+---+\n",
    "|   |   |   |   |   |   |   |\n",
    "+---+---+---+---+---+---+---+\n",
    "|   |   |   | CAR |   |   |\n",
    "|   |   |   |[‚Ä¢]|   |   |   | <- Center of car in this cell\n",
    "+---+---+---+---+---+---+---+\n",
    "```\n",
    "\n",
    "### Each Grid Cell Predicts\n",
    "For each of B bounding boxes:\n",
    "- **x, y**: Center coordinates (relative to cell)\n",
    "- **w, h**: Width and height (relative to image)\n",
    "- **confidence**: Objectness score (0-1)\n",
    "- **class probabilities**: C values for C classes\n",
    "\n",
    "**Total predictions per cell**: B √ó (5 + C)\n",
    "- 5 = x, y, w, h, confidence\n",
    "- C = number of classes (e.g., 80 for COCO dataset)\n",
    "\n",
    "### Example Calculation\n",
    "- Grid: 13√ó13\n",
    "- Boxes per cell: 3\n",
    "- Classes: 80\n",
    "- **Total predictions**: 13 √ó 13 √ó 3 √ó (5 + 80) = 43,095 predictions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Anchor Boxes\n",
    "\n",
    "### The Problem\n",
    "Different objects have different shapes:\n",
    "- Person: tall and narrow (aspect ratio ~1:3)\n",
    "- Car: wide and flat (aspect ratio ~3:1)\n",
    "- Ball: square (aspect ratio ~1:1)\n",
    "\n",
    "### The Solution: Predefined Anchor Boxes\n",
    "Instead of predicting boxes from scratch, YOLO:\n",
    "1. Uses predefined anchor boxes at different scales and aspect ratios\n",
    "2. Predicts **offsets** from these anchors\n",
    "\n",
    "### Example Anchors (3 per scale)\n",
    "```\n",
    "Small scale:  [10√ó13], [16√ó30], [33√ó23]\n",
    "Medium scale: [30√ó61], [62√ó45], [59√ó119]\n",
    "Large scale:  [116√ó90], [156√ó198], [373√ó326]\n",
    "```\n",
    "\n",
    "### Why Anchors?\n",
    "- Faster convergence during training\n",
    "- Better handling of different object shapes\n",
    "- Multiple detections per grid cell\n",
    "\n",
    "### Anchor Box Visualization\n",
    "```\n",
    "Tall Person     Wide Car       Square Ball\n",
    "  +--+           +------+         +---+\n",
    "  |  |           |      |         |   |\n",
    "  |  |           +------+         +---+\n",
    "  |  |\n",
    "  +--+\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize anchor boxes concept\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "\n",
    "# Create figure\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Define anchor boxes (width, height) - normalized\n",
    "anchors = [\n",
    "    [(0.1, 0.3), (0.2, 0.4), (0.15, 0.5)],  # Tall anchors\n",
    "    [(0.3, 0.2), (0.4, 0.25), (0.5, 0.3)],  # Wide anchors\n",
    "    [(0.2, 0.2), (0.3, 0.3), (0.4, 0.4)]    # Square anchors\n",
    "]\n",
    "\n",
    "titles = ['Tall Anchors\\n(for people, bottles)', \n",
    "          'Wide Anchors\\n(for cars, buses)',\n",
    "          'Square Anchors\\n(for balls, signs)']\n",
    "colors = ['red', 'green', 'blue']\n",
    "\n",
    "for idx, (ax, anchor_set, title) in enumerate(zip(axes, anchors, titles)):\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_title(title, fontsize=12, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Draw each anchor box centered at (0.5, 0.5)\n",
    "    for i, (w, h) in enumerate(anchor_set):\n",
    "        x = 0.5 - w/2\n",
    "        y = 0.5 - h/2\n",
    "        rect = patches.Rectangle((x, y), w, h, \n",
    "                                linewidth=2, \n",
    "                                edgecolor=colors[i], \n",
    "                                facecolor='none',\n",
    "                                label=f'Anchor {i+1}: {w:.2f}√ó{h:.2f}')\n",
    "        ax.add_patch(rect)\n",
    "    \n",
    "    ax.legend(loc='upper right', fontsize=8)\n",
    "    ax.set_xlabel('Width', fontsize=10)\n",
    "    ax.set_ylabel('Height', fontsize=10)\n",
    "\n",
    "plt.suptitle('YOLO Anchor Boxes at Different Aspect Ratios', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìå Key Points:\")\n",
    "print(\"‚Ä¢ Each grid cell has multiple anchor boxes\")\n",
    "print(\"‚Ä¢ Model predicts offsets from these anchors\")\n",
    "print(\"‚Ä¢ Different shapes help detect various object types\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. YOLO Architecture Visual\n",
    "\n",
    "### YOLOv8 Architecture Overview\n",
    "\n",
    "```\n",
    "INPUT IMAGE (640√ó640√ó3)\n",
    "        |\n",
    "        v\n",
    "+------------------+\n",
    "|   BACKBONE       |  <- Feature extraction (CSPDarknet)\n",
    "|   (CNN Layers)   |\n",
    "+------------------+\n",
    "        |\n",
    "        v\n",
    "+------------------+\n",
    "|   NECK           |  <- Feature fusion (PANet)\n",
    "|   (FPN + PAN)    |\n",
    "+------------------+\n",
    "        |\n",
    "        v\n",
    "+------------------+\n",
    "|   HEAD           |  <- Detection layers\n",
    "|   (Predictions)  |\n",
    "+------------------+\n",
    "        |\n",
    "        v\n",
    "    OUTPUT:\n",
    "    - 80√ó80 grid (small objects)\n",
    "    - 40√ó40 grid (medium objects)\n",
    "    - 20√ó20 grid (large objects)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create YOLO architecture flow diagram\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "ax.set_xlim(0, 10)\n",
    "ax.set_ylim(0, 10)\n",
    "ax.axis('off')\n",
    "\n",
    "# Define components\n",
    "components = [\n",
    "    {'name': 'Input Image\\n640√ó640√ó3', 'pos': (5, 9), 'color': 'lightblue'},\n",
    "    {'name': 'Backbone\\n(CSPDarknet)', 'pos': (5, 7.5), 'color': 'lightcoral'},\n",
    "    {'name': 'Neck\\n(PANet)', 'pos': (5, 6), 'color': 'lightgreen'},\n",
    "    {'name': 'Detection Head', 'pos': (5, 4.5), 'color': 'lightyellow'},\n",
    "    {'name': 'Small Objects\\n80√ó80 grid', 'pos': (2, 2.5), 'color': 'pink'},\n",
    "    {'name': 'Medium Objects\\n40√ó40 grid', 'pos': (5, 2.5), 'color': 'pink'},\n",
    "    {'name': 'Large Objects\\n20√ó20 grid', 'pos': (8, 2.5), 'color': 'pink'},\n",
    "]\n",
    "\n",
    "# Draw boxes\n",
    "for comp in components:\n",
    "    bbox = mpatches.FancyBboxPatch(\n",
    "        (comp['pos'][0] - 1, comp['pos'][1] - 0.4),\n",
    "        2, 0.8,\n",
    "        boxstyle=\"round,pad=0.1\",\n",
    "        edgecolor='black',\n",
    "        facecolor=comp['color'],\n",
    "        linewidth=2\n",
    "    )\n",
    "    ax.add_patch(bbox)\n",
    "    ax.text(comp['pos'][0], comp['pos'][1], comp['name'],\n",
    "            ha='center', va='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "# Draw arrows\n",
    "arrows = [\n",
    "    ((5, 8.6), (5, 7.9)),  # Input to Backbone\n",
    "    ((5, 7.1), (5, 6.4)),  # Backbone to Neck\n",
    "    ((5, 5.6), (5, 4.9)),  # Neck to Head\n",
    "    ((4.5, 4.1), (2.5, 2.9)),  # Head to Small\n",
    "    ((5, 4.1), (5, 2.9)),  # Head to Medium\n",
    "    ((5.5, 4.1), (7.5, 2.9)),  # Head to Large\n",
    "]\n",
    "\n",
    "for start, end in arrows:\n",
    "    ax.annotate('', xy=end, xytext=start,\n",
    "                arrowprops=dict(arrowstyle='->', lw=2, color='black'))\n",
    "\n",
    "# Add title and notes\n",
    "ax.text(5, 9.8, 'YOLOv8 Architecture Flow', \n",
    "        ha='center', fontsize=16, fontweight='bold')\n",
    "ax.text(5, 0.8, 'Multi-scale predictions enable detection of objects at different sizes',\n",
    "        ha='center', fontsize=10, style='italic')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüîç Architecture Components:\")\n",
    "print(\"1. Backbone: Extracts features from input image\")\n",
    "print(\"2. Neck: Fuses features at different scales\")\n",
    "print(\"3. Head: Makes predictions at multiple scales\")\n",
    "print(\"4. Output: Three detection layers for small, medium, and large objects\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. YOLO Evolution: v1 vs v3 vs v8\n",
    "\n",
    "| Feature | YOLOv1 (2015) | YOLOv3 (2018) | YOLOv8 (2023) |\n",
    "|---------|---------------|---------------|---------------|\n",
    "| **Backbone** | Custom CNN (24 layers) | Darknet-53 | CSPDarknet + C2f |\n",
    "| **Detection Scales** | 1 (7√ó7 grid) | 3 (13√ó13, 26√ó26, 52√ó52) | 3 (20√ó20, 40√ó40, 80√ó80) |\n",
    "| **Anchor Boxes** | No | Yes (9 anchors) | Anchor-free (evolved) |\n",
    "| **Batch Normalization** | No | Yes | Yes + advanced |\n",
    "| **Activation** | Leaky ReLU | Leaky ReLU | SiLU (Swish) |\n",
    "| **Loss Function** | Sum-squared error | Binary cross-entropy | CIoU + BCE |\n",
    "| **Small Object Detection** | Poor | Good | Excellent |\n",
    "| **Speed (FPS)** | 45 | 30-60 | 60-80+ |\n",
    "| **mAP (COCO)** | ~63% | ~60% | ~53% (YOLOv8n) to 53.9% (YOLOv8x) |\n",
    "| **Parameters** | 50M | 62M | 3M (nano) to 68M (xlarge) |\n",
    "\n",
    "### Key Improvements Over Versions\n",
    "\n",
    "**YOLOv1 ‚Üí YOLOv3:**\n",
    "- ‚úÖ Multi-scale predictions (better small objects)\n",
    "- ‚úÖ Anchor boxes (better shape prediction)\n",
    "- ‚úÖ Feature Pyramid Network (FPN)\n",
    "- ‚úÖ Better classification (logistic regression instead of softmax)\n",
    "\n",
    "**YOLOv3 ‚Üí YOLOv8:**\n",
    "- ‚úÖ Anchor-free detection (simpler, faster)\n",
    "- ‚úÖ Advanced data augmentation (Mosaic, MixUp)\n",
    "- ‚úÖ Better feature fusion (C2f modules)\n",
    "- ‚úÖ Decoupled head (separate classification and localization)\n",
    "- ‚úÖ Model scaling (nano to xlarge variants)\n",
    "- ‚úÖ Easier to train and deploy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Multi-Scale Predictions\n",
    "\n",
    "### Why Multiple Scales?\n",
    "\n",
    "Objects in real-world images vary greatly in size:\n",
    "- **Small objects**: People in distance, small animals (need fine grid)\n",
    "- **Medium objects**: Cars, furniture (need medium grid)\n",
    "- **Large objects**: Buildings, trucks (need coarse grid)\n",
    "\n",
    "### How YOLO Handles This\n",
    "\n",
    "YOLOv8 predicts at 3 different scales:\n",
    "\n",
    "```\n",
    "Scale 1: 80√ó80 grid ‚Üí 6,400 cells ‚Üí Small objects\n",
    "         (Each cell covers 8√ó8 pixels in 640√ó640 image)\n",
    "         \n",
    "Scale 2: 40√ó40 grid ‚Üí 1,600 cells ‚Üí Medium objects\n",
    "         (Each cell covers 16√ó16 pixels)\n",
    "         \n",
    "Scale 3: 20√ó20 grid ‚Üí 400 cells ‚Üí Large objects\n",
    "         (Each cell covers 32√ó32 pixels)\n",
    "```\n",
    "\n",
    "### Example: Detecting a Scene\n",
    "\n",
    "```\n",
    "Image: Street scene with people, cars, and buildings\n",
    "\n",
    "80√ó80 grid detects:  üë§ Person (20√ó50 pixels)\n",
    "                     üêï Dog (30√ó30 pixels)\n",
    "                     \n",
    "40√ó40 grid detects:  üöó Car (80√ó120 pixels)\n",
    "                     üö¥ Bicycle (60√ó80 pixels)\n",
    "                     \n",
    "20√ó20 grid detects:  üè¢ Building (200√ó300 pixels)\n",
    "                     üöå Bus (150√ó200 pixels)\n",
    "```\n",
    "\n",
    "### Feature Pyramid Network (FPN)\n",
    "\n",
    "YOLO uses FPN to combine:\n",
    "- **Deep features**: High-level semantic information (what is it?)\n",
    "- **Shallow features**: Fine-grained details (where exactly is it?)\n",
    "\n",
    "This fusion happens in the **Neck** of the architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Loss Function Components\n",
    "\n",
    "YOLO's loss function has three main components:\n",
    "\n",
    "### 1. Localization Loss (Box Coordinates)\n",
    "Measures how well predicted boxes match ground truth boxes.\n",
    "\n",
    "**YOLOv8 uses CIoU (Complete Intersection over Union):**\n",
    "\n",
    "$$L_{loc} = 1 - \\text{CIoU}(B_{pred}, B_{gt})$$\n",
    "\n",
    "CIoU considers:\n",
    "- Overlap area\n",
    "- Distance between centers\n",
    "- Aspect ratio difference\n",
    "\n",
    "### 2. Objectness Loss (Confidence)\n",
    "Measures whether a bounding box contains an object.\n",
    "\n",
    "$$L_{obj} = -\\sum_{i=0}^{S^2} \\sum_{j=0}^{B} \\mathbb{1}_{ij}^{obj} \\log(C_{ij}) - \\sum_{i=0}^{S^2} \\sum_{j=0}^{B} \\mathbb{1}_{ij}^{noobj} \\log(1-C_{ij})$$\n",
    "\n",
    "Where:\n",
    "- $C_{ij}$ = predicted confidence for box j in cell i\n",
    "- $\\mathbb{1}_{ij}^{obj}$ = 1 if object present, 0 otherwise\n",
    "\n",
    "### 3. Classification Loss (Class Probabilities)\n",
    "Measures how well the model predicts the correct class.\n",
    "\n",
    "**Binary Cross-Entropy (BCE):**\n",
    "\n",
    "$$L_{cls} = -\\sum_{i=0}^{S^2} \\mathbb{1}_i^{obj} \\sum_{c \\in classes} [p_i(c) \\log(\\hat{p}_i(c)) + (1-p_i(c)) \\log(1-\\hat{p}_i(c))]$$\n",
    "\n",
    "### Total Loss\n",
    "\n",
    "$$L_{total} = \\lambda_{box} L_{loc} + \\lambda_{obj} L_{obj} + \\lambda_{cls} L_{cls}$$\n",
    "\n",
    "Where Œª values are hyperparameters that balance the three components.\n",
    "\n",
    "### Why Three Components?\n",
    "1. **Localization**: Get boxes in the right place\n",
    "2. **Objectness**: Know when there's something to detect\n",
    "3. **Classification**: Identify what that something is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize IoU vs CIoU\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "def calculate_iou(box1, box2):\n",
    "    \"\"\"Calculate IoU between two boxes [x, y, w, h]\"\"\"\n",
    "    x1, y1, w1, h1 = box1\n",
    "    x2, y2, w2, h2 = box2\n",
    "    \n",
    "    # Calculate intersection\n",
    "    x_left = max(x1, x2)\n",
    "    y_top = max(y1, y2)\n",
    "    x_right = min(x1 + w1, x2 + w2)\n",
    "    y_bottom = min(y1 + h1, y2 + h2)\n",
    "    \n",
    "    if x_right < x_left or y_bottom < y_top:\n",
    "        return 0.0\n",
    "    \n",
    "    intersection = (x_right - x_left) * (y_bottom - y_top)\n",
    "    area1 = w1 * h1\n",
    "    area2 = w2 * h2\n",
    "    union = area1 + area2 - intersection\n",
    "    \n",
    "    return intersection / union if union > 0 else 0\n",
    "\n",
    "# Create example scenarios\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "scenarios = [\n",
    "    {'gt': [1, 1, 2, 2], 'pred': [1.5, 1.5, 2, 2], 'title': 'Good Overlap\\nIoU ‚âà 0.5'},\n",
    "    {'gt': [1, 1, 2, 2], 'pred': [2.5, 1, 2, 2], 'title': 'Partial Overlap\\nIoU ‚âà 0.2'},\n",
    "    {'gt': [1, 1, 2, 2], 'pred': [3.5, 1, 2, 2], 'title': 'No Overlap\\nIoU = 0.0'}\n",
    "]\n",
    "\n",
    "for ax, scenario in zip(axes, scenarios):\n",
    "    ax.set_xlim(0, 6)\n",
    "    ax.set_ylim(0, 4)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Draw ground truth box (green)\n",
    "    gt_box = patches.Rectangle(\n",
    "        (scenario['gt'][0], scenario['gt'][1]),\n",
    "        scenario['gt'][2], scenario['gt'][3],\n",
    "        linewidth=3, edgecolor='green', facecolor='green', alpha=0.3,\n",
    "        label='Ground Truth'\n",
    "    )\n",
    "    ax.add_patch(gt_box)\n",
    "    \n",
    "    # Draw predicted box (red)\n",
    "    pred_box = patches.Rectangle(\n",
    "        (scenario['pred'][0], scenario['pred'][1]),\n",
    "        scenario['pred'][2], scenario['pred'][3],\n",
    "        linewidth=3, edgecolor='red', facecolor='red', alpha=0.3,\n",
    "        label='Prediction'\n",
    "    )\n",
    "    ax.add_patch(pred_box)\n",
    "    \n",
    "    iou = calculate_iou(scenario['gt'], scenario['pred'])\n",
    "    ax.set_title(f\"{scenario['title']}\\nCalculated IoU: {iou:.2f}\", fontsize=11, fontweight='bold')\n",
    "    ax.legend(loc='upper right')\n",
    "\n",
    "plt.suptitle('Intersection over Union (IoU) Examples', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Loss Function Summary:\")\n",
    "print(\"‚Ä¢ Localization Loss: Penalizes poorly positioned boxes\")\n",
    "print(\"‚Ä¢ Objectness Loss: Penalizes false positives and negatives\")\n",
    "print(\"‚Ä¢ Classification Loss: Penalizes wrong class predictions\")\n",
    "print(\"\\nüí° Higher IoU = Better localization = Lower loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Understanding Grid Predictions (Hands-On)\n",
    "\n",
    "Let's see how YOLO actually makes predictions on a real image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install ultralytics if needed\n",
    "try:\n",
    "    from ultralytics import YOLO\n",
    "except ImportError:\n",
    "    print(\"Installing ultralytics...\")\n",
    "    !pip install -q ultralytics\n",
    "    from ultralytics import YOLO\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load YOLOv8 nano model\n",
    "model = YOLO('yolov8n.pt')\n",
    "\n",
    "# Download a sample image\n",
    "import urllib.request\n",
    "url = 'https://ultralytics.com/images/bus.jpg'\n",
    "urllib.request.urlretrieve(url, 'bus.jpg')\n",
    "\n",
    "# Load image\n",
    "image = cv2.imread('bus.jpg')\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Run detection\n",
    "results = model(image, verbose=False)\n",
    "\n",
    "# Visualize results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Original image\n",
    "axes[0].imshow(image)\n",
    "axes[0].set_title('Original Image', fontsize=14, fontweight='bold')\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Predictions\n",
    "result_img = results[0].plot()\n",
    "axes[1].imshow(result_img)\n",
    "axes[1].set_title('YOLO Predictions', fontsize=14, fontweight='bold')\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print detection details\n",
    "print(\"\\nüéØ Detections:\")\n",
    "print(\"-\" * 60)\n",
    "for box in results[0].boxes:\n",
    "    cls_id = int(box.cls[0])\n",
    "    conf = float(box.conf[0])\n",
    "    bbox = box.xyxy[0].cpu().numpy()\n",
    "    class_name = model.names[cls_id]\n",
    "    print(f\"Class: {class_name:15s} | Confidence: {conf:.3f} | Box: [{bbox[0]:.0f}, {bbox[1]:.0f}, {bbox[2]:.0f}, {bbox[3]:.0f}]\")\n",
    "\n",
    "print(f\"\\nüìä Total detections: {len(results[0].boxes)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Non-Maximum Suppression (NMS)\n",
    "\n",
    "### The Problem: Duplicate Detections\n",
    "\n",
    "YOLO predicts thousands of boxes. Many boxes detect the same object!\n",
    "\n",
    "```\n",
    "Example: Detecting a car\n",
    "\n",
    "Box 1: Confidence 0.9, IoU with Box 2 = 0.8\n",
    "Box 2: Confidence 0.85, IoU with Box 1 = 0.8\n",
    "Box 3: Confidence 0.7, IoU with Box 1 = 0.6\n",
    "\n",
    "All three boxes detect the SAME car!\n",
    "```\n",
    "\n",
    "### The Solution: Non-Maximum Suppression\n",
    "\n",
    "**Algorithm:**\n",
    "1. Sort all boxes by confidence (highest first)\n",
    "2. Take the box with highest confidence\n",
    "3. Remove all boxes with IoU > threshold (e.g., 0.5) with this box\n",
    "4. Repeat until no boxes left\n",
    "\n",
    "### NMS in Action\n",
    "\n",
    "```\n",
    "Before NMS:          After NMS:\n",
    "   [0.9]               [0.9]  ‚Üê Kept (highest confidence)\n",
    "   [0.85]              \n",
    "   [0.7]               \n",
    "   \n",
    "Result: 3 boxes ‚Üí 1 box\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple NMS implementation\n",
    "def non_max_suppression(boxes, confidences, iou_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Perform Non-Maximum Suppression\n",
    "    \n",
    "    Args:\n",
    "        boxes: List of bounding boxes [x, y, w, h]\n",
    "        confidences: List of confidence scores\n",
    "        iou_threshold: IoU threshold for suppression\n",
    "    \n",
    "    Returns:\n",
    "        Indices of boxes to keep\n",
    "    \"\"\"\n",
    "    if len(boxes) == 0:\n",
    "        return []\n",
    "    \n",
    "    # Sort by confidence (descending)\n",
    "    indices = np.argsort(confidences)[::-1]\n",
    "    keep = []\n",
    "    \n",
    "    while len(indices) > 0:\n",
    "        # Keep the box with highest confidence\n",
    "        current = indices[0]\n",
    "        keep.append(current)\n",
    "        \n",
    "        # Calculate IoU with remaining boxes\n",
    "        remaining_indices = []\n",
    "        for idx in indices[1:]:\n",
    "            iou = calculate_iou(boxes[current], boxes[idx])\n",
    "            if iou < iou_threshold:\n",
    "                remaining_indices.append(idx)\n",
    "        \n",
    "        indices = remaining_indices\n",
    "    \n",
    "    return keep\n",
    "\n",
    "# Example: Multiple overlapping boxes\n",
    "boxes = [\n",
    "    [100, 100, 200, 200],  # Box 1\n",
    "    [110, 110, 200, 200],  # Box 2 - similar to Box 1\n",
    "    [120, 105, 200, 200],  # Box 3 - similar to Box 1\n",
    "    [400, 400, 150, 150],  # Box 4 - different location\n",
    "]\n",
    "confidences = [0.9, 0.85, 0.75, 0.8]\n",
    "\n",
    "# Apply NMS\n",
    "keep_indices = non_max_suppression(boxes, confidences, iou_threshold=0.5)\n",
    "\n",
    "print(\"\\nüîç NMS Results:\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"Before NMS: {len(boxes)} boxes\")\n",
    "print(f\"After NMS:  {len(keep_indices)} boxes\")\n",
    "print(f\"\\nKept boxes (indices): {keep_indices}\")\n",
    "print(f\"\\nKept boxes details:\")\n",
    "for idx in keep_indices:\n",
    "    print(f\"  Box {idx}: confidence={confidences[idx]:.2f}, bbox={boxes[idx]}\")\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "for ax, title, show_all in [(axes[0], 'Before NMS', True), (axes[1], 'After NMS', False)]:\n",
    "    ax.set_xlim(0, 600)\n",
    "    ax.set_ylim(0, 600)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.invert_yaxis()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_title(title, fontsize=14, fontweight='bold')\n",
    "    \n",
    "    indices_to_show = range(len(boxes)) if show_all else keep_indices\n",
    "    colors = ['red', 'blue', 'green', 'purple']\n",
    "    \n",
    "    for i in indices_to_show:\n",
    "        x, y, w, h = boxes[i]\n",
    "        rect = patches.Rectangle(\n",
    "            (x, y), w, h,\n",
    "            linewidth=2,\n",
    "            edgecolor=colors[i],\n",
    "            facecolor='none',\n",
    "            label=f'Box {i} ({confidences[i]:.2f})'\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(x + w/2, y - 10, f'{confidences[i]:.2f}', \n",
    "                ha='center', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° NMS removes overlapping boxes, keeping only the most confident prediction!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. YOLO Strengths & Weaknesses\n",
    "\n",
    "### ‚úÖ Strengths\n",
    "\n",
    "1. **Speed**: Real-time detection (30-80+ FPS)\n",
    "   - Single forward pass through network\n",
    "   - No region proposal step\n",
    "\n",
    "2. **Global Context**: Sees entire image\n",
    "   - Better at understanding relationships between objects\n",
    "   - Fewer background false positives\n",
    "\n",
    "3. **Generalizes Well**: Works on new domains\n",
    "   - Trained on diverse COCO dataset\n",
    "   - Transfer learning capabilities\n",
    "\n",
    "4. **End-to-End Training**: Optimizes entire pipeline\n",
    "   - Joint optimization of detection and classification\n",
    "\n",
    "5. **Easy to Deploy**: Simple architecture\n",
    "   - Available in multiple sizes (nano to xlarge)\n",
    "   - Good mobile/edge support\n",
    "\n",
    "### ‚ùå Weaknesses\n",
    "\n",
    "1. **Small Object Detection**: Can struggle with tiny objects\n",
    "   - Limited by grid resolution\n",
    "   - Multiple small objects in same grid cell\n",
    "\n",
    "2. **Unusual Aspect Ratios**: May miss oddly-shaped objects\n",
    "   - Anchor boxes designed for common shapes\n",
    "\n",
    "3. **Precise Localization**: Sometimes less accurate than two-stage detectors\n",
    "   - Trade-off for speed\n",
    "\n",
    "4. **Crowded Scenes**: Challenges with overlapping objects\n",
    "   - Each grid cell has limited predictions\n",
    "\n",
    "5. **New Object Shapes**: Needs retraining for very different domains\n",
    "   - Anchor boxes optimized for COCO-like datasets\n",
    "\n",
    "### When to Use YOLO?\n",
    "\n",
    "**Good for:**\n",
    "- Real-time applications (video surveillance, autonomous driving)\n",
    "- General object detection (common objects)\n",
    "- Resource-constrained environments (mobile, edge devices)\n",
    "\n",
    "**Consider alternatives for:**\n",
    "- Very high accuracy requirements (use Mask R-CNN, Cascade R-CNN)\n",
    "- Dense small object detection (use specialized architectures)\n",
    "- Instance segmentation (use Mask R-CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Exercise: Conceptual Questions\n",
    "\n",
    "Test your understanding:\n",
    "\n",
    "### Question 1\n",
    "If YOLO uses a 13√ó13 grid and predicts 3 boxes per cell with 80 classes:\n",
    "- How many total predictions does it make?\n",
    "- How many values per prediction?\n",
    "\n",
    "### Question 2\n",
    "Explain why YOLO is called \"You Only Look Once\". How is this different from R-CNN?\n",
    "\n",
    "### Question 3\n",
    "What is the purpose of anchor boxes? Why can't YOLO just predict boxes directly?\n",
    "\n",
    "### Question 4\n",
    "Why does YOLO use multiple detection scales (e.g., 20√ó20, 40√ó40, 80√ó80)?\n",
    "\n",
    "### Question 5\n",
    "What would happen if we didn't use Non-Maximum Suppression?\n",
    "\n",
    "### Question 6\n",
    "Compare YOLOv1 and YOLOv8. Name at least 3 major improvements.\n",
    "\n",
    "---\n",
    "\n",
    "**Answers:**\n",
    "\n",
    "1. **Total predictions**: 13 √ó 13 √ó 3 = 507 boxes. **Values per prediction**: 5 (x, y, w, h, confidence) + 80 (class probabilities) = 85 values.\n",
    "\n",
    "2. **\"You Only Look Once\"**: YOLO makes all predictions in a single forward pass through the network, unlike R-CNN which:\n",
    "   - Proposes ~2000 regions (selective search)\n",
    "   - Runs CNN on each region separately\n",
    "   - Classifies each region\n",
    "   YOLO is much faster because it processes the image only once.\n",
    "\n",
    "3. **Anchor boxes**: Help the model learn different object shapes by providing starting templates. Direct prediction is harder because:\n",
    "   - Network would need to learn all possible shapes from scratch\n",
    "   - Anchors provide priors (tall for people, wide for cars)\n",
    "   - Faster convergence during training\n",
    "\n",
    "4. **Multiple scales**: Different scales detect different object sizes:\n",
    "   - Fine grid (80√ó80): Small objects (people far away)\n",
    "   - Medium grid (40√ó40): Medium objects (cars)\n",
    "   - Coarse grid (20√ó20): Large objects (buses, buildings)\n",
    "\n",
    "5. **Without NMS**: We'd get multiple overlapping boxes for the same object, making results unusable. The output would be cluttered with duplicate detections.\n",
    "\n",
    "6. **YOLOv1 ‚Üí YOLOv8 improvements**:\n",
    "   - Multi-scale predictions (1 scale ‚Üí 3 scales)\n",
    "   - Anchor-free detection (simpler, faster)\n",
    "   - Better backbone (Custom CNN ‚Üí CSPDarknet)\n",
    "   - Advanced data augmentation\n",
    "   - Decoupled head (separate classification and localization)\n",
    "   - Model scaling options (nano to xlarge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Summary & Next Steps\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "‚úÖ **YOLO's core paradigm**: Single-pass object detection\n",
    "\n",
    "‚úÖ **Grid-based detection**: Divide image into grid, predict from each cell\n",
    "\n",
    "‚úÖ **Anchor boxes**: Predefined templates for different object shapes\n",
    "\n",
    "‚úÖ **Multi-scale predictions**: Detect small, medium, and large objects\n",
    "\n",
    "‚úÖ **Loss function**: Localization + Objectness + Classification\n",
    "\n",
    "‚úÖ **NMS**: Remove duplicate detections\n",
    "\n",
    "‚úÖ **Evolution**: v1 ‚Üí v3 ‚Üí v8 improvements\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Speed vs Accuracy**: YOLO trades some accuracy for real-time performance\n",
    "2. **One-stage detector**: Unlike R-CNN family (two-stage)\n",
    "3. **End-to-end trainable**: All components optimized together\n",
    "4. **Practical**: Easy to use, deploy, and scale\n",
    "\n",
    "### Preview: Notebook 02 - YOLOv8 Pretrained Detection\n",
    "\n",
    "In the next notebook, we'll:\n",
    "- Install and use YOLOv8 with Ultralytics\n",
    "- Detect objects in images and videos\n",
    "- Tune confidence and NMS thresholds\n",
    "- Compare different model sizes (nano to xlarge)\n",
    "- Apply YOLO to real-world scenarios\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "- **YOLOv8 Documentation**: https://docs.ultralytics.com\n",
    "- **Original YOLO Paper**: \"You Only Look Once: Unified, Real-Time Object Detection\" (Redmon et al., 2015)\n",
    "- **YOLOv3 Paper**: \"YOLOv3: An Incremental Improvement\" (Redmon & Farhadi, 2018)\n",
    "\n",
    "---\n",
    "\n",
    "**Ready to detect objects? Let's move to Notebook 02!** üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
