{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 - Complete Regularization Comparison\n",
    "\n",
    "**Course:** 21CSE558T - Deep Neural Network Architectures  \n",
    "**Module 4:** CNNs (Week 2 of 3)  \n",
    "**Date:** October 31, 2025  \n",
    "**Duration:** 60-75 minutes\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "1. Compare all regularization techniques side-by-side\n",
    "2. Understand the cumulative effect of combining techniques\n",
    "3. Diagnose overfitting and choose appropriate solutions\n",
    "4. Build production-ready modern CNNs\n",
    "5. Validate regularization effectiveness systematically\n",
    "\n",
    "---\n",
    "\n",
    "## Story: Character: Arjun's Restaurant Quality Control\n",
    "\n",
    "**Character: Arjun** runs a chain of restaurants. He noticed some branches had problems:\n",
    "- **Branch A:** Cooks memorize recipes perfectly but panic with ingredient substitutions\n",
    "- **Branch B:** Uses quality checkpoints at every cooking stage\n",
    "- **Branch C:** Trains chefs with varied ingredients and conditions\n",
    "- **Branch D:** Randomly tests chefs by removing team members (cross-training)\n",
    "- **Branch E:** Combines ALL techniques\n",
    "\n",
    "**Results:**\n",
    "- Branch A: Perfect in kitchen, terrible with customers (overfitting!)\n",
    "- Branch E: Consistently excellent everywhere (generalization!)\n",
    "\n",
    "**This is exactly what we'll compare: regularization techniques and their combinations!**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (\n",
    "    Conv2D, BatchNormalization, Activation,\n",
    "    MaxPooling2D, GlobalAveragePooling2D,\n",
    "    Dropout, Dense, Flatten\n",
    ")\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU available: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Prepare Dataset\n",
    "\n",
    "We'll use CIFAR-10 with a smaller subset for faster training and clearer comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CIFAR-10\n",
    "(x_train_full, y_train_full), (x_test_full, y_test_full) = cifar10.load_data()\n",
    "\n",
    "# Use smaller subset for faster demonstration\n",
    "# In real scenarios, use full dataset!\n",
    "TRAIN_SAMPLES = 10000\n",
    "TEST_SAMPLES = 2000\n",
    "\n",
    "x_train = x_train_full[:TRAIN_SAMPLES].astype('float32') / 255.0\n",
    "y_train = tf.keras.utils.to_categorical(y_train_full[:TRAIN_SAMPLES], 10)\n",
    "x_test = x_test_full[:TEST_SAMPLES].astype('float32') / 255.0\n",
    "y_test = tf.keras.utils.to_categorical(y_test_full[:TEST_SAMPLES], 10)\n",
    "\n",
    "# Class names\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', \n",
    "               'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "print(f\"Training samples: {x_train.shape[0]}\")\n",
    "print(f\"Test samples: {x_test.shape[0]}\")\n",
    "print(f\"Image shape: {x_train.shape[1:]}\")\n",
    "print(f\"Number of classes: {len(class_names)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Define All Model Variants\n",
    "\n",
    "We'll create 6 different models:\n",
    "1. **Baseline:** No regularization (old-style CNN)\n",
    "2. **+ Batch Normalization:** Add BatchNorm only\n",
    "3. **+ Dropout:** Add Dropout only\n",
    "4. **+ Data Augmentation:** Add augmentation only\n",
    "5. **+ Global Avg Pooling:** Modern architecture\n",
    "6. **ALL Combined:** Complete modern CNN\n",
    "\n",
    "### Character: Arjun's Restaurant Branches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_baseline_model():\n",
    "    \"\"\"\n",
    "    Model 1: Baseline (No Regularization)\n",
    "    Character: Arjun's Branch A - memorizes recipes, no adaptation\n",
    "    \n",
    "    Old-style CNN:\n",
    "    - Conv with built-in activation\n",
    "    - No BatchNorm\n",
    "    - No Dropout\n",
    "    - Flatten + Dense (parameter explosion)\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        Conv2D(32, (3,3), activation='relu', padding='same', input_shape=(32, 32, 3)),\n",
    "        MaxPooling2D((2,2)),\n",
    "        \n",
    "        Conv2D(64, (3,3), activation='relu', padding='same'),\n",
    "        MaxPooling2D((2,2)),\n",
    "        \n",
    "        Conv2D(128, (3,3), activation='relu', padding='same'),\n",
    "        MaxPooling2D((2,2)),\n",
    "        \n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(10, activation='softmax')\n",
    "    ], name='Baseline')\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_batchnorm_model():\n",
    "    \"\"\"\n",
    "    Model 2: With Batch Normalization\n",
    "    Character: Arjun's Branch B - quality checkpoints at every stage\n",
    "    \n",
    "    Improvements:\n",
    "    - Conv → BN → Activation pattern\n",
    "    - Faster training\n",
    "    - More stable gradients\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        Conv2D(32, (3,3), padding='same', input_shape=(32, 32, 3)),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "        MaxPooling2D((2,2)),\n",
    "        \n",
    "        Conv2D(64, (3,3), padding='same'),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "        MaxPooling2D((2,2)),\n",
    "        \n",
    "        Conv2D(128, (3,3), padding='same'),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "        MaxPooling2D((2,2)),\n",
    "        \n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(10, activation='softmax')\n",
    "    ], name='BatchNorm')\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_dropout_model():\n",
    "    \"\"\"\n",
    "    Model 3: With Dropout\n",
    "    Character: Arjun's Branch D - random chef absence for cross-training\n",
    "    \n",
    "    Improvements:\n",
    "    - Dropout after pooling (0.2, 0.3)\n",
    "    - Dropout before output (0.5)\n",
    "    - Forces network to not rely on specific neurons\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        Conv2D(32, (3,3), activation='relu', padding='same', input_shape=(32, 32, 3)),\n",
    "        MaxPooling2D((2,2)),\n",
    "        Dropout(0.2),\n",
    "        \n",
    "        Conv2D(64, (3,3), activation='relu', padding='same'),\n",
    "        MaxPooling2D((2,2)),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        Conv2D(128, (3,3), activation='relu', padding='same'),\n",
    "        MaxPooling2D((2,2)),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(10, activation='softmax')\n",
    "    ], name='Dropout')\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_modern_architecture_model():\n",
    "    \"\"\"\n",
    "    Model 5: Modern Architecture (Global Average Pooling)\n",
    "    \n",
    "    Improvements:\n",
    "    - GlobalAveragePooling2D instead of Flatten+Dense\n",
    "    - 512K parameters instead of 2M+\n",
    "    - Reduces overfitting dramatically\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        Conv2D(32, (3,3), activation='relu', padding='same', input_shape=(32, 32, 3)),\n",
    "        MaxPooling2D((2,2)),\n",
    "        \n",
    "        Conv2D(64, (3,3), activation='relu', padding='same'),\n",
    "        MaxPooling2D((2,2)),\n",
    "        \n",
    "        Conv2D(128, (3,3), activation='relu', padding='same'),\n",
    "        MaxPooling2D((2,2)),\n",
    "        \n",
    "        GlobalAveragePooling2D(),  # Modern approach!\n",
    "        Dense(10, activation='softmax')\n",
    "    ], name='GlobalAvgPool')\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_complete_modern_model():\n",
    "    \"\"\"\n",
    "    Model 6: Complete Modern CNN (ALL techniques combined)\n",
    "    Character: Arjun's Branch E - uses ALL quality techniques!\n",
    "    \n",
    "    Combines:\n",
    "    - Batch Normalization (faster training, regularization)\n",
    "    - Dropout (prevents co-adaptation)\n",
    "    - Global Average Pooling (parameter reduction)\n",
    "    - Proper layer stacking ([Conv→BN→ReLU]×2 → Pool)\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        # Block 1: 32 filters\n",
    "        Conv2D(32, (3,3), padding='same', input_shape=(32, 32, 3)),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "        Conv2D(32, (3,3), padding='same'),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "        MaxPooling2D((2,2)),\n",
    "        Dropout(0.2),\n",
    "        \n",
    "        # Block 2: 64 filters\n",
    "        Conv2D(64, (3,3), padding='same'),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "        Conv2D(64, (3,3), padding='same'),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "        MaxPooling2D((2,2)),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        # Block 3: 128 filters\n",
    "        Conv2D(128, (3,3), padding='same'),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "        GlobalAveragePooling2D(),\n",
    "        \n",
    "        # Output\n",
    "        Dropout(0.5),\n",
    "        Dense(10, activation='softmax')\n",
    "    ], name='Modern_Complete')\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "print(\"✅ All model architectures defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Compare Model Parameters\n",
    "\n",
    "Before training, let's compare the number of parameters.\n",
    "\n",
    "**Hypothesis:** Modern architecture with Global Average Pooling will have far fewer parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create all models\n",
    "models = {\n",
    "    '1. Baseline': create_baseline_model(),\n",
    "    '2. + BatchNorm': create_batchnorm_model(),\n",
    "    '3. + Dropout': create_dropout_model(),\n",
    "    '5. + GlobalAvgPool': create_modern_architecture_model(),\n",
    "    '6. ALL Combined': create_complete_modern_model()\n",
    "}\n",
    "\n",
    "# Compare parameters\n",
    "print(\"=\" * 80)\n",
    "print(\"MODEL PARAMETER COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Model':<25} {'Total Params':>15} {'Trainable':>15}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for name, model in models.items():\n",
    "    total_params = model.count_params()\n",
    "    trainable_params = sum([tf.size(w).numpy() for w in model.trainable_weights])\n",
    "    print(f\"{name:<25} {total_params:>15,} {trainable_params:>15,}\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Calculate parameter reduction\n",
    "baseline_params = models['1. Baseline'].count_params()\n",
    "modern_params = models['6. ALL Combined'].count_params()\n",
    "reduction = (baseline_params - modern_params) / baseline_params * 100\n",
    "\n",
    "print(f\"\\n💡 KEY INSIGHT:\")\n",
    "print(f\"   Modern architecture has {reduction:.1f}% fewer parameters!\")\n",
    "print(f\"   But we'll see it performs BETTER! (Less overfitting)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Train All Models\n",
    "\n",
    "Now let's train each model and record the results.\n",
    "\n",
    "**Training Configuration:**\n",
    "- Epochs: 20\n",
    "- Batch size: 64\n",
    "- Early stopping: patience=5 on validation loss\n",
    "\n",
    "**Note:** Model 4 (+ Data Augmentation) will be trained with augmented data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Early stopping callback\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=0)\n",
    "\n",
    "# Storage for results\n",
    "histories = {}\n",
    "training_times = {}\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"TRAINING ALL MODELS\")\n",
    "print(\"=\" * 80)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Model 1: Baseline\n",
    "print(\"[1/6] Training Baseline (No Regularization)...\")\n",
    "print(\"Character: Arjun's Branch A - memorizes recipes\")\n",
    "start_time = time.time()\n",
    "histories['1. Baseline'] = models['1. Baseline'].fit(\n",
    "    x_train, y_train,\n",
    "    validation_data=(x_test, y_test),\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=0\n",
    ")\n",
    "training_times['1. Baseline'] = time.time() - start_time\n",
    "print(f\"   ✅ Complete in {training_times['1. Baseline']:.1f}s\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Model 2: BatchNorm\n",
    "print(\"[2/6] Training with Batch Normalization...\")\n",
    "print(\"Character: Arjun's Branch B - quality checkpoints\")\n",
    "start_time = time.time()\n",
    "histories['2. + BatchNorm'] = models['2. + BatchNorm'].fit(\n",
    "    x_train, y_train,\n",
    "    validation_data=(x_test, y_test),\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=0\n",
    ")\n",
    "training_times['2. + BatchNorm'] = time.time() - start_time\n",
    "print(f\"   ✅ Complete in {training_times['2. + BatchNorm']:.1f}s\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Model 3: Dropout\n",
    "print(\"[3/6] Training with Dropout...\")\n",
    "print(\"Character: Arjun's Branch D - random chef absence\")\n",
    "start_time = time.time()\n",
    "histories['3. + Dropout'] = models['3. + Dropout'].fit(\n",
    "    x_train, y_train,\n",
    "    validation_data=(x_test, y_test),\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=0\n",
    ")\n",
    "training_times['3. + Dropout'] = time.time() - start_time\n",
    "print(f\"   ✅ Complete in {training_times['3. + Dropout']:.1f}s\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Model 4: Data Augmentation (using baseline architecture + augmentation)\n",
    "print(\"[4/6] Training with Data Augmentation...\")\n",
    "print(\"Character: Arjun's Branch C - varied ingredient training\")\n",
    "\n",
    "# Create augmentation generator\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    zoom_range=0.1\n",
    ")\n",
    "\n",
    "# Create new baseline model for augmentation\n",
    "model_aug = create_baseline_model()\n",
    "models['4. + Augmentation'] = model_aug\n",
    "\n",
    "start_time = time.time()\n",
    "histories['4. + Augmentation'] = model_aug.fit(\n",
    "    train_datagen.flow(x_train, y_train, batch_size=BATCH_SIZE),\n",
    "    validation_data=(x_test, y_test),\n",
    "    epochs=EPOCHS,\n",
    "    steps_per_epoch=len(x_train) // BATCH_SIZE,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=0\n",
    ")\n",
    "training_times['4. + Augmentation'] = time.time() - start_time\n",
    "print(f\"   ✅ Complete in {training_times['4. + Augmentation']:.1f}s\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Model 5: Global Average Pooling\n",
    "print(\"[5/6] Training with Global Average Pooling...\")\n",
    "start_time = time.time()\n",
    "histories['5. + GlobalAvgPool'] = models['5. + GlobalAvgPool'].fit(\n",
    "    x_train, y_train,\n",
    "    validation_data=(x_test, y_test),\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=0\n",
    ")\n",
    "training_times['5. + GlobalAvgPool'] = time.time() - start_time\n",
    "print(f\"   ✅ Complete in {training_times['5. + GlobalAvgPool']:.1f}s\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Model 6: Complete Modern CNN\n",
    "print(\"[6/6] Training Complete Modern CNN (ALL techniques)...\")\n",
    "print(\"Character: Arjun's Branch E - uses everything!\")\n",
    "start_time = time.time()\n",
    "histories['6. ALL Combined'] = models['6. ALL Combined'].fit(\n",
    "    x_train, y_train,\n",
    "    validation_data=(x_test, y_test),\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=0\n",
    ")\n",
    "training_times['6. ALL Combined'] = time.time() - start_time\n",
    "print(f\"   ✅ Complete in {training_times['6. ALL Combined']:.1f}s\\n\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"✅ ALL MODELS TRAINED!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Results Comparison\n",
    "\n",
    "Now let's compare all models across multiple metrics:\n",
    "1. Final training accuracy\n",
    "2. Final test accuracy\n",
    "3. Overfitting gap (train - test)\n",
    "4. Training time\n",
    "5. Training curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract final results\n",
    "results = {}\n",
    "for name, history in histories.items():\n",
    "    train_acc = history.history['accuracy'][-1]\n",
    "    val_acc = history.history['val_accuracy'][-1]\n",
    "    gap = train_acc - val_acc\n",
    "    \n",
    "    results[name] = {\n",
    "        'train_acc': train_acc,\n",
    "        'val_acc': val_acc,\n",
    "        'gap': gap,\n",
    "        'time': training_times[name],\n",
    "        'epochs': len(history.history['accuracy'])\n",
    "    }\n",
    "\n",
    "# Display comparison table\n",
    "print(\"=\" * 100)\n",
    "print(\"FINAL RESULTS COMPARISON\")\n",
    "print(\"=\" * 100)\n",
    "print(f\"{'Model':<25} {'Train Acc':>12} {'Test Acc':>12} {'Gap':>10} {'Time':>10} {'Epochs':>8}\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "for name, res in results.items():\n",
    "    print(f\"{name:<25} {res['train_acc']:>11.1%} {res['val_acc']:>11.1%} \"\n",
    "          f\"{res['gap']:>9.1%} {res['time']:>9.1f}s {res['epochs']:>7}\")\n",
    "\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# Find best model\n",
    "best_model = max(results.items(), key=lambda x: x[1]['val_acc'])\n",
    "print(f\"\\n🏆 BEST MODEL: {best_model[0]}\")\n",
    "print(f\"   Test Accuracy: {best_model[1]['val_acc']:.1%}\")\n",
    "print(f\"   Overfitting Gap: {best_model[1]['gap']:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training curves for all models\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (name, history) in enumerate(histories.items()):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Plot accuracy\n",
    "    ax.plot(history.history['accuracy'], 'b-', label='Train', linewidth=2)\n",
    "    ax.plot(history.history['val_accuracy'], 'r-', label='Test', linewidth=2)\n",
    "    \n",
    "    # Add gap annotation\n",
    "    final_gap = results[name]['gap']\n",
    "    gap_color = 'red' if final_gap > 0.15 else 'orange' if final_gap > 0.08 else 'green'\n",
    "    \n",
    "    ax.set_title(f\"{name}\\nGap: {final_gap:.1%}\", fontsize=14, fontweight='bold', color=gap_color)\n",
    "    ax.set_xlabel('Epoch', fontsize=11)\n",
    "    ax.set_ylabel('Accuracy', fontsize=11)\n",
    "    ax.legend(fontsize=10)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_ylim([0.3, 1.0])\n",
    "\n",
    "plt.suptitle(\"Training Curves Comparison - All Regularization Techniques\\nCharacter: Arjun's Restaurant Branches\", \n",
    "             fontsize=18, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Key Insights and Analysis\n",
    "\n",
    "**Let's analyze what we learned from this comprehensive comparison.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparative improvements\n",
    "baseline_val = results['1. Baseline']['val_acc']\n",
    "baseline_gap = results['1. Baseline']['gap']\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"KEY INSIGHTS: Regularization Technique Effectiveness\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "for name, res in results.items():\n",
    "    if name == '1. Baseline':\n",
    "        continue\n",
    "    \n",
    "    val_improvement = res['val_acc'] - baseline_val\n",
    "    gap_reduction = baseline_gap - res['gap']\n",
    "    gap_reduction_pct = (gap_reduction / baseline_gap) * 100 if baseline_gap > 0 else 0\n",
    "    \n",
    "    print(f\"📊 {name}:\")\n",
    "    print(f\"   Test Accuracy:  {res['val_acc']:.1%} ({val_improvement:+.1%} vs baseline)\")\n",
    "    print(f\"   Overfitting:    {res['gap']:.1%} ({gap_reduction_pct:+.0f}% reduction)\")\n",
    "    print(f\"   Training Time:  {res['time']:.1f}s\")\n",
    "    print()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "print(\"💡 RANKING BY TEST ACCURACY:\")\n",
    "ranked = sorted(results.items(), key=lambda x: x[1]['val_acc'], reverse=True)\n",
    "for rank, (name, res) in enumerate(ranked, 1):\n",
    "    print(f\"   {rank}. {name:<25} {res['val_acc']:.1%}\")\n",
    "\n",
    "print()\n",
    "print(\"💡 RANKING BY OVERFITTING (Lower is better):\")\n",
    "ranked = sorted(results.items(), key=lambda x: x[1]['gap'])\n",
    "for rank, (name, res) in enumerate(ranked, 1):\n",
    "    print(f\"   {rank}. {name:<25} {res['gap']:.1%}\")\n",
    "\n",
    "print()\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Visual Summary - Side-by-Side Comparison\n",
    "\n",
    "**One final visualization to see everything at a glance.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive comparison chart\n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 14))\n",
    "\n",
    "model_names = list(results.keys())\n",
    "train_accs = [results[m]['train_acc'] for m in model_names]\n",
    "val_accs = [results[m]['val_acc'] for m in model_names]\n",
    "gaps = [results[m]['gap'] for m in model_names]\n",
    "times = [results[m]['time'] for m in model_names]\n",
    "\n",
    "x_pos = np.arange(len(model_names))\n",
    "short_names = ['Baseline', 'BatchNorm', 'Dropout', 'Augment', 'GlobalAvg', 'Modern']\n",
    "\n",
    "# 1. Accuracy comparison\n",
    "axes[0, 0].bar(x_pos - 0.2, train_accs, 0.4, label='Train Acc', color='skyblue')\n",
    "axes[0, 0].bar(x_pos + 0.2, val_accs, 0.4, label='Test Acc', color='coral')\n",
    "axes[0, 0].set_ylabel('Accuracy', fontsize=12)\n",
    "axes[0, 0].set_title('Train vs Test Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_xticks(x_pos)\n",
    "axes[0, 0].set_xticklabels(short_names, rotation=45, ha='right')\n",
    "axes[0, 0].legend(fontsize=11)\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "axes[0, 0].set_ylim([0.5, 1.0])\n",
    "\n",
    "# 2. Overfitting gap\n",
    "colors = ['red' if g > 0.15 else 'orange' if g > 0.08 else 'green' for g in gaps]\n",
    "axes[0, 1].bar(x_pos, gaps, color=colors)\n",
    "axes[0, 1].set_ylabel('Overfitting Gap', fontsize=12)\n",
    "axes[0, 1].set_title('Overfitting Comparison (Lower is Better)', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_xticks(x_pos)\n",
    "axes[0, 1].set_xticklabels(short_names, rotation=45, ha='right')\n",
    "axes[0, 1].axhline(y=0.08, color='green', linestyle='--', label='Good (<8%)', linewidth=2)\n",
    "axes[0, 1].axhline(y=0.15, color='orange', linestyle='--', label='Warning (<15%)', linewidth=2)\n",
    "axes[0, 1].legend(fontsize=10)\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Training time\n",
    "axes[1, 0].bar(x_pos, times, color='lightgreen')\n",
    "axes[1, 0].set_ylabel('Time (seconds)', fontsize=12)\n",
    "axes[1, 0].set_title('Training Time Comparison', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_xticks(x_pos)\n",
    "axes[1, 0].set_xticklabels(short_names, rotation=45, ha='right')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Final convergence curves (all on one plot)\n",
    "for name in histories.keys():\n",
    "    short_name = short_names[list(histories.keys()).index(name)]\n",
    "    axes[1, 1].plot(histories[name].history['val_accuracy'], label=short_name, linewidth=2)\n",
    "\n",
    "axes[1, 1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1, 1].set_ylabel('Test Accuracy', fontsize=12)\n",
    "axes[1, 1].set_title('Test Accuracy Convergence', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].legend(fontsize=10)\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "axes[1, 1].set_ylim([0.3, 1.0])\n",
    "\n",
    "plt.suptitle(\"Complete Regularization Technique Comparison\\nCharacter: Arjun's Restaurant Performance Dashboard\", \n",
    "             fontsize=18, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: Key Takeaways\n",
    "\n",
    "### 1. Individual Technique Effects\n",
    "- **Batch Normalization:** Faster convergence, slight regularization\n",
    "- **Dropout:** Strong regularization, prevents co-adaptation\n",
    "- **Data Augmentation:** Improves generalization, more varied training\n",
    "- **Global Average Pooling:** Parameter reduction, less overfitting\n",
    "\n",
    "### 2. Cumulative Effect\n",
    "Combining **ALL techniques** (Model 6) typically gives:\n",
    "- Best test accuracy\n",
    "- Lowest overfitting gap\n",
    "- Best generalization\n",
    "- Production-ready performance\n",
    "\n",
    "### 3. Trade-offs\n",
    "- **Training time:** BatchNorm adds overhead but converges faster\n",
    "- **Train accuracy:** Regularization lowers train accuracy but improves test\n",
    "- **Complexity:** More techniques = more hyperparameters to tune\n",
    "\n",
    "### 4. Practical Guidelines\n",
    "\n",
    "**Start with:** BatchNormalization everywhere (almost no downsides)\n",
    "\n",
    "**If overfitting:**\n",
    "1. Add data augmentation (if dataset < 50K)\n",
    "2. Add dropout (0.5 for FC, 0.2-0.3 for Conv)\n",
    "3. Use Global Average Pooling instead of Flatten+Dense\n",
    "4. Reduce model size\n",
    "5. Get more training data\n",
    "\n",
    "**If underfitting:**\n",
    "1. Remove some dropout\n",
    "2. Increase model capacity\n",
    "3. Train longer\n",
    "4. Reduce augmentation intensity\n",
    "\n",
    "### 5. Character: Arjun's Restaurant Lesson\n",
    "**Best restaurant (Branch E)** uses ALL quality techniques:\n",
    "- Quality checkpoints (BatchNorm)\n",
    "- Cross-training (Dropout)\n",
    "- Varied ingredient practice (Augmentation)\n",
    "- Efficient operations (Global Average Pooling)\n",
    "\n",
    "**Result:** Consistently excellent across all locations!\n",
    "\n",
    "### 6. Modern CNN Checklist\n",
    "When building production CNNs, include:\n",
    "- ✅ Batch Normalization (after Conv, before activation)\n",
    "- ✅ Dropout (0.2-0.3 after pooling, 0.5 before output)\n",
    "- ✅ Data Augmentation (if dataset < 50K)\n",
    "- ✅ Global Average Pooling (replace Flatten+Dense)\n",
    "- ✅ Early Stopping (monitor val_loss, patience=5)\n",
    "- ✅ Proper layer stacking ([Conv→BN→ReLU]×N → Pool)\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. Review all DO3 Oct-31 materials:\n",
    "   - Comprehensive lecture notes\n",
    "   - Notebook 01: Pooling layers\n",
    "   - Notebook 02: Batch Normalization\n",
    "   - Notebook 03: Data Augmentation\n",
    "   - Notebook 04: This comparison (complete!)\n",
    "   - Quick reference cheat sheet\n",
    "   - Architecture design worksheet\n",
    "\n",
    "2. Complete architecture design worksheet\n",
    "\n",
    "3. Prepare for Tutorial T11 (Monday, Nov 3):\n",
    "   - CIFAR-10 implementation\n",
    "   - Apply all learned techniques\n",
    "\n",
    "---\n",
    "\n",
    "**🎯 Learning Objective Check:**\n",
    "- ✅ Compared all regularization techniques side-by-side\n",
    "- ✅ Understood cumulative effects of combining techniques\n",
    "- ✅ Can diagnose overfitting and choose solutions\n",
    "- ✅ Can build production-ready modern CNNs\n",
    "- ✅ Validated regularization effectiveness systematically\n",
    "\n",
    "**Congratulations! You've completed the comprehensive regularization comparison!** 🎉"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
