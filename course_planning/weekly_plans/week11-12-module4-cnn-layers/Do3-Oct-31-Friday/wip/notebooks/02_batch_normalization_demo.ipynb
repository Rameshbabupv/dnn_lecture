{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 2: Batch Normalization Demo\n",
    "\n",
    "**Course:** 21CSE558T - Deep Neural Network Architectures  \n",
    "**Module:** 4 - CNNs (Week 2 of 3)  \n",
    "**Date:** October 31, 2025  \n",
    "**Duration:** ~25 minutes\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "1. Explain the Internal Covariate Shift problem\n",
    "2. Understand how Batch Normalization works mathematically\n",
    "3. Know the correct placement of BatchNorm layers\n",
    "4. Observe the training speed improvement from BatchNorm\n",
    "5. Implement BatchNormalization in Keras correctly\n",
    "\n",
    "---\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import (\n",
    "    Conv2D, BatchNormalization, Activation, \n",
    "    MaxPooling2D, GlobalAveragePooling2D, Dense, Flatten\n",
    ")\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: The Internal Covariate Shift Problem\n",
    "\n",
    "### Character: Sneha's Factory Assembly Line\n",
    "\n",
    "**Meet Character: Sneha - Factory Manager**\n",
    "\n",
    "**The Problem:**\n",
    "\n",
    "**Character: Sneha** manages an assembly line with 5 stations:\n",
    "\n",
    "**Day 1:** Station 1 receives parts sized 10-20cm\n",
    "- Station 2 calibrated for 10-20cm inputs ‚úÖ\n",
    "- Station 3 calibrated for 15-25cm outputs from Station 2 ‚úÖ\n",
    "- Everything works smoothly!\n",
    "\n",
    "**Day 2:** Station 1 suddenly produces parts sized 50-80cm\n",
    "- Station 2 NOT calibrated for 50-80cm (expects 10-20cm) ‚ùå\n",
    "- Station 2 produces weird 70-100cm outputs\n",
    "- Station 3 completely confused (expects 15-25cm, gets 70-100cm) ‚ùå\n",
    "- Entire line breaks down!\n",
    "\n",
    "**Character: Sneha's Solution:**\n",
    "\"Add a quality control checkpoint after EACH station that normalizes parts to expected size range BEFORE sending to next station!\"\n",
    "\n",
    "**Batch Normalization = Quality Control for Neural Networks**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Understanding Batch Normalization Mathematics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate layer outputs WITHOUT normalization\n",
    "# These values shift during training (Internal Covariate Shift)\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Batch of 32 samples, 10 features\n",
    "batch_size = 32\n",
    "num_features = 10\n",
    "\n",
    "# Simulate unstable layer outputs\n",
    "layer_output_epoch1 = np.random.randn(batch_size, num_features) * 2 + 5  # Mean‚âà5, Std‚âà2\n",
    "layer_output_epoch2 = np.random.randn(batch_size, num_features) * 5 + 15  # Mean‚âà15, Std‚âà5\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"INTERNAL COVARIATE SHIFT DEMONSTRATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nüìä Epoch 1 Layer Output:\")\n",
    "print(f\"  Mean: {layer_output_epoch1.mean():.2f}\")\n",
    "print(f\"  Std:  {layer_output_epoch1.std():.2f}\")\n",
    "print(f\"  Range: [{layer_output_epoch1.min():.2f}, {layer_output_epoch1.max():.2f}]\")\n",
    "\n",
    "print(\"\\nüìä Epoch 2 Layer Output (SHIFTED!):\")\n",
    "print(f\"  Mean: {layer_output_epoch2.mean():.2f}\")\n",
    "print(f\"  Std:  {layer_output_epoch2.std():.2f}\")\n",
    "print(f\"  Range: [{layer_output_epoch2.min():.2f}, {layer_output_epoch2.max():.2f}]\")\n",
    "\n",
    "print(\"\\n‚ùå Problem: Next layer expects inputs from Epoch 1 distribution\")\n",
    "print(\"   But gets inputs from Epoch 2 distribution (very different!)\")\n",
    "print(\"   ‚Üí Slow, unstable training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the distribution shift\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Epoch 1 distribution\n",
    "axes[0].hist(layer_output_epoch1.flatten(), bins=30, alpha=0.7, color='blue', edgecolor='black')\n",
    "axes[0].axvline(layer_output_epoch1.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {layer_output_epoch1.mean():.2f}')\n",
    "axes[0].set_title('Epoch 1: Layer Output Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Value')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Epoch 2 distribution (SHIFTED)\n",
    "axes[1].hist(layer_output_epoch2.flatten(), bins=30, alpha=0.7, color='orange', edgecolor='black')\n",
    "axes[1].axvline(layer_output_epoch2.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {layer_output_epoch2.mean():.2f}')\n",
    "axes[1].set_title('Epoch 2: Layer Output Distribution (SHIFTED!)', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Value')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Distribution keeps shifting ‚Üí Next layer must constantly adapt!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Normalization: The Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_batch_norm(x, gamma=1.0, beta=0.0, epsilon=1e-5):\n",
    "    \"\"\"\n",
    "    Manual Batch Normalization implementation.\n",
    "    \n",
    "    Steps:\n",
    "    1. Calculate batch mean (Œº) and variance (œÉ¬≤)\n",
    "    2. Normalize: x_hat = (x - Œº) / sqrt(œÉ¬≤ + Œµ)\n",
    "    3. Scale and shift: y = Œ≥ * x_hat + Œ≤\n",
    "    \n",
    "    Args:\n",
    "        x: Input data (batch_size, num_features)\n",
    "        gamma: Learnable scale parameter\n",
    "        beta: Learnable shift parameter\n",
    "        epsilon: Small constant to prevent division by zero\n",
    "    \n",
    "    Returns:\n",
    "        Normalized output\n",
    "    \"\"\"\n",
    "    # Step 1: Calculate batch statistics\n",
    "    batch_mean = np.mean(x, axis=0)\n",
    "    batch_var = np.var(x, axis=0)\n",
    "    batch_std = np.sqrt(batch_var + epsilon)\n",
    "    \n",
    "    print(\"Step 1: Batch Statistics\")\n",
    "    print(f\"  Batch Mean: {batch_mean[:3]} ...\")\n",
    "    print(f\"  Batch Std:  {batch_std[:3]} ...\")\n",
    "    \n",
    "    # Step 2: Normalize\n",
    "    x_normalized = (x - batch_mean) / batch_std\n",
    "    \n",
    "    print(\"\\nStep 2: After Normalization\")\n",
    "    print(f\"  Mean: {x_normalized.mean():.6f} (‚âà0)\")\n",
    "    print(f\"  Std:  {x_normalized.std():.6f} (‚âà1)\")\n",
    "    \n",
    "    # Step 3: Scale and shift\n",
    "    y = gamma * x_normalized + beta\n",
    "    \n",
    "    print(\"\\nStep 3: After Scale (Œ≥) and Shift (Œ≤)\")\n",
    "    print(f\"  Œ≥ (gamma): {gamma}\")\n",
    "    print(f\"  Œ≤ (beta):  {beta}\")\n",
    "    print(f\"  Output Mean: {y.mean():.6f}\")\n",
    "    print(f\"  Output Std:  {y.std():.6f}\")\n",
    "    \n",
    "    return y\n",
    "\n",
    "# Apply Batch Normalization to Epoch 2 data\n",
    "print(\"=\"*60)\n",
    "print(\"APPLYING BATCH NORMALIZATION\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nOriginal Epoch 2 Statistics:\")\n",
    "print(f\"  Mean: {layer_output_epoch2.mean():.2f}\")\n",
    "print(f\"  Std:  {layer_output_epoch2.std():.2f}\")\n",
    "print(\"\\n\" + \"-\"*60 + \"\\n\")\n",
    "\n",
    "normalized_output = manual_batch_norm(layer_output_epoch2, gamma=1.0, beta=0.0)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ Result: Stable distribution (Mean‚âà0, Std‚âà1) regardless of input!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the normalization effect\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Before BatchNorm\n",
    "axes[0].hist(layer_output_epoch2.flatten(), bins=30, alpha=0.7, color='orange', edgecolor='black')\n",
    "axes[0].axvline(layer_output_epoch2.mean(), color='red', linestyle='--', linewidth=2, \n",
    "               label=f'Mean: {layer_output_epoch2.mean():.2f}')\n",
    "axes[0].set_title('Before Batch Normalization\\n(Shifted Distribution)', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Value')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# After BatchNorm\n",
    "axes[1].hist(normalized_output.flatten(), bins=30, alpha=0.7, color='green', edgecolor='black')\n",
    "axes[1].axvline(normalized_output.mean(), color='red', linestyle='--', linewidth=2, \n",
    "               label=f'Mean: {normalized_output.mean():.4f} ‚âà 0')\n",
    "axes[1].set_title('After Batch Normalization\\n(Normalized: Mean‚âà0, Std‚âà1)', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Value')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Batch Normalization stabilizes layer inputs ‚Üí Faster, more stable training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Batch Normalization Placement\n",
    "\n",
    "### Modern Best Practice: Conv ‚Üí BatchNorm ‚Üí Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"BATCH NORMALIZATION PLACEMENT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n‚ùå OLD (Incorrect Placement):\")\n",
    "print(\"-\" * 60)\n",
    "print(\"Conv2D(32, activation='relu')  ‚Üê Activation built-in\")\n",
    "print(\"BatchNormalization()            ‚Üê After activation (less effective)\")\n",
    "\n",
    "print(\"\\n‚úÖ MODERN (Correct Placement):\")\n",
    "print(\"-\" * 60)\n",
    "print(\"Conv2D(32)                      ‚Üê No activation\")\n",
    "print(\"BatchNormalization()            ‚Üê BEFORE activation\")\n",
    "print(\"Activation('relu')              ‚Üê Activation applied last\")\n",
    "\n",
    "print(\"\\nüí° Why this order?\")\n",
    "print(\"  1. Conv2D: Linear transformation (can shift distribution)\")\n",
    "print(\"  2. BatchNorm: Stabilize distribution before non-linearity\")\n",
    "print(\"  3. ReLU: Apply non-linearity to normalized values\")\n",
    "print(\"\\n  Result: Stable gradients, faster convergence!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build example architectures\n",
    "\n",
    "# WITHOUT BatchNorm (Old approach)\n",
    "model_without_bn = Sequential([\n",
    "    Conv2D(32, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    MaxPooling2D((2,2)),\n",
    "    Conv2D(64, (3,3), activation='relu'),\n",
    "    MaxPooling2D((2,2)),\n",
    "    Flatten(),\n",
    "    Dense(10, activation='softmax')\n",
    "], name='Without_BatchNorm')\n",
    "\n",
    "print(\"\\nModel WITHOUT Batch Normalization:\")\n",
    "print(\"=\"*60)\n",
    "model_without_bn.summary()\n",
    "\n",
    "# WITH BatchNorm (Modern approach)\n",
    "model_with_bn = Sequential([\n",
    "    Conv2D(32, (3,3), input_shape=(28, 28, 1)),  # No activation\n",
    "    BatchNormalization(),                         # Add BatchNorm\n",
    "    Activation('relu'),                           # Then activation\n",
    "    MaxPooling2D((2,2)),\n",
    "    \n",
    "    Conv2D(64, (3,3)),                           # No activation\n",
    "    BatchNormalization(),                         # Add BatchNorm\n",
    "    Activation('relu'),                           # Then activation\n",
    "    MaxPooling2D((2,2)),\n",
    "    \n",
    "    Flatten(),\n",
    "    Dense(10, activation='softmax')\n",
    "], name='With_BatchNorm')\n",
    "\n",
    "print(\"\\nModel WITH Batch Normalization:\")\n",
    "print(\"=\"*60)\n",
    "model_with_bn.summary()\n",
    "\n",
    "# Compare parameters\n",
    "params_without = model_without_bn.count_params()\n",
    "params_with = model_with_bn.count_params()\n",
    "extra_params = params_with - params_without\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PARAMETER COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Without BatchNorm: {params_without:,} parameters\")\n",
    "print(f\"With BatchNorm:    {params_with:,} parameters\")\n",
    "print(f\"Extra parameters:  {extra_params:,} ({extra_params/params_without*100:.1f}% increase)\")\n",
    "print(\"\\nüí° Small parameter overhead, HUGE training benefit!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Training Speed Comparison (Live Demo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Fashion-MNIST dataset\n",
    "print(\"Loading Fashion-MNIST dataset...\")\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "# Preprocess\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "x_train = x_train.reshape(-1, 28, 28, 1)\n",
    "x_test = x_test.reshape(-1, 28, 28, 1)\n",
    "\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "# Use subset for faster demo (10% of data)\n",
    "subset_size = len(x_train) // 10\n",
    "x_train_subset = x_train[:subset_size]\n",
    "y_train_subset = y_train[:subset_size]\n",
    "\n",
    "print(f\"\\nTraining subset: {len(x_train_subset):,} samples\")\n",
    "print(f\"Test set: {len(x_test):,} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile both models\n",
    "model_without_bn.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model_with_bn.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"Models compiled and ready to train!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train WITHOUT BatchNorm\n",
    "print(\"=\"*60)\n",
    "print(\"Training Model WITHOUT Batch Normalization\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "history_without_bn = model_without_bn.fit(\n",
    "    x_train_subset, y_train_subset,\n",
    "    validation_data=(x_test, y_test),\n",
    "    epochs=10,\n",
    "    batch_size=128,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train WITH BatchNorm\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Training Model WITH Batch Normalization\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "history_with_bn = model_with_bn.fit(\n",
    "    x_train_subset, y_train_subset,\n",
    "    validation_data=(x_test, y_test),\n",
    "    epochs=10,\n",
    "    batch_size=128,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Training Accuracy\n",
    "axes[0].plot(history_without_bn.history['accuracy'], 'o-', label='Without BatchNorm', linewidth=2, markersize=8)\n",
    "axes[0].plot(history_with_bn.history['accuracy'], 's-', label='With BatchNorm', linewidth=2, markersize=8)\n",
    "axes[0].set_title('Training Accuracy Comparison', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].legend(fontsize=12)\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Validation Accuracy\n",
    "axes[1].plot(history_without_bn.history['val_accuracy'], 'o-', label='Without BatchNorm', linewidth=2, markersize=8)\n",
    "axes[1].plot(history_with_bn.history['val_accuracy'], 's-', label='With BatchNorm', linewidth=2, markersize=8)\n",
    "axes[1].set_title('Validation Accuracy Comparison', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].legend(fontsize=12)\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print final results\n",
    "final_train_acc_without = history_without_bn.history['accuracy'][-1]\n",
    "final_val_acc_without = history_without_bn.history['val_accuracy'][-1]\n",
    "final_train_acc_with = history_with_bn.history['accuracy'][-1]\n",
    "final_val_acc_with = history_with_bn.history['val_accuracy'][-1]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL RESULTS (After 10 Epochs)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nWithout BatchNorm:\")\n",
    "print(f\"  Train Accuracy: {final_train_acc_without:.4f}\")\n",
    "print(f\"  Val Accuracy:   {final_val_acc_without:.4f}\")\n",
    "\n",
    "print(f\"\\nWith BatchNorm:\")\n",
    "print(f\"  Train Accuracy: {final_train_acc_with:.4f}\")\n",
    "print(f\"  Val Accuracy:   {final_val_acc_with:.4f}\")\n",
    "\n",
    "improvement = (final_val_acc_with - final_val_acc_without) * 100\n",
    "print(f\"\\n‚úÖ Improvement: {improvement:+.2f}% validation accuracy\")\n",
    "print(\"üí° BatchNorm converges faster AND generalizes better!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: When to Use Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"BATCH NORMALIZATION GUIDELINES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n‚úÖ USE Batch Normalization When:\")\n",
    "print(\"-\" * 60)\n",
    "print(\"  ‚Ä¢ Deep networks (>5 layers)\")\n",
    "print(\"  ‚Ä¢ Training is slow or unstable\")\n",
    "print(\"  ‚Ä¢ Want to use higher learning rates\")\n",
    "print(\"  ‚Ä¢ Building modern CNNs (almost always!)\")\n",
    "print(\"  ‚Ä¢ Batch size ‚â• 16 (reliable statistics)\")\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è BE CAREFUL When:\")\n",
    "print(\"-\" * 60)\n",
    "print(\"  ‚Ä¢ Very small batch size (< 8)\")\n",
    "print(\"    ‚Üí Use Layer Normalization or Group Normalization instead\")\n",
    "print(\"  ‚Ä¢ Recurrent networks (RNNs/LSTMs)\")\n",
    "print(\"    ‚Üí Layer Normalization often better\")\n",
    "print(\"  ‚Ä¢ Real-time inference (slight overhead)\")\n",
    "print(\"    ‚Üí Usually negligible, but consider for edge devices\")\n",
    "\n",
    "print(\"\\n‚ùå DON'T Use (Rare Cases):\")\n",
    "print(\"-\" * 60)\n",
    "print(\"  ‚Ä¢ Extremely shallow networks (2-3 layers)\")\n",
    "print(\"    ‚Üí Not enough depth to benefit\")\n",
    "print(\"  ‚Ä¢ Already using heavy regularization (might over-regularize)\")\n",
    "print(\"    ‚Üí Monitor for underfitting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Practice Exercise - Build Modern CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Build a modern CNN with proper BatchNorm placement\n",
    "\n",
    "def build_modern_cnn(input_shape=(32, 32, 3), num_classes=10):\n",
    "    \"\"\"\n",
    "    Build modern CNN with BatchNormalization.\n",
    "    \n",
    "    Architecture pattern:\n",
    "    [Conv ‚Üí BatchNorm ‚Üí ReLU] √ó 2 ‚Üí Pool ‚Üí Dropout\n",
    "    \"\"\"\n",
    "    from tensorflow.keras.layers import Dropout\n",
    "    \n",
    "    model = Sequential([\n",
    "        # Block 1: 32 filters\n",
    "        Conv2D(32, (3,3), padding='same', input_shape=input_shape),\n",
    "        BatchNormalization(),  # TODO: Add BatchNorm\n",
    "        Activation('relu'),\n",
    "        \n",
    "        Conv2D(32, (3,3), padding='same'),\n",
    "        BatchNormalization(),  # TODO: Add BatchNorm\n",
    "        Activation('relu'),\n",
    "        MaxPooling2D((2,2)),\n",
    "        Dropout(0.2),\n",
    "        \n",
    "        # Block 2: 64 filters\n",
    "        Conv2D(64, (3,3), padding='same'),\n",
    "        BatchNormalization(),  # TODO: Add BatchNorm\n",
    "        Activation('relu'),\n",
    "        \n",
    "        Conv2D(64, (3,3), padding='same'),\n",
    "        BatchNormalization(),  # TODO: Add BatchNorm\n",
    "        Activation('relu'),\n",
    "        MaxPooling2D((2,2)),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        # Block 3: 128 filters\n",
    "        Conv2D(128, (3,3), padding='same'),\n",
    "        BatchNormalization(),  # TODO: Add BatchNorm\n",
    "        Activation('relu'),\n",
    "        GlobalAveragePooling2D(),\n",
    "        \n",
    "        # Output\n",
    "        Dropout(0.5),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ], name='Modern_CNN_with_BatchNorm')\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Build and display model\n",
    "modern_cnn = build_modern_cnn()\n",
    "modern_cnn.summary()\n",
    "\n",
    "print(\"\\n‚úÖ Modern CNN with BatchNormalization created!\")\n",
    "print(\"üí° Ready for Monday's Tutorial T11 (CIFAR-10 implementation)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Internal Covariate Shift Problem:**\n",
    "   - Layer inputs shift during training\n",
    "   - Each layer must constantly adapt\n",
    "   - Slows down training, unstable gradients\n",
    "\n",
    "2. **Batch Normalization Solution:**\n",
    "   - Normalizes layer inputs to mean=0, std=1\n",
    "   - Learns optimal scale (Œ≥) and shift (Œ≤)\n",
    "   - Stabilizes training, enables higher learning rates\n",
    "\n",
    "3. **Correct Placement:**\n",
    "   - **Modern:** Conv ‚Üí BatchNorm ‚Üí Activation\n",
    "   - NOT Conv(activation) ‚Üí BatchNorm\n",
    "\n",
    "4. **Benefits:**\n",
    "   - ‚úÖ 2-3√ó faster training\n",
    "   - ‚úÖ Acts as regularization\n",
    "   - ‚úÖ Less sensitive to initialization\n",
    "   - ‚úÖ Allows deeper networks\n",
    "   - ‚úÖ More stable gradients\n",
    "\n",
    "5. **When to Use:**\n",
    "   - Almost always for modern CNNs!\n",
    "   - Deep networks (>5 layers)\n",
    "   - Batch size ‚â• 16\n",
    "\n",
    "### Character: Sneha's Summary\n",
    "\n",
    "**Character: Sneha** says:\n",
    "- \"Quality control checkpoints after each station keep production stable!\"\n",
    "- \"Workers (next layers) always receive parts in expected size range\"\n",
    "- \"Assembly line runs smoothly and efficiently!\"\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "In the next notebook, we'll explore **Data Augmentation** - artificially expanding your training dataset!\n",
    "\n",
    "---\n",
    "\n",
    "**End of Notebook 2**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
