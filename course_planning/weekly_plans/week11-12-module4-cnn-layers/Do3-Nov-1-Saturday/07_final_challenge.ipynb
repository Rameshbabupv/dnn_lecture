{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 7: Final CNN Challenge üèÜ\n",
    "\n",
    "**Course:** 21CSE558T - Deep Neural Network Architectures  \n",
    "**Module 4:** CNNs - Practical Session  \n",
    "**Type:** FINAL SUBMISSION ASSIGNMENT  \n",
    "**Due:** [Instructor will specify]  \n",
    "**Weight:** 25 points (major assignment)\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Challenge Objective\n",
    "\n",
    "Build the **best possible CNN** for Fashion-MNIST classification using all techniques you learned:\n",
    "\n",
    "‚úÖ Convolution fundamentals  \n",
    "‚úÖ CNN architecture design  \n",
    "‚úÖ Regularization (Dropout, BatchNorm)  \n",
    "‚úÖ Data augmentation  \n",
    "‚úÖ Hyperparameter tuning  \n",
    "\n",
    "---\n",
    "\n",
    "## üèÅ Target Performance\n",
    "\n",
    "| Grade | Test Accuracy | Description |\n",
    "|-------|--------------|-------------|\n",
    "| ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Excellent | ‚â•93% | Outstanding performance |\n",
    "| ‚≠ê‚≠ê‚≠ê‚≠ê Very Good | 91-92.9% | Strong CNN design |\n",
    "| ‚≠ê‚≠ê‚≠ê Good | 89-90.9% | Solid understanding |\n",
    "| ‚≠ê‚≠ê Acceptable | 87-88.9% | Basic proficiency |\n",
    "| ‚≠ê Needs Work | <87% | Requires improvement |\n",
    "\n",
    "**Baseline to beat:** Simple CNN achieves ~88%. Your optimized model should do better!\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Assignment Requirements\n",
    "\n",
    "### Mandatory Components:\n",
    "\n",
    "1. ‚úÖ **Architecture Design** - Justify your CNN structure\n",
    "2. ‚úÖ **Regularization** - Use at least 2 techniques\n",
    "3. ‚úÖ **Data Augmentation** - Apply appropriate transformations\n",
    "4. ‚úÖ **Training Strategy** - Show training curves\n",
    "5. ‚úÖ **Evaluation** - Test set performance + analysis\n",
    "6. ‚úÖ **Documentation** - Explain your choices\n",
    "\n",
    "### Bonus Points (up to +5):\n",
    "\n",
    "- üåü **+2 points:** Achieve ‚â•93% test accuracy\n",
    "- üåü **+1 point:** Learning rate scheduling\n",
    "- üåü **+1 point:** Ensemble of multiple models\n",
    "- üåü **+1 point:** Detailed error analysis (confusion matrix, per-class metrics)\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Conv2D, MaxPooling2D, AveragePooling2D, GlobalAveragePooling2D,\n",
    "    Flatten, Dense, Dropout, BatchNormalization, Activation\n",
    ")\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import pandas as pd\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"‚úÖ TensorFlow version: {tf.__version__}\")\n",
    "print(f\"‚úÖ GPU available: {len(tf.config.list_physical_devices('GPU')) > 0}\")\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(\"\\nüéØ Challenge Started! Good luck!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Load and Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Fashion-MNIST\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "print(f\"Training samples: {x_train.shape[0]:,}\")\n",
    "print(f\"Test samples: {x_test.shape[0]:,}\")\n",
    "print(f\"Image shape: {x_train.shape[1:]}\")\n",
    "print(f\"Number of classes: {len(class_names)}\")\n",
    "\n",
    "# Visualize class distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Training set distribution\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "axes[0].bar(unique, counts, color='steelblue', alpha=0.8)\n",
    "axes[0].set_xticks(unique)\n",
    "axes[0].set_xticklabels([class_names[i][:8] for i in unique], rotation=45, ha='right')\n",
    "axes[0].set_xlabel('Class', fontsize=11)\n",
    "axes[0].set_ylabel('Count', fontsize=11)\n",
    "axes[0].set_title('Training Set Class Distribution', fontsize=13, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Sample images\n",
    "axes[1].axis('off')\n",
    "sample_grid = np.zeros((28*2, 28*5))\n",
    "for i in range(10):\n",
    "    idx = np.where(y_train == i)[0][0]\n",
    "    row = i // 5\n",
    "    col = i % 5\n",
    "    sample_grid[row*28:(row+1)*28, col*28:(col+1)*28] = x_train[idx]\n",
    "axes[1].imshow(sample_grid, cmap='gray')\n",
    "axes[1].set_title('Sample Images (One per Class)', fontsize=13, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Dataset is balanced - all classes have similar counts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Data Preprocessing\n",
    "\n",
    "**TODO:** Implement your preprocessing strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# YOUR CODE HERE: Preprocess the data\n",
    "# ==========================================\n",
    "\n",
    "# Reshape to add channel dimension\n",
    "x_train = x_train.reshape(-1, 28, 28, 1).astype('float32')\n",
    "x_test = x_test.reshape(-1, 28, 28, 1).astype('float32')\n",
    "\n",
    "# Normalize pixel values to [0, 1]\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "\n",
    "# One-hot encode labels\n",
    "y_train_cat = to_categorical(y_train, 10)\n",
    "y_test_cat = to_categorical(y_test, 10)\n",
    "\n",
    "# Split training data for validation\n",
    "validation_split = 0.1\n",
    "split_idx = int((1 - validation_split) * len(x_train))\n",
    "\n",
    "x_train_split = x_train[:split_idx]\n",
    "y_train_split = y_train_cat[:split_idx]\n",
    "x_val_split = x_train[split_idx:]\n",
    "y_val_split = y_train_cat[split_idx:]\n",
    "\n",
    "print(f\"‚úÖ Preprocessing complete\")\n",
    "print(f\"Training: {x_train_split.shape[0]:,} samples\")\n",
    "print(f\"Validation: {x_val_split.shape[0]:,} samples\")\n",
    "print(f\"Test: {x_test.shape[0]:,} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Data Augmentation Strategy\n",
    "\n",
    "**TODO:** Design your data augmentation pipeline\n",
    "\n",
    "**Questions to answer:**\n",
    "1. Which augmentations are appropriate for Fashion-MNIST?\n",
    "2. What are good parameter ranges?\n",
    "3. Why did you choose these specific transformations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# YOUR CODE HERE: Configure data augmentation\n",
    "# ==========================================\n",
    "\n",
    "# Training data generator with augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=15,          # TODO: Adjust this\n",
    "    width_shift_range=0.1,      # TODO: Adjust this\n",
    "    height_shift_range=0.1,     # TODO: Adjust this\n",
    "    zoom_range=0.1,             # TODO: Adjust this\n",
    "    horizontal_flip=True,       # TODO: Keep or remove?\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Validation data generator (no augmentation!)\n",
    "val_datagen = ImageDataGenerator()\n",
    "\n",
    "# Create generators\n",
    "train_generator = train_datagen.flow(x_train_split, y_train_split, batch_size=128)\n",
    "val_generator = val_datagen.flow(x_val_split, y_val_split, batch_size=128)\n",
    "\n",
    "print(\"‚úÖ Data augmentation configured\")\n",
    "\n",
    "# Visualize augmented samples\n",
    "sample_img = x_train[0:1]\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "axes = axes.ravel()\n",
    "\n",
    "axes[0].imshow(sample_img[0].reshape(28, 28), cmap='gray')\n",
    "axes[0].set_title('Original', fontsize=11, fontweight='bold')\n",
    "axes[0].axis('off')\n",
    "\n",
    "aug_iter = train_datagen.flow(sample_img, batch_size=1)\n",
    "for i in range(1, 10):\n",
    "    aug_img = next(aug_iter)[0]\n",
    "    axes[i].imshow(aug_img.reshape(28, 28), cmap='gray')\n",
    "    axes[i].set_title(f'Augmented {i}', fontsize=10)\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle('Your Data Augmentation Examples', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìù Document Your Augmentation Choices\n",
    "\n",
    "**‚úçÔ∏è Answer these questions in the cell below:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR ANSWERS:\n",
    "\n",
    "**1. Which augmentation techniques did you use and why?**\n",
    "\n",
    "[Write your answer here]\n",
    "\n",
    "**2. What parameter ranges did you choose? Why?**\n",
    "\n",
    "[Write your answer here]\n",
    "\n",
    "**3. Are there any augmentations you specifically avoided? Why?**\n",
    "\n",
    "[Write your answer here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Build Your CNN Architecture\n",
    "\n",
    "**TODO:** Design your best CNN architecture\n",
    "\n",
    "**Design considerations:**\n",
    "- How many convolutional blocks?\n",
    "- Filter sizes and counts?\n",
    "- Pooling strategy?\n",
    "- Regularization techniques?\n",
    "- Dense layer configuration?\n",
    "\n",
    "**Recommended pattern:**\n",
    "```\n",
    "Conv ‚Üí BatchNorm ‚Üí ReLU ‚Üí Conv ‚Üí BatchNorm ‚Üí ReLU ‚Üí MaxPool ‚Üí Dropout\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# YOUR CODE HERE: Build your CNN model\n",
    "# ==========================================\n",
    "\n",
    "# Example architecture (you should improve this!)\n",
    "model = Sequential([\n",
    "    # Block 1\n",
    "    Conv2D(64, (3, 3), padding='same', input_shape=(28, 28, 1)),\n",
    "    BatchNormalization(),\n",
    "    Activation('relu'),\n",
    "    Conv2D(64, (3, 3), padding='same'),\n",
    "    BatchNormalization(),\n",
    "    Activation('relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Dropout(0.25),\n",
    "    \n",
    "    # Block 2\n",
    "    Conv2D(128, (3, 3), padding='same'),\n",
    "    BatchNormalization(),\n",
    "    Activation('relu'),\n",
    "    Conv2D(128, (3, 3), padding='same'),\n",
    "    BatchNormalization(),\n",
    "    Activation('relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Dropout(0.25),\n",
    "    \n",
    "    # Block 3 - Add more blocks if needed\n",
    "    Conv2D(256, (3, 3), padding='same'),\n",
    "    BatchNormalization(),\n",
    "    Activation('relu'),\n",
    "    Conv2D(256, (3, 3), padding='same'),\n",
    "    BatchNormalization(),\n",
    "    Activation('relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Dropout(0.25),\n",
    "    \n",
    "    # Classifier\n",
    "    GlobalAveragePooling2D(),\n",
    "    Dense(512),\n",
    "    BatchNormalization(),\n",
    "    Activation('relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(10, activation='softmax')\n",
    "], name='My_Optimized_CNN')\n",
    "\n",
    "# Display architecture\n",
    "model.summary()\n",
    "\n",
    "print(f\"\\nüìä Total parameters: {model.count_params():,}\")\n",
    "print(\"\\nüí° TIP: Try to keep parameters under 5 million for efficiency!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìù Document Your Architecture Choices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR ANSWERS:\n",
    "\n",
    "**1. Describe your CNN architecture. How many blocks? What's the filter progression?**\n",
    "\n",
    "[Write your answer here]\n",
    "\n",
    "**2. Which regularization techniques did you use and where?**\n",
    "\n",
    "[Write your answer here]\n",
    "\n",
    "**3. Why did you choose GlobalAveragePooling vs Flatten? (or explain your choice)**\n",
    "\n",
    "[Write your answer here]\n",
    "\n",
    "**4. What design principles guided your architecture?**\n",
    "\n",
    "[Write your answer here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Configure Training Strategy\n",
    "\n",
    "**TODO:** Set up optimizer, callbacks, and training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# YOUR CODE HERE: Configure training\n",
    "# ==========================================\n",
    "\n",
    "# Compile model\n",
    "model.compile(\n",
    "    optimizer='adam',  # TODO: Try different optimizers or learning rates\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Callbacks\n",
    "callbacks = [\n",
    "    # Early stopping\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=10,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    \n",
    "    # Learning rate reduction (BONUS POINT!)\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    ),\n",
    "    \n",
    "    # Save best model\n",
    "    ModelCheckpoint(\n",
    "        'best_model.h5',\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"‚úÖ Training configuration complete\")\n",
    "print(\"\\nüìã Callbacks configured:\")\n",
    "print(\"  ‚Ä¢ Early Stopping (patience=10)\")\n",
    "print(\"  ‚Ä¢ Learning Rate Reduction (patience=5)\")\n",
    "print(\"  ‚Ä¢ Model Checkpoint (save best)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Train Your Model\n",
    "\n",
    "**This is it! Train your best model!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# TRAIN YOUR MODEL\n",
    "# ==========================================\n",
    "\n",
    "print(\"üöÄ Training started...\\n\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=len(x_train_split) // 128,\n",
    "    epochs=50,  # Early stopping will terminate if needed\n",
    "    validation_data=val_generator,\n",
    "    validation_steps=len(x_val_split) // 128,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(f\"‚úÖ Training complete! Time: {training_time/60:.1f} minutes\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Visualize Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# Accuracy\n",
    "axes[0].plot(history.history['accuracy'], 'b-o', label='Training', linewidth=2, markersize=4)\n",
    "axes[0].plot(history.history['val_accuracy'], 'r-o', label='Validation', linewidth=2, markersize=4)\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Accuracy', fontsize=12)\n",
    "axes[0].set_title('Model Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Loss\n",
    "axes[1].plot(history.history['loss'], 'b-o', label='Training', linewidth=2, markersize=4)\n",
    "axes[1].plot(history.history['val_loss'], 'r-o', label='Validation', linewidth=2, markersize=4)\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('Loss', fontsize=12)\n",
    "axes[1].set_title('Model Loss', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Training Performance', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Training summary\n",
    "final_train_acc = history.history['accuracy'][-1]\n",
    "final_val_acc = history.history['val_accuracy'][-1]\n",
    "best_val_acc = max(history.history['val_accuracy'])\n",
    "epochs_trained = len(history.history['accuracy'])\n",
    "\n",
    "print(f\"\\nüìä Training Summary:\")\n",
    "print(f\"Epochs trained: {epochs_trained}\")\n",
    "print(f\"Final training accuracy: {final_train_acc:.2%}\")\n",
    "print(f\"Final validation accuracy: {final_val_acc:.2%}\")\n",
    "print(f\"Best validation accuracy: {best_val_acc:.2%}\")\n",
    "print(f\"Overfitting gap: {final_train_acc - final_val_acc:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 8: Evaluate on Test Set\n",
    "\n",
    "**The moment of truth!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "print(\"üéØ Evaluating on test set...\\n\")\n",
    "\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test_cat, verbose=0)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"FINAL TEST RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Test Loss:     {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.2%}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Grade based on accuracy\n",
    "if test_acc >= 0.93:\n",
    "    grade = \"‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê EXCELLENT\"\n",
    "    print(f\"\\nüèÜ {grade}! Outstanding performance!\")\n",
    "elif test_acc >= 0.91:\n",
    "    grade = \"‚≠ê‚≠ê‚≠ê‚≠ê VERY GOOD\"\n",
    "    print(f\"\\nüëç {grade}! Strong CNN design!\")\n",
    "elif test_acc >= 0.89:\n",
    "    grade = \"‚≠ê‚≠ê‚≠ê GOOD\"\n",
    "    print(f\"\\n‚úÖ {grade}! Solid understanding!\")\n",
    "elif test_acc >= 0.87:\n",
    "    grade = \"‚≠ê‚≠ê ACCEPTABLE\"\n",
    "    print(f\"\\n‚úì {grade}. Basic proficiency shown.\")\n",
    "else:\n",
    "    grade = \"‚≠ê NEEDS WORK\"\n",
    "    print(f\"\\n‚ö†Ô∏è {grade}. Try experimenting more!\")\n",
    "\n",
    "print(f\"\\nYour Grade: {grade}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 9: Detailed Error Analysis (BONUS +1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "predictions = model.predict(x_test, verbose=0)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Classification report\n",
    "print(\"üìã Classification Report:\\n\")\n",
    "print(classification_report(y_test, predicted_classes, target_names=class_names, digits=4))\n",
    "\n",
    "# Per-class accuracy\n",
    "per_class_acc = []\n",
    "for i in range(10):\n",
    "    mask = y_test == i\n",
    "    acc = (predicted_classes[mask] == y_test[mask]).mean()\n",
    "    per_class_acc.append(acc)\n",
    "\n",
    "# Plot per-class accuracy\n",
    "plt.figure(figsize=(12, 5))\n",
    "bars = plt.bar(range(10), per_class_acc, color='steelblue', alpha=0.8)\n",
    "plt.xticks(range(10), [class_names[i] for i in range(10)], rotation=45, ha='right')\n",
    "plt.xlabel('Class', fontsize=12)\n",
    "plt.ylabel('Accuracy', fontsize=12)\n",
    "plt.title('Per-Class Accuracy', fontsize=14, fontweight='bold')\n",
    "plt.ylim([0.8, 1.0])\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, (bar, acc) in enumerate(zip(bars, per_class_acc)):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005,\n",
    "            f'{acc:.1%}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Identify best and worst classes\n",
    "best_class = np.argmax(per_class_acc)\n",
    "worst_class = np.argmin(per_class_acc)\n",
    "\n",
    "print(f\"\\nüéØ Best performing class: {class_names[best_class]} ({per_class_acc[best_class]:.2%})\")\n",
    "print(f\"‚ö†Ô∏è Worst performing class: {class_names[worst_class]} ({per_class_acc[worst_class]:.2%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, predicted_classes)\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=class_names, yticklabels=class_names,\n",
    "            cbar_kws={'label': 'Count'})\n",
    "plt.xlabel('Predicted Label', fontsize=13, fontweight='bold')\n",
    "plt.ylabel('True Label', fontsize=13, fontweight='bold')\n",
    "plt.title('Confusion Matrix', fontsize=16, fontweight='bold')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find most confused pairs\n",
    "cm_no_diag = cm.copy()\n",
    "np.fill_diagonal(cm_no_diag, 0)\n",
    "most_confused_idx = np.unravel_index(cm_no_diag.argmax(), cm_no_diag.shape)\n",
    "true_class = class_names[most_confused_idx[0]]\n",
    "pred_class = class_names[most_confused_idx[1]]\n",
    "confusion_count = cm_no_diag[most_confused_idx]\n",
    "\n",
    "print(f\"\\nüîç Most confused pair: {true_class} misclassified as {pred_class} ({confusion_count} times)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize correct and incorrect predictions\n",
    "correct_idx = np.where(predicted_classes == y_test)[0]\n",
    "incorrect_idx = np.where(predicted_classes != y_test)[0]\n",
    "\n",
    "fig, axes = plt.subplots(2, 5, figsize=(16, 7))\n",
    "\n",
    "# Correct predictions\n",
    "for i in range(5):\n",
    "    idx = correct_idx[i]\n",
    "    axes[0, i].imshow(x_test[idx].reshape(28, 28), cmap='gray')\n",
    "    axes[0, i].set_title(f'‚úÖ True: {class_names[y_test[idx]]}\\nPred: {class_names[predicted_classes[idx]]}\\nConf: {predictions[idx][predicted_classes[idx]]:.2%}',\n",
    "                        fontsize=9, color='green')\n",
    "    axes[0, i].axis('off')\n",
    "\n",
    "# Incorrect predictions\n",
    "for i in range(5):\n",
    "    idx = incorrect_idx[i]\n",
    "    axes[1, i].imshow(x_test[idx].reshape(28, 28), cmap='gray')\n",
    "    axes[1, i].set_title(f'‚ùå True: {class_names[y_test[idx]]}\\nPred: {class_names[predicted_classes[idx]]}\\nConf: {predictions[idx][predicted_classes[idx]]:.2%}',\n",
    "                        fontsize=9, color='red')\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "plt.suptitle('Sample Predictions: Correct (Top) vs Incorrect (Bottom)', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nTotal correct: {len(correct_idx):,} ({len(correct_idx)/len(y_test):.2%})\")\n",
    "print(f\"Total incorrect: {len(incorrect_idx):,} ({len(incorrect_idx)/len(y_test):.2%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìù Error Analysis Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR ANSWERS:\n",
    "\n",
    "**1. Which class(es) did your model struggle with most? Why do you think this happened?**\n",
    "\n",
    "[Write your answer here]\n",
    "\n",
    "**2. Looking at the confusion matrix, which classes are most commonly confused with each other?**\n",
    "\n",
    "[Write your answer here]\n",
    "\n",
    "**3. What could you do to improve performance on the worst-performing classes?**\n",
    "\n",
    "[Write your answer here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 10: Final Summary and Reflection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive summary\n",
    "summary_data = {\n",
    "    'Metric': [\n",
    "        'Test Accuracy',\n",
    "        'Test Loss',\n",
    "        'Total Parameters',\n",
    "        'Epochs Trained',\n",
    "        'Training Time',\n",
    "        'Best Validation Accuracy',\n",
    "        'Overfitting Gap'\n",
    "    ],\n",
    "    'Value': [\n",
    "        f\"{test_acc:.2%}\",\n",
    "        f\"{test_loss:.4f}\",\n",
    "        f\"{model.count_params():,}\",\n",
    "        f\"{epochs_trained}\",\n",
    "        f\"{training_time/60:.1f} min\",\n",
    "        f\"{best_val_acc:.2%}\",\n",
    "        f\"{final_train_acc - final_val_acc:.2%}\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_summary = pd.DataFrame(summary_data)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL PROJECT SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(df_summary.to_string(index=False))\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nüéØ Final Grade: {grade}\")\n",
    "print(f\"\\nüí° Estimated Score: {20 + (2 if test_acc >= 0.93 else 0)} / 25 points\")\n",
    "if test_acc >= 0.93:\n",
    "    print(\"   (Base: 20 points + Bonus: 2 points for ‚â•93% accuracy)\")\n",
    "else:\n",
    "    print(\"   (Can earn up to +5 bonus points with additional features!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìù Final Reflection (Required)\n",
    "\n",
    "**Answer ALL questions below:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR FINAL REFLECTION:\n",
    "\n",
    "**1. What was your overall strategy for achieving high accuracy?**\n",
    "\n",
    "[Write a detailed answer - at least 3-4 sentences]\n",
    "\n",
    "**2. Which technique(s) had the biggest impact on performance? (Architecture, regularization, augmentation, etc.)**\n",
    "\n",
    "[Write your answer]\n",
    "\n",
    "**3. What challenges did you face? How did you overcome them?**\n",
    "\n",
    "[Write your answer]\n",
    "\n",
    "**4. If you had more time/resources, what would you try next to improve the model?**\n",
    "\n",
    "[Write your answer]\n",
    "\n",
    "**5. What are the 3 most important lessons you learned from this CNN project?**\n",
    "\n",
    "1. [Lesson 1]\n",
    "2. [Lesson 2]\n",
    "3. [Lesson 3]\n",
    "\n",
    "**6. How would you apply this knowledge to a real-world computer vision problem?**\n",
    "\n",
    "[Write your answer]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üì§ Submission Checklist\n",
    "\n",
    "Before submitting, verify:\n",
    "\n",
    "### Mandatory Requirements:\n",
    "- [ ] All code cells run without errors\n",
    "- [ ] Model achieves >87% test accuracy\n",
    "- [ ] Data augmentation implemented and explained\n",
    "- [ ] Architecture documented with justification\n",
    "- [ ] Training curves displayed\n",
    "- [ ] Test set evaluation complete\n",
    "- [ ] All reflection questions answered\n",
    "\n",
    "### Optional (for bonus points):\n",
    "- [ ] Learning rate scheduling used (+1 point)\n",
    "- [ ] Detailed error analysis with confusion matrix (+1 point)\n",
    "- [ ] Test accuracy ‚â•93% (+2 points)\n",
    "- [ ] Ensemble model attempted (+1 point)\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Grading Rubric (25 points total)\n",
    "\n",
    "| Component | Points | Criteria |\n",
    "|-----------|--------|----------|\n",
    "| **Architecture Design** | 5 | Well-designed CNN with proper justification |\n",
    "| **Data Augmentation** | 4 | Appropriate augmentation with explanation |\n",
    "| **Regularization** | 4 | Multiple techniques used effectively |\n",
    "| **Test Accuracy** | 6 | 87%:2pts, 89%:4pts, 91%:6pts |\n",
    "| **Documentation** | 4 | Clear explanations and reflections |\n",
    "| **Code Quality** | 2 | Clean, well-commented code |\n",
    "| **BONUS** | +5 | Extra features and high performance |\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Submission Instructions\n",
    "\n",
    "1. **Save this notebook** with all outputs visible\n",
    "2. **Rename** to: `YourName_CNN_Challenge.ipynb`\n",
    "3. **Upload** to Google Classroom / Course Portal\n",
    "4. **Include** the saved model file (`best_model.h5`) if required\n",
    "\n",
    "---\n",
    "\n",
    "## üéâ Congratulations!\n",
    "\n",
    "You've completed the CNN practical challenge! You've learned:\n",
    "\n",
    "‚úÖ 1D and 2D convolution fundamentals  \n",
    "‚úÖ CNN architecture design principles  \n",
    "‚úÖ Regularization techniques (Dropout, BatchNorm)  \n",
    "‚úÖ Data augmentation strategies  \n",
    "‚úÖ Training optimization and callbacks  \n",
    "‚úÖ Model evaluation and error analysis  \n",
    "\n",
    "**These skills are fundamental to modern computer vision and deep learning!**\n",
    "\n",
    "---\n",
    "\n",
    "*‚è±Ô∏è Expected time: 3-4 hours*  \n",
    "*üí™ Difficulty: Advanced*  \n",
    "*üéì Value: 25 points (major assignment)*  \n",
    "*üèÜ Challenge level: Production-ready CNN skills*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
