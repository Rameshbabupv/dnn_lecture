{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 3: Building Your First CNN\n",
    "\n",
    "**Course:** 21CSE558T - Deep Neural Network Architectures  \n",
    "**Module 4:** CNNs - Practical Session  \n",
    "**Date:** Saturday, November 1, 2025  \n",
    "**Duration:** 40 minutes  \n",
    "**Objective:** Build, train, and understand a complete CNN on Fashion-MNIST\n",
    "\n",
    "---\n",
    "\n",
    "## What We'll Build\n",
    "\n",
    "**A simple but effective CNN:**\n",
    "```\n",
    "Input (28×28×1)\n",
    "    ↓\n",
    "Conv2D(32 filters, 3×3) + ReLU\n",
    "    ↓\n",
    "MaxPooling(2×2)\n",
    "    ↓\n",
    "Conv2D(64 filters, 3×3) + ReLU\n",
    "    ↓\n",
    "MaxPooling(2×2)\n",
    "    ↓\n",
    "Flatten\n",
    "    ↓\n",
    "Dense(128) + ReLU\n",
    "    ↓\n",
    "Dense(10) + Softmax\n",
    "```\n",
    "\n",
    "**Expected accuracy:** ~90% (much better than MLP!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"✅ TensorFlow version: {tf.__version__}\")\n",
    "print(f\"✅ GPU available: {len(tf.config.list_physical_devices('GPU')) > 0}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Fashion-MNIST\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "print(f\"Training samples: {x_train.shape[0]:,}\")\n",
    "print(f\"Test samples: {x_test.shape[0]:,}\")\n",
    "print(f\"Image shape: {x_train.shape[1:]}\")\n",
    "print(f\"Number of classes: {len(class_names)}\")\n",
    "\n",
    "# Visualize samples\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(x_train[i], cmap='gray')\n",
    "    ax.set_title(f'{class_names[y_train[i]]}', fontsize=12, fontweight='bold')\n",
    "    ax.axis('off')\n",
    "plt.suptitle('Fashion-MNIST Dataset', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Reshape to add channel dimension (H, W, C)\n",
    "x_train = x_train.reshape(-1, 28, 28, 1)\n",
    "x_test = x_test.reshape(-1, 28, 28, 1)\n",
    "\n",
    "# 2. Normalize to [0, 1]\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "\n",
    "# 3. One-hot encode labels\n",
    "y_train_cat = to_categorical(y_train, 10)\n",
    "y_test_cat = to_categorical(y_test, 10)\n",
    "\n",
    "print(f\"\\n✅ Preprocessing complete!\")\n",
    "print(f\"Training data shape: {x_train.shape}\")\n",
    "print(f\"Training labels shape: {y_train_cat.shape}\")\n",
    "print(f\"Pixel value range: [{x_train.min():.2f}, {x_train.max():.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Build CNN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build CNN model\n",
    "model = Sequential([\n",
    "    # First convolutional block\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1), name='conv1'),\n",
    "    MaxPooling2D((2, 2), name='pool1'),\n",
    "    \n",
    "    # Second convolutional block\n",
    "    Conv2D(64, (3, 3), activation='relu', name='conv2'),\n",
    "    MaxPooling2D((2, 2), name='pool2'),\n",
    "    \n",
    "    # Fully connected layers\n",
    "    Flatten(name='flatten'),\n",
    "    Dense(128, activation='relu', name='fc1'),\n",
    "    Dense(10, activation='softmax', name='output')\n",
    "], name='SimpleCNN')\n",
    "\n",
    "# Display architecture\n",
    "model.summary()\n",
    "\n",
    "# Count parameters\n",
    "total_params = model.count_params()\n",
    "print(f\"\\n📊 Total parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Architecture\n",
    "\n",
    "**Layer-by-layer breakdown:**\n",
    "\n",
    "1. **Conv2D(32, 3×3):** 32 filters of size 3×3\n",
    "   - Input: 28×28×1\n",
    "   - Output: 26×26×32 (each filter creates one feature map)\n",
    "   - Parameters: (3×3×1 + 1 bias) × 32 = 320\n",
    "\n",
    "2. **MaxPooling(2×2):** Downsample by taking maximum in 2×2 windows\n",
    "   - Input: 26×26×32\n",
    "   - Output: 13×13×32\n",
    "   - Parameters: 0 (no learning)\n",
    "\n",
    "3. **Conv2D(64, 3×3):** 64 filters\n",
    "   - Input: 13×13×32\n",
    "   - Output: 11×11×64\n",
    "   - Parameters: (3×3×32 + 1) × 64 = 18,496\n",
    "\n",
    "4. **MaxPooling(2×2):**\n",
    "   - Input: 11×11×64\n",
    "   - Output: 5×5×64\n",
    "\n",
    "5. **Flatten:** Convert to 1D vector\n",
    "   - Input: 5×5×64\n",
    "   - Output: 1600\n",
    "\n",
    "6. **Dense(128):** Fully connected\n",
    "   - Parameters: 1600×128 + 128 = 204,928\n",
    "\n",
    "7. **Dense(10):** Output layer\n",
    "   - Parameters: 128×10 + 10 = 1,290"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Compile Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"✅ Model compiled!\")\n",
    "print(\"\\nConfiguration:\")\n",
    "print(f\"  Optimizer: Adam\")\n",
    "print(f\"  Loss: Categorical Crossentropy\")\n",
    "print(f\"  Metrics: Accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "print(\"🚀 Starting training...\\n\")\n",
    "\n",
    "history = model.fit(\n",
    "    x_train, y_train_cat,\n",
    "    batch_size=128,\n",
    "    epochs=10,\n",
    "    validation_split=0.1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n✅ Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Visualize Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Accuracy\n",
    "axes[0].plot(history.history['accuracy'], 'b-o', label='Training', linewidth=2)\n",
    "axes[0].plot(history.history['val_accuracy'], 'r-o', label='Validation', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Accuracy', fontsize=12)\n",
    "axes[0].set_title('Model Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Loss\n",
    "axes[1].plot(history.history['loss'], 'b-o', label='Training', linewidth=2)\n",
    "axes[1].plot(history.history['val_loss'], 'r-o', label='Validation', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('Loss', fontsize=12)\n",
    "axes[1].set_title('Model Loss', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Training Performance', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print final metrics\n",
    "final_train_acc = history.history['accuracy'][-1]\n",
    "final_val_acc = history.history['val_accuracy'][-1]\n",
    "print(f\"\\n📊 Final Results:\")\n",
    "print(f\"Training Accuracy:   {final_train_acc:.2%}\")\n",
    "print(f\"Validation Accuracy: {final_val_acc:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test data\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test_cat, verbose=0)\n",
    "\n",
    "print(f\"\\n🎯 Test Set Performance:\")\n",
    "print(f\"Test Loss:     {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.2%}\")\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(x_test, verbose=0)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Calculate per-class accuracy\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print(f\"\\n📋 Classification Report:\\n\")\n",
    "print(classification_report(y_test, predicted_classes, target_names=class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_test, predicted_classes)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=class_names, yticklabels=class_names,\n",
    "            cbar_kws={'label': 'Count'})\n",
    "plt.xlabel('Predicted Label', fontsize=13, fontweight='bold')\n",
    "plt.ylabel('True Label', fontsize=13, fontweight='bold')\n",
    "plt.title('Confusion Matrix - CNN on Fashion-MNIST', fontsize=16, fontweight='bold')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n💡 Confusion Matrix Insights:\")\n",
    "print(\"  • Diagonal = Correct predictions\")\n",
    "print(\"  • Off-diagonal = Misclassifications\")\n",
    "print(\"  • Look for patterns: Which classes confuse the model?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 8: Visualize Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show correct and incorrect predictions\n",
    "correct_idx = np.where(predicted_classes == y_test)[0]\n",
    "incorrect_idx = np.where(predicted_classes != y_test)[0]\n",
    "\n",
    "# Sample predictions\n",
    "fig, axes = plt.subplots(2, 5, figsize=(16, 7))\n",
    "\n",
    "# Correct predictions (top row)\n",
    "for i, ax in enumerate(axes[0]):\n",
    "    idx = correct_idx[i]\n",
    "    ax.imshow(x_test[idx].reshape(28, 28), cmap='gray')\n",
    "    ax.set_title(f'✅ True: {class_names[y_test[idx]]}\\nPred: {class_names[predicted_classes[idx]]}\\nConf: {predictions[idx][predicted_classes[idx]]:.2%}',\n",
    "                fontsize=10, color='green')\n",
    "    ax.axis('off')\n",
    "\n",
    "# Incorrect predictions (bottom row)\n",
    "for i, ax in enumerate(axes[1]):\n",
    "    idx = incorrect_idx[i]\n",
    "    ax.imshow(x_test[idx].reshape(28, 28), cmap='gray')\n",
    "    ax.set_title(f'❌ True: {class_names[y_test[idx]]}\\nPred: {class_names[predicted_classes[idx]]}\\nConf: {predictions[idx][predicted_classes[idx]]:.2%}',\n",
    "                fontsize=10, color='red')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('CNN Predictions: Correct (Top) vs Incorrect (Bottom)', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 9: Compare with MLP (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build simple MLP for comparison\n",
    "mlp_model = Sequential([\n",
    "    Flatten(input_shape=(28, 28, 1)),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "], name='SimpleMLP')\n",
    "\n",
    "mlp_model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "print(\"🔄 Training MLP baseline...\\n\")\n",
    "mlp_history = mlp_model.fit(x_train, y_train_cat,\n",
    "                            batch_size=128,\n",
    "                            epochs=10,\n",
    "                            validation_split=0.1,\n",
    "                            verbose=0)\n",
    "\n",
    "# Evaluate\n",
    "mlp_test_loss, mlp_test_acc = mlp_model.evaluate(x_test, y_test_cat, verbose=0)\n",
    "\n",
    "# Compare\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Accuracy comparison\n",
    "axes[0].plot(history.history['val_accuracy'], 'b-o', label='CNN', linewidth=2)\n",
    "axes[0].plot(mlp_history.history['val_accuracy'], 'r-o', label='MLP', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Validation Accuracy', fontsize=12)\n",
    "axes[0].set_title('CNN vs MLP: Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Final test accuracy bar chart\n",
    "axes[1].bar(['CNN', 'MLP'], [test_acc, mlp_test_acc], color=['blue', 'red'], alpha=0.7)\n",
    "axes[1].set_ylabel('Test Accuracy', fontsize=12)\n",
    "axes[1].set_title('Final Test Accuracy Comparison', fontsize=14, fontweight='bold')\n",
    "axes[1].set_ylim([0.8, 0.95])\n",
    "for i, v in enumerate([test_acc, mlp_test_acc]):\n",
    "    axes[1].text(i, v + 0.005, f'{v:.2%}', ha='center', fontsize=13, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n📊 Comparison:\")\n",
    "print(f\"CNN Test Accuracy: {test_acc:.2%}\")\n",
    "print(f\"MLP Test Accuracy: {mlp_test_acc:.2%}\")\n",
    "print(f\"Improvement:       +{(test_acc - mlp_test_acc):.2%}\")\n",
    "print(f\"\\n💡 CNN Parameters: {model.count_params():,}\")\n",
    "print(f\"💡 MLP Parameters: {mlp_model.count_params():,}\")\n",
    "print(f\"\\n✅ CNN is better with FEWER parameters!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary: Key Takeaways 🎯\n",
    "\n",
    "### What You Built:\n",
    "\n",
    "1. **✅ Complete CNN** with 2 conv blocks + 2 dense layers\n",
    "2. **✅ ~90% accuracy** on Fashion-MNIST\n",
    "3. **✅ Better than MLP** with fewer parameters\n",
    "\n",
    "### Why CNN Works Better:\n",
    "\n",
    "- **Spatial structure:** Conv layers preserve 2D relationships\n",
    "- **Local patterns:** Filters detect edges, textures, shapes\n",
    "- **Translation invariance:** Same filter slides across entire image\n",
    "- **Parameter sharing:** Same weights used at all positions\n",
    "- **Hierarchical features:** Low-level → High-level patterns\n",
    "\n",
    "### Architecture Insights:\n",
    "\n",
    "1. **Conv → Pool pattern:** Standard building block\n",
    "2. **Increasing filters:** 32 → 64 (more complex patterns in deeper layers)\n",
    "3. **Decreasing spatial size:** 28×28 → 13×13 → 5×5 (via pooling)\n",
    "4. **MaxPooling:** Downsamples and adds translation invariance\n",
    "5. **ReLU activation:** Enables learning complex patterns\n",
    "\n",
    "---\n",
    "\n",
    "## Practice Exercises 📝\n",
    "\n",
    "**Before Notebook 4:**\n",
    "\n",
    "1. **Experiment:** Change the number of filters in conv layers. What happens?\n",
    "\n",
    "2. **Experiment:** Add a third conv block. Does accuracy improve?\n",
    "\n",
    "3. **Calculate:** What's the output size after Conv2D(64, 5×5) on a 28×28 input?\n",
    "\n",
    "4. **Challenge:** Can you achieve >91% accuracy by modifying the architecture?\n",
    "\n",
    "---\n",
    "\n",
    "## Next: Notebook 4 - Architecture Experiments 🔬\n",
    "\n",
    "**Your homework!** Systematically experiment with different architectures.\n",
    "\n",
    "---\n",
    "\n",
    "*⏱️ Time spent: ~40 minutes*  \n",
    "*💪 Difficulty: Intermediate*  \n",
    "*🎓 Mastery: Complete CNN implementation*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
