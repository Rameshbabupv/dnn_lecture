# Famous CNN Architectures - Quick Reference Cheat Sheet

**Course:** 21CSE558T - Deep Neural Network Architectures
**Module 4:** CNNs (Week 2 of 3)
**Date:** November 1, 2025

---

## ğŸ›ï¸ The Big Three: LeNet â†’ AlexNet â†’ VGG

| Architecture | Year | Layers | Parameters | Input Size | Key Innovation |
|--------------|------|--------|------------|------------|----------------|
| **LeNet-5** | 1998 | 7 | 60K | 32Ã—32Ã—1 | First practical CNN |
| **AlexNet** | 2012 | 8 | 60M | 224Ã—224Ã—3 | ReLU, Dropout, GPU |
| **VGG-16** | 2014 | 16 | 138M | 224Ã—224Ã—3 | 3Ã—3 everywhere, Depth |

---

## ğŸ“Š Architecture Comparison at a Glance

### LeNet-5 (1998) - The Pioneer

**Structure:**
```
Input (32Ã—32Ã—1)
  â†“
Conv(6, 5Ã—5) â†’ Sigmoid â†’ AvgPool(2Ã—2)
  â†“
Conv(16, 5Ã—5) â†’ Sigmoid â†’ AvgPool(2Ã—2)
  â†“
Conv(120, 5Ã—5) â†’ Sigmoid
  â†“
FC(84) â†’ Sigmoid
  â†“
FC(10) â†’ Softmax
```

**Stats:**
- Parameters: 60,000
- FLOPs: ~0.4M
- Model size: 240 KB
- Application: Handwritten digits (MNIST)
- Accuracy: 99% on MNIST

**Key Features:**
- âœ… Proved CNNs work for vision
- âœ… Hierarchical feature learning
- âœ… End-to-end trainable
- âŒ Sigmoid (vanishing gradients)
- âŒ Too small for complex images

---

### AlexNet (2012) - The Breakthrough

**Structure:**
```
Input (224Ã—224Ã—3)
  â†“
Conv(96, 11Ã—11, s=4) â†’ ReLU â†’ MaxPool â†’ LRN
  â†“
Conv(256, 5Ã—5) â†’ ReLU â†’ MaxPool â†’ LRN
  â†“
Conv(384, 3Ã—3) â†’ ReLU
  â†“
Conv(384, 3Ã—3) â†’ ReLU
  â†“
Conv(256, 3Ã—3) â†’ ReLU â†’ MaxPool
  â†“
FC(4096) â†’ ReLU â†’ Dropout(0.5)
  â†“
FC(4096) â†’ ReLU â†’ Dropout(0.5)
  â†“
FC(1000) â†’ Softmax
```

**Stats:**
- Parameters: 60 million
- FLOPs: ~720M
- Model size: 240 MB
- Application: ImageNet (1000 classes)
- Accuracy: 84% top-5 on ImageNet

**Key Innovations:**
- âœ… **ReLU activation** (6Ã— faster training)
- âœ… **Dropout regularization** (0.5 rate)
- âœ… **GPU training** (7Ã— speedup)
- âœ… **Data augmentation** (crops, flips)
- âœ… **Max pooling** (vs average)
- âœ… **Overlapping pooling** (3Ã—3, stride=2)

**Parameter Distribution:**
- Conv layers: 3.7M (6%)
- FC layers: 58.6M (94%) â† Problem!

---

### VGG-16 (2014) - Simplicity Through Depth

**Structure:**
```
Input (224Ã—224Ã—3)
  â†“
[Conv(64, 3Ã—3) â†’ ReLU] Ã— 2 â†’ MaxPool
  â†“
[Conv(128, 3Ã—3) â†’ ReLU] Ã— 2 â†’ MaxPool
  â†“
[Conv(256, 3Ã—3) â†’ ReLU] Ã— 3 â†’ MaxPool
  â†“
[Conv(512, 3Ã—3) â†’ ReLU] Ã— 3 â†’ MaxPool
  â†“
[Conv(512, 3Ã—3) â†’ ReLU] Ã— 3 â†’ MaxPool
  â†“
FC(4096) â†’ ReLU â†’ Dropout(0.5)
  â†“
FC(4096) â†’ ReLU â†’ Dropout(0.5)
  â†“
FC(1000) â†’ Softmax
```

**Stats:**
- Parameters: 138 million
- FLOPs: ~15.5B
- Model size: 552 MB
- Application: ImageNet (1000 classes)
- Accuracy: 93% top-5 on ImageNet

**Key Innovations:**
- âœ… **Only 3Ã—3 filters** (uniformity)
- âœ… **Deeper network** (16-19 layers)
- âœ… **Simple, modular blocks**
- âœ… **Small filters stacked** = large receptive field

**Why 3Ã—3 Filters?**
```
Two 3Ã—3 filters = One 5Ã—5 filter (same receptive field)
Parameters: 18 vs 25 (28% fewer!)
Non-linearity: 2 ReLU vs 1 (more expressive)
```

**Parameter Distribution:**
- Conv layers: 14.7M (11%)
- FC layers: 123.6M (89%) â† Still a problem!

---

## ğŸ”„ Evolution Timeline

**1998 â†’ 2012 (14 years):**
- LeNet proved concept but limited
- "AI Winter" for neural networks
- Traditional CV methods dominated

**2012: The Breakthrough**
- AlexNet wins ImageNet by huge margin
- 84% accuracy (vs 74% traditional methods)
- CNNs become mainstream overnight

**2012 â†’ 2014 (2 years):**
- Rapid innovation period
- VGG, GoogLeNet, others
- Accuracy jumps to 93%

**2015+:**
- ResNet enables 100+ layer networks
- Superhuman performance achieved
- Modern architectures (Transformers, etc.)

---

## ğŸ“ Key Design Patterns

### Pattern 1: Progressive Filter Growth
```
LeNet:   6 â†’ 16 â†’ 120
AlexNet: 96 â†’ 256 â†’ 384 â†’ 384 â†’ 256
VGG:     64 â†’ 128 â†’ 256 â†’ 512 â†’ 512

Rule: Double filters when spatial size halves
```

### Pattern 2: Spatial Reduction
```
All architectures:
224Ã—224 â†’ 112Ã—112 â†’ 56Ã—56 â†’ 28Ã—28 â†’ 14Ã—14 â†’ 7Ã—7

Mechanism: Pooling (2Ã—2) halves dimensions
```

### Pattern 3: Hierarchical Features
```
Early layers:  Edges, colors, simple patterns
Middle layers: Textures, combinations
Deep layers:   Parts, objects, concepts
```

### Pattern 4: Conv â†’ Pool Blocks
```
Standard pattern (all architectures):
[Conv â†’ Activation â†’ (Conv) â†’ Pool] Ã— N â†’ FC â†’ Output
```

---

## ğŸ¯ Activation Functions Evolution

| Era | Activation | Formula | Pros | Cons |
|-----|------------|---------|------|------|
| 1998 | Sigmoid | Ïƒ(x) = 1/(1+e^(-x)) | Smooth | Vanishing gradients |
| 1998 | Tanh | tanh(x) = (e^x - e^(-x))/(e^x + e^(-x)) | Zero-centered | Vanishing gradients |
| 2012+ | ReLU | max(0, x) | Fast, no saturation | Dead neurons |

**Why ReLU Won:**
- No vanishing gradients for x > 0
- 6Ã— faster training than sigmoid
- Simple computation
- Sparse activation (biological)

---

## ğŸ’¾ Parameter & Memory Comparison

### Parameters
| Architecture | Conv Params | FC Params | Total | FC % |
|--------------|-------------|-----------|-------|------|
| LeNet-5 | 10K | 50K | 60K | 83% |
| AlexNet | 3.7M | 58.6M | 60M | 94% |
| VGG-16 | 14.7M | 123.6M | 138M | 89% |

**Key Insight:** FC layers dominate parameter count!

### Computation (FLOPs)
| Architecture | FLOPs | Relative Cost |
|--------------|-------|---------------|
| LeNet-5 | 0.4M | 1Ã— |
| AlexNet | 720M | 1,800Ã— |
| VGG-16 | 15.5B | 38,750Ã— |

### Model Size (FP32)
| Architecture | Size | Mobile-Friendly? |
|--------------|------|------------------|
| LeNet-5 | 240 KB | âœ… Yes |
| AlexNet | 240 MB | âš ï¸ Maybe |
| VGG-16 | 552 MB | âŒ No |

---

## ğŸ§® Parameter Calculation Formulas

### Convolutional Layer
```
Parameters = (K_h Ã— K_w Ã— C_in Ã— C_out) + C_out
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”¬â”€â”€â”˜
                      weights                 biases

Example: Conv(256, 3Ã—3) from 128 channels
= (3 Ã— 3 Ã— 128 Ã— 256) + 256
= 294,912 + 256
= 295,168 parameters
```

### Fully Connected Layer
```
Parameters = (input_size Ã— output_size) + output_size

Example: FC(4096) from flattened 6Ã—6Ã—256
= (9,216 Ã— 4,096) + 4,096
= 37,752,832 parameters (~38M!)
```

### Output Size (Conv/Pool)
```
Output_size = âŒŠ(Input_size - Kernel_size + 2Ã—Padding) / StrideâŒ‹ + 1

Example: 224Ã—224 input, Conv(11Ã—11, stride=4, pad=0)
= âŒŠ(224 - 11 + 0) / 4âŒ‹ + 1
= âŒŠ213 / 4âŒ‹ + 1
= 53 + 1
= 54 (Actually 55 with padding adjustments)
```

---

## ğŸ“ Receptive Field Concept

**Receptive Field:** Region of input image that affects one output pixel

### LeNet-5
```
Layer 1 (5Ã—5 conv): RF = 5Ã—5
Layer 2 (5Ã—5 conv): RF = 13Ã—13
Final: Sees most of 32Ã—32 input
```

### Why Stack Small Filters?
```
One 7Ã—7 conv:
- Receptive field: 7Ã—7
- Parameters per filter: 49

Three 3Ã—3 convs:
- Receptive field: 7Ã—7 (same!)
- Parameters per filter: 27 (45% fewer!)
- Non-linearity: 3 ReLU vs 1 (more expressive!)
```

**VGG Insight:** Small filters stacked deeply = efficiency + expressiveness

---

## ğŸš€ Historical Impact

### ImageNet Competition Results
| Year | Winner | Top-5 Error | Key Innovation |
|------|--------|-------------|----------------|
| 2010 | Traditional CV | 28.2% | Hand-crafted features |
| 2011 | Traditional CV | 25.8% | Better features |
| 2012 | **AlexNet** | **15.3%** | **Deep learning revolution** |
| 2013 | ZFNet | 11.7% | Hyperparameter tuning |
| 2014 | **VGG** | **7.3%** | **Depth + simplicity** |
| 2014 | GoogLeNet | 6.7% | Inception modules |
| 2015 | **ResNet** | **3.6%** | **Skip connections** |
| 2015 | Human | ~5% | (Baseline) |

**2015: First superhuman performance!**

---

## ğŸ”§ Regularization Techniques by Architecture

| Technique | LeNet-5 | AlexNet | VGG-16 |
|-----------|---------|---------|--------|
| Dropout | âŒ | âœ… (0.5) | âœ… (0.5) |
| Data Augmentation | âŒ | âœ… | âœ… |
| BatchNorm | âŒ | âŒ | âŒ |
| L2 Regularization | âš ï¸ | âœ… | âœ… |
| Early Stopping | âš ï¸ | âœ… | âœ… |

**Note:** BatchNorm came in 2015 (after VGG), now standard in all modern CNNs

---

## ğŸ“± When to Use Which Architecture?

### LeNet-5 (or variants)
**Use When:**
- âœ… Simple grayscale images (28Ã—28, 32Ã—32)
- âœ… 10-100 classes
- âœ… Digit recognition, simple OCR
- âœ… Learning CNN fundamentals
- âœ… CPU-only deployment

**Don't Use When:**
- âŒ Complex natural images (224Ã—224 RGB)
- âŒ 1000+ classes
- âŒ Need high accuracy

---

### AlexNet
**Use When:**
- âœ… Quick prototyping
- âœ… Baseline model comparison
- âœ… Educational purposes
- âœ… Medium-sized datasets (10K-100K)

**Don't Use When:**
- âŒ Need best accuracy (use VGG/ResNet)
- âŒ Mobile deployment (too large)
- âŒ Production systems (outdated)

**Modern Alternative:** ResNet-18

---

### VGG-16
**Use When:**
- âœ… Transfer learning (pre-trained weights)
- âœ… Feature extraction (conv blocks)
- âœ… Style transfer tasks
- âœ… High accuracy needed
- âœ… Have powerful GPU

**Don't Use When:**
- âŒ Limited computational budget
- âŒ Mobile/edge deployment
- âŒ Real-time inference needed
- âŒ Training from scratch on small data

**Modern Alternative:** ResNet-50, EfficientNet

---

## ğŸ¯ Design Principles Learned

### 1. Depth Matters
```
Shallow networks: Limited feature complexity
Deep networks: Hierarchical, rich features
Trend: 8 layers (AlexNet) â†’ 16 layers (VGG) â†’ 152 layers (ResNet)
```

### 2. Small Filters Are Better
```
Large filters (11Ã—11, 7Ã—7): Expensive, less expressive
Small filters (3Ã—3) stacked: Cheap, more expressive
Modern standard: 3Ã—3 or 1Ã—1 only
```

### 3. ReLU Revolutionized Training
```
Before (Sigmoid/Tanh): Slow, vanishing gradients
After (ReLU): 6Ã— faster, stable training
Impact: Made deep networks practical
```

### 4. FC Layers Are the Bottleneck
```
Problem: 90%+ parameters in FC layers
Solution (2015+): Global Average Pooling
Reduction: 234Ã— fewer parameters!
```

### 5. Regularization Is Essential
```
Without: Overfitting on large networks
With: Dropout + Augmentation = Generalization
Modern: + BatchNorm + Early Stopping
```

---

## ğŸ”® What Came Next? (Preview of Week 12)

### ResNet (2015) - Skip Connections
```
Problem: VGG-30, VGG-50 won't train (gradients vanish)
Solution: Residual connections (shortcuts)
Result: Can train 152+ layer networks!
```

### Modern Architectures
- **Inception/GoogLeNet:** Multi-scale features
- **MobileNet:** Efficient for mobile (< 5MB)
- **EfficientNet:** Optimal accuracy-efficiency
- **Vision Transformers:** State-of-art (2020+)

---

## ğŸ’¡ Quick Decision Guide

**Starting a new project?**
```
Is your problem similar to ImageNet?
â”œâ”€ YES â†’ Use pre-trained VGG/ResNet (transfer learning)
â””â”€ NO  â†’ Continue

Is your dataset < 10K images?
â”œâ”€ YES â†’ Must use transfer learning OR simple custom network
â””â”€ NO  â†’ Continue

Do you need mobile deployment?
â”œâ”€ YES â†’ Use MobileNet/EfficientNet
â””â”€ NO  â†’ Continue

Training from scratch?
â”œâ”€ YES â†’ Start with AlexNet-style, scale up if underfitting
â””â”€ NO  â†’ Use pre-trained ResNet-50 (best default choice)
```

---

## ğŸ“š Code Examples (Keras)

### LeNet-5 (Simplified)
```python
model = Sequential([
    Conv2D(6, (5,5), activation='relu', input_shape=(32,32,1)),
    AveragePooling2D((2,2)),
    Conv2D(16, (5,5), activation='relu'),
    AveragePooling2D((2,2)),
    Flatten(),
    Dense(120, activation='relu'),
    Dense(84, activation='relu'),
    Dense(10, activation='softmax')
])
```

### AlexNet (Simplified)
```python
model = Sequential([
    Conv2D(96, (11,11), strides=4, activation='relu', input_shape=(224,224,3)),
    MaxPooling2D((3,3), strides=2),
    Conv2D(256, (5,5), activation='relu'),
    MaxPooling2D((3,3), strides=2),
    Conv2D(384, (3,3), activation='relu'),
    Conv2D(384, (3,3), activation='relu'),
    Conv2D(256, (3,3), activation='relu'),
    MaxPooling2D((3,3), strides=2),
    Flatten(),
    Dense(4096, activation='relu'),
    Dropout(0.5),
    Dense(4096, activation='relu'),
    Dropout(0.5),
    Dense(1000, activation='softmax')
])
```

### VGG-16 (Using Keras)
```python
from tensorflow.keras.applications import VGG16

# Load pre-trained weights
model = VGG16(weights='imagenet', include_top=True)

# Or custom top for your classes
base = VGG16(weights='imagenet', include_top=False, input_shape=(224,224,3))
x = base.output
x = GlobalAveragePooling2D()(x)
x = Dense(10, activation='softmax')(x)
model = Model(inputs=base.input, outputs=x)
```

---

## ğŸ“ Study Tips

### For Understanding
1. Draw architectures on paper (block diagrams)
2. Calculate parameters for each layer
3. Trace one example through the network
4. Compare architectures side-by-side

### For Exams
- **Memorize:** Timeline (1998, 2012, 2014, 2015)
- **Understand:** Why each innovation mattered
- **Calculate:** Parameter counts, output dimensions
- **Compare:** Advantages/disadvantages of each

### For Projects
- **Start simple:** LeNet-style for simple problems
- **Use pre-trained:** VGG/ResNet for most tasks
- **Optimize later:** MobileNet/EfficientNet for deployment

---

## âš ï¸ Common Mistakes to Avoid

âŒ **Training VGG from scratch on small dataset**
- 138M parameters will overfit badly
- Use transfer learning instead

âŒ **Using sigmoid/tanh in 2025**
- Vanishing gradients, slow training
- Use ReLU or variants

âŒ **Forgetting dropout in FC layers**
- AlexNet/VGG need dropout(0.5) in FC
- Without it, overfitting guaranteed

âŒ **Not using data augmentation**
- Essential for AlexNet/VGG
- Without it, test accuracy drops 10-15%

âŒ **Comparing architectures unfairly**
- Different training epochs
- Different regularization
- Different hardware
- Always control variables!

---

## ğŸ“Š Summary Comparison Table

| Aspect | LeNet-5 | AlexNet | VGG-16 |
|--------|---------|---------|--------|
| **Best For** | Simple tasks | Prototyping | Transfer learning |
| **Strengths** | Fast, small | Proven, balanced | Simple, accurate |
| **Weaknesses** | Limited capacity | Dated | Huge, slow |
| **Modern Use** | Education | Baseline | Feature extractor |
| **Replaced By** | Custom CNNs | ResNet-18 | ResNet-50 |

---

**Print this cheat sheet and bring it to Tutorial T11 (Monday, Nov 3)!**

**Next:** Complete architecture analysis worksheet to practice these concepts ğŸš€
