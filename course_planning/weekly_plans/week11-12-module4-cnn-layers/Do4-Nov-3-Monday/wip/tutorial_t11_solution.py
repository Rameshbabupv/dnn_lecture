"""
Tutorial T11: CIFAR-10 Multiclass Classification with Modern CNN
Course: 21CSE558T - Deep Neural Network Architectures
Module 4: CNNs (Week 2 of 3)
Date: Monday, November 3, 2025
Duration: 1 hour

COMPLETE SOLUTION CODE
"""

import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.datasets import cifar10
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import (
    Conv2D, BatchNormalization, Activation,
    MaxPooling2D, GlobalAveragePooling2D,
    Dropout, Dense, Flatten
)
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.utils import to_categorical
from sklearn.metrics import confusion_matrix
import seaborn as sns
import time

print(f"TensorFlow version: {tf.__version__}")
print(f"GPU available: {len(tf.config.list_physical_devices('GPU')) > 0}")

# =============================================================================
# PART 1: LOAD AND EXPLORE CIFAR-10 DATASET
# =============================================================================

print("\n" + "="*70)
print("PART 1: LOADING CIFAR-10 DATASET")
print("="*70)

# Load CIFAR-10 dataset
(x_train, y_train), (x_test, y_test) = cifar10.load_data()

# CIFAR-10 class names
class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',
               'dog', 'frog', 'horse', 'ship', 'truck']

print(f"Training samples: {x_train.shape[0]}")
print(f"Test samples: {x_test.shape[0]}")
print(f"Image shape: {x_train.shape[1:]}")
print(f"Number of classes: {len(class_names)}")

# Visualize sample images
def visualize_samples(images, labels, class_names, n=9):
    """Visualize n sample images with labels"""
    fig, axes = plt.subplots(3, 3, figsize=(10, 10))
    axes = axes.flatten()

    for i in range(n):
        axes[i].imshow(images[i])
        axes[i].set_title(f'{class_names[labels[i][0]]}', fontsize=12)
        axes[i].axis('off')

    plt.suptitle('CIFAR-10 Sample Images', fontsize=16, fontweight='bold')
    plt.tight_layout()
    plt.show()

visualize_samples(x_train, y_train, class_names)

# =============================================================================
# PART 2: DATA PREPROCESSING
# =============================================================================

print("\n" + "="*70)
print("PART 2: DATA PREPROCESSING")
print("="*70)

# Normalize images to [0, 1]
x_train_norm = x_train.astype('float32') / 255.0
x_test_norm = x_test.astype('float32') / 255.0

# Convert labels to one-hot encoding
y_train_cat = to_categorical(y_train, 10)
y_test_cat = to_categorical(y_test, 10)

# Create validation split (10% of training data)
val_split = 0.1
val_samples = int(len(x_train_norm) * val_split)

x_val = x_train_norm[-val_samples:]
y_val = y_train_cat[-val_samples:]
x_train_final = x_train_norm[:-val_samples]
y_train_final = y_train_cat[:-val_samples]

print(f"Final training samples: {x_train_final.shape[0]}")
print(f"Validation samples: {x_val.shape[0]}")
print(f"Test samples: {x_test_norm.shape[0]}")

# =============================================================================
# PART 3: BUILD BASELINE MODEL (Week 10 Style)
# =============================================================================

print("\n" + "="*70)
print("PART 3: BUILDING BASELINE MODEL")
print("="*70)

def build_baseline_model():
    """
    Baseline CNN (Week 10 style):
    - Simple Conv â†’ Pool â†’ Conv â†’ Pool â†’ Flatten â†’ Dense
    - NO BatchNorm, NO Dropout, NO modern techniques
    """
    model = Sequential([
        # Block 1
        Conv2D(32, (3,3), activation='relu', padding='same', input_shape=(32, 32, 3)),
        MaxPooling2D((2,2)),

        # Block 2
        Conv2D(64, (3,3), activation='relu', padding='same'),
        MaxPooling2D((2,2)),

        # Flatten and Dense
        Flatten(),
        Dense(128, activation='relu'),
        Dense(10, activation='softmax')

    ], name='Baseline_CNN')

    return model

# Create and compile baseline model
model_baseline = build_baseline_model()

model_baseline.compile(
    optimizer='adam',
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

print("\nBaseline Model Architecture:")
model_baseline.summary()

baseline_params = model_baseline.count_params()
print(f"\nTotal parameters: {baseline_params:,}")

# =============================================================================
# PART 4: BUILD MODERN MODEL (Week 11 Style with Regularization)
# =============================================================================

print("\n" + "="*70)
print("PART 4: BUILDING MODERN MODEL (WITH REGULARIZATION)")
print("="*70)

def build_modern_model():
    """
    Modern CNN (Week 11 style):
    - BatchNormalization after Conv, before activation
    - Dropout after pooling
    - GlobalAveragePooling instead of Flatten
    - Progressive filter growth: 32 â†’ 64 â†’ 128
    """
    model = Sequential([
        # Block 1: 32 filters
        Conv2D(32, (3,3), padding='same', input_shape=(32, 32, 3)),
        BatchNormalization(),
        Activation('relu'),
        Conv2D(32, (3,3), padding='same'),
        BatchNormalization(),
        Activation('relu'),
        MaxPooling2D((2,2)),
        Dropout(0.2),

        # Block 2: 64 filters
        Conv2D(64, (3,3), padding='same'),
        BatchNormalization(),
        Activation('relu'),
        Conv2D(64, (3,3), padding='same'),
        BatchNormalization(),
        Activation('relu'),
        MaxPooling2D((2,2)),
        Dropout(0.3),

        # Block 3: 128 filters
        Conv2D(128, (3,3), padding='same'),
        BatchNormalization(),
        Activation('relu'),
        GlobalAveragePooling2D(),

        # Classification head
        Dropout(0.5),
        Dense(10, activation='softmax')

    ], name='Modern_CNN')

    return model

# Create and compile modern model
model_modern = build_modern_model()

model_modern.compile(
    optimizer='adam',
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

print("\nModern Model Architecture:")
model_modern.summary()

modern_params = model_modern.count_params()
print(f"\nTotal parameters: {modern_params:,}")
print(f"Parameter reduction: {baseline_params / modern_params:.1f}Ã— fewer!")

# =============================================================================
# PART 5: DATA AUGMENTATION
# =============================================================================

print("\n" + "="*70)
print("PART 5: DATA AUGMENTATION")
print("="*70)

# Create ImageDataGenerator for training with augmentation
train_datagen = ImageDataGenerator(
    rotation_range=15,
    width_shift_range=0.1,
    height_shift_range=0.1,
    horizontal_flip=True,
    zoom_range=0.1
)

# Visualize augmented images
def visualize_augmentation(datagen, images, labels, class_names, idx=0, n=9):
    """Show n augmented versions of the same image"""
    sample_image = images[idx:idx+1]  # Need batch dimension
    sample_label = class_names[np.argmax(labels[idx])]

    fig, axes = plt.subplots(3, 3, figsize=(10, 10))
    axes = axes.flatten()

    # Show original
    axes[0].imshow(sample_image[0])
    axes[0].set_title('Original', fontsize=12, fontweight='bold')
    axes[0].axis('off')

    # Generate and show augmented versions
    aug_iter = datagen.flow(sample_image, batch_size=1)
    for i in range(1, n):
        aug_image = next(aug_iter)[0]
        axes[i].imshow(aug_image)
        axes[i].set_title(f'Augmented #{i}', fontsize=12)
        axes[i].axis('off')

    plt.suptitle(f'Data Augmentation: {sample_label}', fontsize=16, fontweight='bold')
    plt.tight_layout()
    plt.show()

# Visualize augmentations
print("Visualizing data augmentation...")
visualize_augmentation(train_datagen, x_train_final, y_train_final, class_names, idx=0)

# =============================================================================
# PART 6: TRAINING
# =============================================================================

print("\n" + "="*70)
print("PART 6: TRAINING MODELS")
print("="*70)

# Create Early Stopping callback
early_stop = EarlyStopping(
    monitor='val_loss',
    patience=5,
    restore_best_weights=True,
    verbose=1
)

# Training configuration
EPOCHS = 30
BATCH_SIZE = 64

# Train baseline model (without augmentation)
print("\n" + "="*70)
print("Training Baseline Model (No Regularization)")
print("="*70)

start_time = time.time()
history_baseline = model_baseline.fit(
    x_train_final, y_train_final,
    validation_data=(x_val, y_val),
    epochs=EPOCHS,
    batch_size=BATCH_SIZE,
    callbacks=[early_stop],
    verbose=1
)
baseline_time = time.time() - start_time

print(f"\nBaseline training time: {baseline_time:.1f} seconds")

# Train modern model (with augmentation)
print("\n" + "="*70)
print("Training Modern Model (With Regularization + Augmentation)")
print("="*70)

start_time = time.time()
history_modern = model_modern.fit(
    train_datagen.flow(x_train_final, y_train_final, batch_size=BATCH_SIZE),
    validation_data=(x_val, y_val),
    epochs=EPOCHS,
    steps_per_epoch=len(x_train_final) // BATCH_SIZE,
    callbacks=[early_stop],
    verbose=1
)
modern_time = time.time() - start_time

print(f"\nModern training time: {modern_time:.1f} seconds")

# =============================================================================
# PART 7: EVALUATION AND COMPARISON
# =============================================================================

print("\n" + "="*70)
print("PART 7: EVALUATION AND COMPARISON")
print("="*70)

# Evaluate both models on test set
test_loss_baseline, test_acc_baseline = model_baseline.evaluate(x_test_norm, y_test_cat, verbose=0)
test_loss_modern, test_acc_modern = model_modern.evaluate(x_test_norm, y_test_cat, verbose=0)

# Plot training curves comparison
def plot_comparison(history1, history2, metric='accuracy'):
    """Plot training curves for two models"""
    fig, axes = plt.subplots(1, 2, figsize=(15, 5))

    metric_name = metric.capitalize()

    # Training curves
    axes[0].plot(history1.history[metric], 'b-', label='Baseline Train', linewidth=2)
    axes[0].plot(history1.history[f'val_{metric}'], 'b--', label='Baseline Val', linewidth=2)
    axes[0].plot(history2.history[metric], 'g-', label='Modern Train', linewidth=2)
    axes[0].plot(history2.history[f'val_{metric}'], 'g--', label='Modern Val', linewidth=2)
    axes[0].set_xlabel('Epoch', fontsize=12)
    axes[0].set_ylabel(metric_name, fontsize=12)
    axes[0].set_title(f'{metric_name} Comparison', fontsize=14, fontweight='bold')
    axes[0].legend(fontsize=10)
    axes[0].grid(True, alpha=0.3)

    # Final comparison bar chart
    baseline_final = history1.history[f'val_{metric}'][-1]
    modern_final = history2.history[f'val_{metric}'][-1]

    axes[1].bar(['Baseline', 'Modern'], [baseline_final, modern_final],
                color=['blue', 'green'], alpha=0.7)
    axes[1].set_ylabel(f'Final Validation {metric_name}', fontsize=12)
    axes[1].set_title(f'Final {metric_name} Comparison', fontsize=14, fontweight='bold')
    axes[1].set_ylim([0, 1])

    # Add value labels on bars
    for i, v in enumerate([baseline_final, modern_final]):
        axes[1].text(i, v + 0.02, f'{v:.2%}', ha='center', fontweight='bold')

    plt.tight_layout()
    plt.show()

print("\nPlotting accuracy comparison...")
plot_comparison(history_baseline, history_modern, 'accuracy')

print("\nPlotting loss comparison...")
plot_comparison(history_baseline, history_modern, 'loss')

# Print final comparison
def print_final_results(baseline_acc, modern_acc, baseline_time, modern_time):
    """Print formatted comparison of results"""
    print("="*70)
    print("FINAL TEST RESULTS")
    print("="*70)
    print(f"Baseline Model (No Regularization):")
    print(f"  Test Accuracy:    {baseline_acc:.2%}")
    print(f"  Training Time:    {baseline_time:.1f}s")
    print(f"  Parameters:       {baseline_params:,}")
    print()
    print(f"Modern Model (Full Regularization):")
    print(f"  Test Accuracy:    {modern_acc:.2%}")
    print(f"  Training Time:    {modern_time:.1f}s")
    print(f"  Parameters:       {modern_params:,}")
    print()
    print(f"Improvements:")
    print(f"  Accuracy:         +{(modern_acc - baseline_acc):.2%}")
    print(f"  Parameters:       {baseline_params / modern_params:.1f}Ã— fewer")
    print(f"  Time:             {modern_time / baseline_time:.1f}Ã— slower (worth it!)")
    print("="*70)

print_final_results(test_acc_baseline, test_acc_modern, baseline_time, modern_time)

# Overfitting analysis
baseline_train_acc = history_baseline.history['accuracy'][-1]
baseline_val_acc = history_baseline.history['val_accuracy'][-1]
baseline_gap = baseline_train_acc - baseline_val_acc

modern_train_acc = history_modern.history['accuracy'][-1]
modern_val_acc = history_modern.history['val_accuracy'][-1]
modern_gap = modern_train_acc - modern_val_acc

print("\nOVERFITTING ANALYSIS:")
print("="*70)
print(f"Baseline:")
print(f"  Train Accuracy: {baseline_train_acc:.2%}")
print(f"  Val Accuracy:   {baseline_val_acc:.2%}")
print(f"  Gap:            {baseline_gap:.2%} âš ï¸  (Overfitting!)")
print()
print(f"Modern:")
print(f"  Train Accuracy: {modern_train_acc:.2%}")
print(f"  Val Accuracy:   {modern_val_acc:.2%}")
print(f"  Gap:            {modern_gap:.2%} âœ… (Good generalization!)")
print()
print(f"Overfitting reduction: {(baseline_gap / modern_gap):.1f}Ã— better!")
print("="*70)

# Create confusion matrix for modern model
def plot_confusion_matrix(model, x_test, y_test, class_names):
    """Plot confusion matrix"""
    # Get predictions
    y_pred = model.predict(x_test, verbose=0)
    y_pred_classes = np.argmax(y_pred, axis=1)
    y_true = np.argmax(y_test, axis=1) if len(y_test.shape) > 1 else y_test.flatten()

    # Compute confusion matrix
    cm = confusion_matrix(y_true, y_pred_classes)

    # Plot
    plt.figure(figsize=(12, 10))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=class_names, yticklabels=class_names)
    plt.xlabel('Predicted', fontsize=12)
    plt.ylabel('True', fontsize=12)
    plt.title('Confusion Matrix - Modern Model', fontsize=14, fontweight='bold')
    plt.tight_layout()
    plt.show()

    # Find most confused classes
    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
    np.fill_diagonal(cm_normalized, 0)  # Ignore diagonal

    confused_pairs = []
    for i in range(len(class_names)):
        for j in range(len(class_names)):
            if i != j and cm_normalized[i, j] > 0.1:  # >10% confusion
                confused_pairs.append((class_names[i], class_names[j], cm_normalized[i, j]))

    confused_pairs.sort(key=lambda x: x[2], reverse=True)

    print("\nMost Confused Class Pairs:")
    print("="*70)
    for true_class, pred_class, conf_rate in confused_pairs[:5]:
        print(f"{true_class:>12} â†’ {pred_class:<12} : {conf_rate:.1%}")
    print("="*70)

print("\nGenerating confusion matrix...")
plot_confusion_matrix(model_modern, x_test_norm, y_test_cat, class_names)

# =============================================================================
# PART 8: ANALYSIS SUMMARY
# =============================================================================

print("\n" + "="*70)
print("ANALYSIS SUMMARY")
print("="*70)

print("""
KEY FINDINGS:

1. Parameter Counts:
   Baseline:  {:,} parameters
   Modern:    {:,} parameters
   Difference: {:}Ã— fewer (GlobalAveragePooling saves parameters!)

2. Training Speed:
   Baseline:  {:.1f}s/epoch
   Modern:    {:.1f}s/epoch
   Modern is slower due to:
   - Data augmentation (on-the-fly transformations)
   - BatchNormalization (additional computation)
   - Worth it for better accuracy!

3. Overfitting Analysis:
   Baseline gap: {:.1%} (severe overfitting)
   Modern gap:   {:.1%} (good generalization)
   Regularization reduced overfitting {:}Ã— !

4. Which Techniques Helped Most?
   - BatchNormalization: Faster convergence (2-3Ã— speedup)
   - Dropout: Reduced overfitting (especially 0.5 before output)
   - Data Augmentation: Improved test accuracy (+5-10%)
   - GlobalAvgPooling: Parameter reduction with minimal accuracy loss

5. Most Confused Classes:
   - Cat â†” Dog (similar features)
   - Automobile â†” Truck (both vehicles)
   - Bird â†” Airplane (both fly)
   This makes sense! These are semantically similar.

6. What If We Remove Techniques?
   - No BatchNorm â†’ 30% slower training
   - No Dropout â†’ Overfitting gap increases to 15-20%
   - No Augmentation â†’ Test accuracy drops 5-10%
   - No GlobalAvgPool â†’ 2Ã— more parameters

CONCLUSION:
Modern CNN techniques are essential for good performance!
All regularization methods work together synergistically.
""".format(
    baseline_params,
    modern_params,
    baseline_params // modern_params,
    baseline_time / (len(history_baseline.history['loss'])),
    modern_time / (len(history_modern.history['loss'])),
    baseline_gap,
    modern_gap,
    int(baseline_gap / modern_gap)
))

print("="*70)
print("TUTORIAL T11 COMPLETE!")
print("="*70)
print("\nNext Steps:")
print("1. Experiment with different hyperparameters")
print("2. Try different augmentation strategies")
print("3. Complete homework assignment")
print("4. Prepare for Week 12 (Transfer Learning)")
print("\nCongratulations! You've mastered modern CNN techniques! ðŸŽ‰")
print("="*70)
