# Week 12: Transfer Learning - Don't Reinvent the Wheel

**Course:** 21CSE558T - Deep Neural Network Architectures
**Module 4:** CNNs & Transfer Learning (Week 3 of 3)
**Date:** November 7, 2025 (DO3 - Part B)
**Duration:** 60 minutes (condensed from 120 minutes)
**Instructor:** Prof. Ramesh Babu
**Philosophy:** 80-10-10 (80% concepts, 10% code, 10% math)
**Teaching Approach:** WHY â†’ WHAT â†’ HOW with Real-World Stories

---

## ğŸ“‹ Session Overview

### Learning Objectives

By the end of this 60-minute session, students will be able to:

1. **Explain WHY** transfer learning is essential (data/cost/time savings)
2. **Understand WHAT** ImageNet is and why it's the foundation
3. **Distinguish** between Feature Extraction and Fine-Tuning strategies
4. **Choose** the appropriate strategy based on dataset size
5. **Select** the right pre-trained model (VGG16 vs ResNet50 vs MobileNetV2)
6. **Implement** transfer learning using TensorFlow/Keras in 5 lines of code
7. **Recognize** when transfer learning is the right solution

### Session Structure (60 minutes)

```
Part 1: WHY Transfer Learning? (20 minutes)
â”œâ”€â”€ The Data Problem (5 min)
â”œâ”€â”€ The Cost Problem (5 min)
â”œâ”€â”€ ImageNet - The Universal Knowledge Base (5 min)
â””â”€â”€ Real-World Analogy (5 min)

Part 2: HOW Transfer Learning Works? (25 minutes)
â”œâ”€â”€ Strategy 1: Feature Extraction â„ï¸ (10 min)
â”œâ”€â”€ Strategy 2: Fine-Tuning ğŸ”¥ (10 min)
â””â”€â”€ Decision Matrix - Which Strategy When? (5 min)

Part 3: WHICH Model to Use? (15 minutes)
â”œâ”€â”€ VGG16 - The Simple Giant (5 min)
â”œâ”€â”€ ResNet50 - The Default Choice â­ (5 min)
â”œâ”€â”€ MobileNetV2 - The Speed Demon (2 min)
â””â”€â”€ Model Selection Guide (3 min)
```

---

## ğŸ¯ The Core Message

**Before we start, remember this:**

> **"Why spend weeks training a model from scratch when someone already trained it on 1.2 million images? Just borrow their knowledge and adapt it to your problem."**

That's transfer learning in one sentence.

---

# PART 1: WHY Transfer Learning? (20 minutes)

## ğŸš¨ The Problem: Deep Learning's Three Barriers

### Barrier 1: The Data Hunger Problem

**Question to Class:** *"How many images do you think it takes to train a good CNN from scratch?"*

**Answer:** Millions. Not thousands. **MILLIONS.**

**The Reality Check:**

| Task | Images Needed (From Scratch) | What Most People Have |
|------|------------------------------|----------------------|
| Image Classification | 1-10 million | 1,000-10,000 |
| Object Detection | 5-20 million | 2,000-20,000 |
| Medical Imaging | 100K-1M | 500-5,000 |

**The Gap:** Most people have 100-1000Ã— LESS data than needed.

---

### Barrier 2: The Computational Cost Problem

**Training ResNet50 from scratch on ImageNet:**

- **Hardware Required:** 8Ã— Tesla V100 GPUs ($80,000)
- **Training Time:** 2-4 weeks
- **AWS Cost:** ~$10,000-$15,000
- **Electricity Cost:** ~$2,000
- **Total:** $12,000+ per training run

**Who can afford this?** Google, Facebook, universities. Not students or startups.

---

### Barrier 3: The Expertise Gap Problem

**Training deep CNNs from scratch requires:**
- PhD-level understanding of optimization
- Hyperparameter tuning expertise (learning rate schedules, weight initialization)
- Debugging skills for vanishing/exploding gradients
- Months of trial and error

**Most people don't have this expertise.** And they don't need it if transfer learning exists.

---

## ğŸ’¡ The Solution: Transfer Learning

**The Breakthrough Insight:**

> **"CNNs trained on ImageNet learn UNIVERSAL visual features - edges, textures, shapes, patterns - that work on ANY image task, not just ImageNet categories."**

**This means:**
- Someone already spent $15,000 training ResNet50 on ImageNet
- Those learned features (edges, textures, shapes) work for YOUR task too
- You can **borrow** their pre-trained model
- Train only the last layer on YOUR data (100Ã— faster, 100Ã— cheaper)

**Result:** Go from needing 1 million images to needing just 1,000-5,000 images.

---

## ğŸ“š Story 1: Dr. Anjali's Medical AI Breakthrough

**Meet Dr. Anjali:**
- Radiologist at a rural hospital
- Wants to build AI to detect pneumonia from chest X-rays
- **Her Dataset:** Only 500 X-ray images (took 6 months to collect)

**Attempt 1: Train CNN from Scratch**
```
Dataset: 500 images
Model: Custom CNN (5 Conv layers)
Training: 50 epochs, 2 hours
Result: 55% accuracy (barely better than random!)
Problem: Massive overfitting - not enough data
```

**Attempt 2: Transfer Learning with VGG16**
```
Dataset: Same 500 images
Model: VGG16 pre-trained on ImageNet (freeze base, train classifier only)
Training: 10 epochs, 15 minutes
Result: 92% accuracy! ğŸ‰
```

**What Changed?**
- VGG16 already learned to detect edges, textures, shapes from 1.2M ImageNet images
- Those features work on X-rays too!
- Dr. Anjali only needed to teach: "These patterns = pneumonia"

**Impact:** Deployed in 3 rural hospitals, saved hundreds of lives.

**Lesson:** **Transfer learning makes impossible tasks possible with small datasets.**

---

## ğŸŒ ImageNet - The Universal Knowledge Base

### What is ImageNet?

**ImageNet Dataset:**
- **Created by:** Prof. Fei-Fei Li (Stanford), 2009
- **Total Images:** 14 million labeled photographs
- **Total Categories:** 20,000+ (dogs, cats, vehicles, furniture, food, etc.)
- **Competition Subset (ILSVRC):** 1.2 million images, 1,000 categories
- **Purpose:** Train computers to understand the visual world

**Sample Categories:**
```
Animals: 120 dog breeds, 50 cat species, birds, fish, insects
Vehicles: Cars, trucks, airplanes, boats, motorcycles
Objects: Furniture, tools, instruments, sports equipment
Nature: Trees, flowers, landscapes, food
```

**Why ImageNet Changed Everything:**

| Before ImageNet (Pre-2012) | After ImageNet (Post-2012) |
|----------------------------|----------------------------|
| Small datasets (10K-100K images) | Large-scale dataset (1.2M images) |
| Simple objects (MNIST digits) | Real-world complexity |
| Shallow features learned | Deep hierarchical features learned |
| Transfer learning didn't work well | Transfer learning became standard |

---

### The 2012 AlexNet Shock

**ImageNet Competition (ILSVRC) Results:**

| Year | Winner | Top-5 Error | Method | Improvement |
|------|--------|-------------|--------|-------------|
| 2010 | NEC | 28.2% | Traditional CV (HOG+SVM) | - |
| 2011 | XRCE | 25.8% | Traditional CV | 2.4% |
| **2012** | **AlexNet** | **16.4%** | **CNN (Deep Learning)** | **9.4% (!!)** |
| 2013 | ZFNet | 11.7% | Deeper CNN | 4.7% |
| 2014 | GoogLeNet | 6.7% | Inception | 5.0% |
| 2015 | ResNet | 3.6% | Skip Connections | 3.1% |
| 2017 | SENet | 2.3% | **Better than humans (5%)** | 1.3% |

**The Shock Wave:**
- AlexNet cut error by **37% in one year** (unprecedented!)
- Traditional computer vision died overnight
- Within 2 years, 100% of competitors used CNNs
- Every major company pivoted to deep learning

**Why This Matters for Transfer Learning:**
- These winning models (AlexNet, VGG, ResNet) are now publicly available
- Anyone can download them for FREE
- They're trained on 1.2M images (you don't have to!)
- Transfer learning = standing on the shoulders of giants

---

### The Feature Hierarchy Discovery

**What ImageNet-trained CNNs Learn:**

```
Layer 1 (Early Layers):
â”œâ”€â”€ Edges (horizontal, vertical, diagonal)
â”œâ”€â”€ Colors (red blobs, blue regions)
â”œâ”€â”€ Textures (smooth, rough, dotted, striped)
â””â”€â”€ Simple patterns

â†“ These features are UNIVERSAL!
â†“ Found in ALL images: medical X-rays, satellite photos, faces, crops

Layer 2-3 (Middle Layers):
â”œâ”€â”€ Corners and curves
â”œâ”€â”€ Simple shapes (circles, rectangles, triangles)
â”œâ”€â”€ Patterns (grids, waves, spots)
â””â”€â”€ Textures combinations

â†“ Still quite general
â†“ Work across many domains

Layer 4-5 (Deep Layers):
â”œâ”€â”€ Object parts (eyes, wheels, windows, leaves)
â”œâ”€â”€ Complex patterns (fur, metal, wood grain)
â”œâ”€â”€ Scene elements (sky, grass, water)
â””â”€â”€ Category-specific features

â†“ More specific to ImageNet
â†“ But still transferable!

Final Layer (Classifier):
â”œâ”€â”€ 1,000 ImageNet categories
â”œâ”€â”€ Dog breeds, car models, food types
â””â”€â”€ Very specific to ImageNet

â†“ THIS is what we replace!
```

**Key Insight:**
- **Layers 1-3:** Universal visual features (work everywhere)
- **Layers 4-5:** High-level features (still useful)
- **Final layer:** Task-specific (we replace this)

**Transfer Learning Strategy:**
- **Keep:** Layers 1-5 (universal + high-level features)
- **Replace:** Final layer with YOUR task classifier

---

## ğŸš— Real-World Analogy: Learning to Drive

**Think about learning to drive a new car:**

**Option A: Learn from Scratch (No Transfer Learning)**
```
New car: Tesla Model 3
Learning process:
â”œâ”€â”€ Step 1: Learn what a steering wheel does
â”œâ”€â”€ Step 2: Learn what brakes do
â”œâ”€â”€ Step 3: Learn what accelerator does
â”œâ”€â”€ Step 4: Learn traffic rules
â”œâ”€â”€ Step 5: Learn parking
â””â”€â”€ Time: 6 months of practice
```

**Absurd, right?** You already know how to drive from your previous car!

**Option B: Transfer Learning**
```
New car: Tesla Model 3
Learning process:
â”œâ”€â”€ Keep: Steering, braking, accelerating skills (TRANSFERRED!)
â”œâ”€â”€ Keep: Traffic rules, spatial awareness (TRANSFERRED!)
â”œâ”€â”€ Learn NEW: Electric acceleration feel, regenerative braking
â””â”€â”€ Time: 1 hour of practice
```

**Same concept applies to CNNs:**

| Driving Skills | CNN Features |
|----------------|--------------|
| Steering, braking (universal) | Edges, textures (Layer 1-3) |
| Traffic awareness (general) | Shapes, patterns (Layer 4-5) |
| Specific to Tesla Model 3 | Your task classifier (Final layer) |

**You don't start from scratch - you TRANSFER what you know!**

---

## ğŸ“ Part 1 Summary (Key Takeaways)

**The Three Barriers of Deep Learning:**
1. âŒ **Data Hunger:** Need millions of images (we have thousands)
2. âŒ **Cost:** $15,000+ to train from scratch (we have $0)
3. âŒ **Expertise:** PhD-level knowledge needed (we're learning)

**Transfer Learning Solution:**
âœ… **Borrow** pre-trained ImageNet models (free!)
âœ… **Reuse** universal visual features (works on any image)
âœ… **Adapt** final layer to your task (fast and cheap)

**Result:**
- **Data Needed:** 1M images â†’ 1K-5K images (100Ã— reduction)
- **Training Time:** 2 weeks â†’ 15 minutes (2000Ã— faster)
- **Cost:** $15,000 â†’ $0 (FREE!)
- **Accuracy:** Better than training from scratch!

**Bottom Line:** Transfer learning makes deep learning accessible to everyone.

---

# PART 2: HOW Transfer Learning Works? (25 minutes)

## ğŸ“– Story 2: Rohan's AgriTech Challenge

**Meet Rohan:**
- Agricultural engineer in Punjab
- Building app to detect crop diseases from smartphone photos
- **His Dataset:** 5,000 images of wheat, rice, maize diseases
- **Classes:** 10 diseases + 1 healthy

**The Question:** How should Rohan use transfer learning?

**Two Paths Forward:**
1. **Feature Extraction â„ï¸** (freeze entire base model)
2. **Fine-Tuning ğŸ”¥** (unfreeze some layers)

Let's explore both...

---

## Strategy 1: Feature Extraction â„ï¸ (10 minutes)

### The Concept: Frozen Foundation

**Analogy:** Using a pre-built house foundation
- Foundation is solid (ImageNet features)
- Don't modify foundation
- Just build your custom room on top

**How It Works:**

```
Pre-trained VGG16 (Trained on ImageNet 1.2M images):

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Layer 1-5: Edges, Textures, Shapes     â”‚  â† FROZEN â„ï¸
â”‚  (138 million parameters)               â”‚  â† trainable=False
â”‚  Universal visual features              â”‚  â† Don't touch!
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Remove: Original 1000-class classifier â”‚  â† DELETED
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Add: YOUR custom classifier            â”‚  â† NEW! ğŸ”¥
â”‚  GlobalAveragePooling2D                 â”‚  â† trainable=True
â”‚  Dense(11, softmax)  # 10 diseases + 1  â”‚  â† Only this trains!
â”‚  (Only 11,000 parameters)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Training:**
- â„ï¸ VGG16 base: **FROZEN** (138M params don't update)
- ğŸ”¥ Classifier: **TRAINS** (11K params update)
- **Total trainable:** Only 11,000 params (0.008% of original!)

---

### Code Example: Feature Extraction

```python
import tensorflow as tf
from tensorflow.keras.applications import VGG16
from tensorflow.keras import Sequential
from tensorflow.keras.layers import GlobalAveragePooling2D, Dense

# Step 1: Load pre-trained VGG16 (without top classifier)
base_model = VGG16(
    weights='imagenet',        # Use ImageNet pre-trained weights
    include_top=False,         # Remove original classifier
    input_shape=(224, 224, 3)  # Standard ImageNet input size
)

# Step 2: FREEZE the entire base model
base_model.trainable = False   # â„ï¸ Don't update these weights!

# Step 3: Add YOUR custom classifier
model = Sequential([
    base_model,                              # Frozen VGG16 base
    GlobalAveragePooling2D(),                # Reduce spatial dimensions
    Dense(11, activation='softmax')          # 10 diseases + 1 healthy
])

# Step 4: Compile and train (only classifier trains!)
model.compile(
    optimizer='adam',                        # Fast optimizer
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

# Step 5: Train on YOUR data
history = model.fit(
    train_data,              # Rohan's 5,000 crop images
    epochs=10,               # Only need 10 epochs!
    validation_data=val_data
)

# Result: 85% accuracy in 20 minutes! ğŸ‰
```

**What Happens During Training:**
- VGG16 base acts as **feature extractor** (converts images â†’ feature vectors)
- Only the Dense(11) layer learns: "These features = disease X"
- 138M params stay frozen â†’ **Very fast training**

---

### When to Use Feature Extraction

**âœ… Use Feature Extraction When:**

1. **Small Dataset (<5,000 images)**
   - Not enough data to fine-tune large networks
   - Risk of overfitting if you unfreeze too many params

2. **Limited Compute (Laptop, CPU)**
   - Training only final layer is 10Ã— faster
   - Can run on MacBook, no GPU needed

3. **Similar to ImageNet**
   - Your images look like natural photos (not too different)
   - Example: Cats vs Dogs, Flowers, Food

4. **Quick Prototyping**
   - Need fast results to test idea
   - Can always fine-tune later if needed

**Rohan's Result with Feature Extraction:**
```
Dataset: 5,000 crop disease images
Model: VGG16 feature extraction
Training time: 20 minutes (CPU)
Accuracy: 85%
```

**Good, but can we do better?** Yes â†’ Fine-Tuning!

---

## Strategy 2: Fine-Tuning ğŸ”¥ (10 minutes)

### The Concept: Adaptive Foundation

**Analogy:** Renovating a house foundation
- Foundation is good but not perfect for your soil
- Modify top layers of foundation (carefully!)
- Keep bottom layers unchanged (they're solid)

**How It Works:**

```
Pre-trained ResNet50:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Layers 1-40: Universal Features        â”‚  â† FROZEN â„ï¸
â”‚  (Edges, textures, basic shapes)        â”‚  â† trainable=False
â”‚  Keep these exactly as-is               â”‚  â† Too universal to change
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Layers 41-50: High-level Features      â”‚  â† UNFROZEN ğŸ”¥
â”‚  (Object parts, domain-specific)        â”‚  â† trainable=True
â”‚  Let these ADAPT to crop diseases       â”‚  â† Learn wheat patterns
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  YOUR custom classifier                  â”‚  â† NEW! ğŸ”¥
â”‚  Dense(11, softmax)                      â”‚  â† trainable=True
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Training:**
- â„ï¸ Layers 1-40: **FROZEN** (universal features)
- ğŸ”¥ Layers 41-50: **UNFROZEN** (adapt to crops)
- ğŸ”¥ Classifier: **TRAINS** (your 11 classes)

**Key Difference from Feature Extraction:**
- Feature Extraction: Only classifier trains (11K params)
- Fine-Tuning: Classifier + top layers train (2M params)

---

### Code Example: Fine-Tuning

```python
from tensorflow.keras.applications import ResNet50
from tensorflow.keras import Sequential
from tensorflow.keras.layers import GlobalAveragePooling2D, Dense

# Step 1: Load pre-trained ResNet50
base_model = ResNet50(
    weights='imagenet',
    include_top=False,
    input_shape=(224, 224, 3)
)

# Step 2: FREEZE first 80% of layers, UNFREEZE last 20%
base_model.trainable = True  # Enable training

# Freeze layers 1-40 (80% of 50 layers)
for layer in base_model.layers[:40]:
    layer.trainable = False  # â„ï¸ Keep frozen

# Unfreeze layers 41-50 (last 20%)
for layer in base_model.layers[40:]:
    layer.trainable = True   # ğŸ”¥ Allow adaptation

# Step 3: Add classifier
model = Sequential([
    base_model,
    GlobalAveragePooling2D(),
    Dense(11, activation='softmax')
])

# Step 4: Compile with LOWER learning rate (important!)
model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),  # 100Ã— smaller!
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

# Step 5: Fine-tune on YOUR data
history = model.fit(
    train_data,
    epochs=20,              # Need more epochs than feature extraction
    validation_data=val_data
)

# Result: 92% accuracy in 1 hour! ğŸ‰
```

**Critical Detail - Learning Rate:**
```python
# Feature Extraction:
optimizer=Adam(learning_rate=1e-3)  # Default (large)

# Fine-Tuning:
optimizer=Adam(learning_rate=1e-5)  # 100Ã— smaller!
```

**Why lower learning rate?**
- Pre-trained weights are already good
- Large updates would destroy learned features
- Small updates gently adapt features to your domain

---

### When to Use Fine-Tuning

**âœ… Use Fine-Tuning When:**

1. **Larger Dataset (>10,000 images)**
   - Enough data to safely update millions of params
   - Lower risk of overfitting

2. **Different from ImageNet**
   - Medical images (X-rays, MRIs)
   - Satellite imagery
   - Microscopy images
   - Domain gap is large â†’ need adaptation

3. **Higher Accuracy Needed**
   - Feature extraction plateaus at 85%
   - Fine-tuning can reach 92-95%

4. **Have Compute Resources (GPU)**
   - Fine-tuning takes 5-10Ã— longer than feature extraction
   - GPU recommended (though not strictly required)

**Rohan's Result with Fine-Tuning:**
```
Dataset: 5,000 crop disease images
Model: ResNet50 fine-tuning (last 10 layers unfrozen)
Training time: 1 hour (GPU)
Accuracy: 92% (vs 85% with feature extraction)
```

**Worth it?** +7% accuracy for 3Ã— more training time â†’ YES!

---

## Decision Matrix: Which Strategy When? (5 minutes)

### Quick Reference Table

| Your Situation | Dataset Size | Compute | Domain Similarity | Recommended Strategy |
|----------------|--------------|---------|-------------------|---------------------|
| Student project | <1,000 | Laptop | Natural images | **Feature Extraction** |
| Startup MVP | 1K-5K | Cloud CPU | Natural images | **Feature Extraction** |
| Research project | 5K-20K | Cloud GPU | Natural images | **Feature Extraction â†’ Fine-Tuning** |
| Production app | >20K | Cloud GPU | Natural images | **Fine-Tuning** |
| Medical imaging | Any size | Cloud GPU | Very different | **Fine-Tuning** |
| Mobile deployment | Any size | Phone | Any | **Feature Extraction (MobileNet)** |

### The Two-Stage Approach (BEST PRACTICE)

**For medium datasets (5K-20K images), use BOTH strategies:**

```
Stage 1: Feature Extraction (Fast)
â”œâ”€â”€ Freeze entire base model
â”œâ”€â”€ Train classifier only (10 epochs, 20 min)
â”œâ”€â”€ Get baseline: ~85% accuracy
â””â”€â”€ Save best weights

Stage 2: Fine-Tuning (Slow but better)
â”œâ”€â”€ Unfreeze last 10-20% of layers
â”œâ”€â”€ Use lower learning rate (1e-5)
â”œâ”€â”€ Train for 20 epochs (1 hour)
â”œâ”€â”€ Get improved: ~92% accuracy
â””â”€â”€ Compare with Stage 1 - worth it?
```

**Why this works:**
- Stage 1 finds good classifier quickly
- Stage 2 adapts features without starting from random
- Less risk of overfitting than fine-tuning from start

---

### Common Mistakes to Avoid

**âŒ Mistake 1: Fine-tuning with high learning rate**
```python
# WRONG - destroys pre-trained features!
optimizer=Adam(learning_rate=1e-3)
```
âœ… **Solution:** Always use 10-100Ã— smaller learning rate for fine-tuning

**âŒ Mistake 2: Unfreezing too many layers with small data**
```python
# WRONG with 1,000 images - overfitting guaranteed!
base_model.trainable = True  # All layers trainable
```
âœ… **Solution:** Freeze at least 50-80% of base model

**âŒ Mistake 3: Not using data augmentation**
```python
# WRONG - small data + no augmentation = overfitting
model.fit(train_data, epochs=50)
```
âœ… **Solution:** ALWAYS use augmentation with transfer learning

**âŒ Mistake 4: Wrong input size**
```python
# WRONG - VGG16 expects 224Ã—224
base_model = VGG16(input_shape=(128, 128, 3))  # Error!
```
âœ… **Solution:** Use model's default input size (usually 224Ã—224)

---

## ğŸ“ Part 2 Summary (Key Takeaways)

**Two Transfer Learning Strategies:**

| Aspect | Feature Extraction â„ï¸ | Fine-Tuning ğŸ”¥ |
|--------|----------------------|---------------|
| **What freezes?** | Entire base model | First 50-80% only |
| **What trains?** | Only classifier | Classifier + top layers |
| **Dataset size** | <5K images | >10K images |
| **Training time** | 10-30 minutes | 1-2 hours |
| **Accuracy** | 80-85% | 90-95% |
| **Learning rate** | 1e-3 (normal) | 1e-5 (100Ã— smaller) |
| **Use when** | Quick prototype, small data | Production, large data |

**Decision Flow:**
```
Do you have <5K images?
â”œâ”€â”€ YES â†’ Feature Extraction
â””â”€â”€ NO (>10K) â†’ Fine-Tuning

Is your domain very different from ImageNet?
â”œâ”€â”€ YES â†’ Fine-Tuning (even with small data)
â””â”€â”€ NO â†’ Feature Extraction first

Do you need highest accuracy?
â”œâ”€â”€ YES â†’ Fine-Tuning
â””â”€â”€ NO â†’ Feature Extraction is enough
```

**Rohan's Final Results:**
- Feature Extraction: 85% accuracy, 20 min
- Fine-Tuning: 92% accuracy, 1 hour
- **Choice:** Fine-Tuning (worth the extra time for production app)

---

# PART 3: WHICH Model to Use? (15 minutes)

## ğŸ“– Story 3: Maya's Wildlife Detection

**Meet Maya:**
- Wildlife conservationist in Bandipur Forest
- Building app for forest rangers to detect endangered species
- **Constraints:**
  - App must run on ranger's phone (no internet in forest!)
  - Must identify 20 species in real-time (<1 second)
  - Dataset: 8,000 images collected over 2 years

**The Question:** Which pre-trained model should Maya use?

**Options:**
1. VGG16 - Simple but huge
2. ResNet50 - Accurate and standard
3. MobileNetV2 - Fast and tiny

Let's explore each...

---

## Model 1: VGG16 - The Simple Giant (5 minutes)

### Architecture Overview

**VGG16 (Visual Geometry Group, Oxford, 2014):**

```
Input (224Ã—224Ã—3)
â†“
[Conv 64 â†’ Conv 64] â†’ MaxPool     # Block 1
â†“
[Conv 128 â†’ Conv 128] â†’ MaxPool   # Block 2
â†“
[Conv 256 â†’ Conv 256 â†’ Conv 256] â†’ MaxPool  # Block 3
â†“
[Conv 512 â†’ Conv 512 â†’ Conv 512] â†’ MaxPool  # Block 4
â†“
[Conv 512 â†’ Conv 512 â†’ Conv 512] â†’ MaxPool  # Block 5
â†“
Flatten â†’ Dense 4096 â†’ Dense 4096 â†’ Dense 1000
```

**Key Numbers:**
- **Total Layers:** 16 (13 Conv + 3 Dense)
- **Parameters:** 138 million
- **Model Size:** 528 MB (huge!)
- **ImageNet Top-5 Accuracy:** 92.7%

---

### VGG16 Strengths & Weaknesses

**âœ… Strengths:**

1. **Extreme Simplicity**
   - Same pattern repeated: [Conv â†’ Conv â†’ Pool]
   - All Conv layers use 3Ã—3 filters
   - Easy to understand and visualize

2. **Reliable Performance**
   - Proven architecture (10+ years in production)
   - Works consistently across domains
   - Good baseline for research

3. **Educational Value**
   - Perfect for learning transfer learning
   - Clear feature progression
   - Easy to debug

**âŒ Weaknesses:**

1. **Massive Size**
   - 528 MB model (can't fit on mobile phones)
   - Slow inference (2-3 seconds on CPU)

2. **Computational Cost**
   - Fine-tuning takes 3-4Ã— longer than ResNet50
   - Needs powerful GPU

3. **Outdated**
   - Modern models (ResNet, EfficientNet) outperform it
   - Industry moving away from VGG

---

### When to Use VGG16

**âœ… Use VGG16 When:**
- **Learning transfer learning** (best teaching model)
- **Need simplicity over performance**
- **Have powerful GPU and don't care about speed**
- **Feature visualization projects** (research)

**âŒ Don't Use VGG16 When:**
- Deploying to mobile/edge devices (too big!)
- Need real-time inference (too slow!)
- Production applications (use ResNet instead)

---

### Code Example: VGG16

```python
from tensorflow.keras.applications import VGG16

# Load VGG16 (downloads 528 MB on first run)
base_model = VGG16(
    weights='imagenet',
    include_top=False,
    input_shape=(224, 224, 3)
)

# Print architecture
base_model.summary()

"""
Output:
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1,792
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36,928
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
...
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
=================================================================
Total params: 14,714,688 (138 million if include_top=True)
"""

# For Maya's wildlife app: Too big for mobile!
```

**Verdict for Maya:** âŒ VGG16 won't work - too big for ranger's phone

---

## Model 2: ResNet50 - The Default Choice â­ (5 minutes)

### Architecture Overview

**ResNet50 (Residual Network, Microsoft, 2015):**

**Revolutionary Idea: Skip Connections**

```
Traditional CNN:
Input â†’ Conv â†’ Conv â†’ Conv â†’ ... â†’ Output
Problem: Gradients vanish in deep networks (>20 layers)

ResNet Solution:
Input â†’ Conv â†’ Conv â†’ Output
  â†“                     â†‘
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ Skip Connection!

Math: Output = F(Input) + Input
```

**Why Skip Connections Matter:**
- Allow 50-152 layer networks (vs 16 for VGG)
- Gradients flow directly through skip connections
- Easier to train, better accuracy

---

### ResNet50 Architecture

```
Input (224Ã—224Ã—3)
â†“
Initial Conv + MaxPool
â†“
[Residual Block Ã— 3]  # 64 filters    â† Skip connections
â†“
[Residual Block Ã— 4]  # 128 filters   â† Skip connections
â†“
[Residual Block Ã— 6]  # 256 filters   â† Skip connections
â†“
[Residual Block Ã— 3]  # 512 filters   â† Skip connections
â†“
GlobalAveragePooling â†’ Dense 1000
```

**Key Numbers:**
- **Total Layers:** 50 (48 Conv + 1 Dense)
- **Parameters:** 25.6 million (5Ã— smaller than VGG16!)
- **Model Size:** 98 MB
- **ImageNet Top-5 Accuracy:** 93.3% (better than VGG16!)

**The Magic:** Smaller, faster, AND more accurate than VGG16!

---

### ResNet50 Strengths & Weaknesses

**âœ… Strengths:**

1. **Best Balance**
   - Smaller than VGG16 (98 MB vs 528 MB)
   - More accurate than VGG16 (93.3% vs 92.7%)
   - Faster training and inference

2. **Industry Standard**
   - Default choice in 90% of applications
   - Well-supported in all frameworks
   - Tons of tutorials and examples

3. **Scalable**
   - Variants: ResNet18, ResNet34, ResNet50, ResNet101, ResNet152
   - Can choose based on accuracy vs speed tradeoff

4. **Deep Networks Possible**
   - Skip connections enable 50+ layers
   - Learns richer features

**âŒ Weaknesses:**

1. **More Complex**
   - Skip connections harder to understand initially
   - Debugging is less intuitive than VGG

2. **Still Too Big for Mobile**
   - 98 MB is smaller than VGG but still large
   - Inference takes 1-2 seconds on phone

---

### When to Use ResNet50

**âœ… Use ResNet50 When:**
- **Most real-world applications** (default choice!)
- Need good accuracy with reasonable speed
- Training in cloud/server (not edge deployment)
- Don't have specific constraints

**ğŸ† ResNet50 is the DEFAULT unless you have a specific reason to choose otherwise.**

---

### Code Example: ResNet50

```python
from tensorflow.keras.applications import ResNet50

# Load ResNet50 (downloads 98 MB on first run)
base_model = ResNet50(
    weights='imagenet',
    include_top=False,
    input_shape=(224, 224, 3)
)

# Compare with VGG16
print(f"VGG16 params: 138M, size: 528 MB")
print(f"ResNet50 params: 25.6M, size: 98 MB")
print(f"ResNet50 is 5Ã— smaller and more accurate!")

# For Maya's wildlife app: Still too big for mobile (98 MB)
```

**Verdict for Maya:** âš ï¸ ResNet50 is better but still too big for mobile

---

## Model 3: MobileNetV2 - The Speed Demon (2 minutes)

### Architecture Overview

**MobileNetV2 (Google, 2018):**

**Design Goal:** Extremely efficient for mobile and edge devices

**Key Innovation: Depthwise Separable Convolutions**

```
Standard Convolution:
Input (56Ã—56Ã—64) â†’ Conv 3Ã—3Ã—128 â†’ Output (56Ã—56Ã—128)
Params: 3 Ã— 3 Ã— 64 Ã— 128 = 73,728

Depthwise Separable (MobileNet):
Input (56Ã—56Ã—64)
  â†“ Depthwise Conv (3Ã—3 per channel)
  â†“ Params: 3 Ã— 3 Ã— 64 = 576
  â†“ Pointwise Conv (1Ã—1)
  â†“ Params: 64 Ã— 128 = 8,192
Output (56Ã—56Ã—128)
Total Params: 576 + 8,192 = 8,768

Reduction: 73,728 â†’ 8,768 (8Ã— smaller!)
```

**Key Numbers:**
- **Parameters:** 3.5 million (40Ã— smaller than VGG16!)
- **Model Size:** 14 MB (tiny!)
- **ImageNet Top-5 Accuracy:** 90.1% (slightly lower)
- **Inference Speed:** 0.1 seconds on phone (20Ã— faster!)

---

### When to Use MobileNetV2

**âœ… Use MobileNetV2 When:**
- **Mobile app deployment** (phones, tablets)
- **Edge devices** (Raspberry Pi, IoT cameras)
- **Real-time inference** needed (<0.5 seconds)
- **Limited storage** (14 MB vs 98 MB)

**Trade-off:**
- Lose 2-3% accuracy vs ResNet50
- Gain 10-20Ã— speed improvement

**For 90% of mobile apps, this trade-off is worth it!**

---

### Code Example: MobileNetV2

```python
from tensorflow.keras.applications import MobileNetV2

# Load MobileNetV2 (downloads only 14 MB!)
base_model = MobileNetV2(
    weights='imagenet',
    include_top=False,
    input_shape=(224, 224, 3)
)

# Model comparison
print("Model Sizes:")
print(f"  VGG16:      528 MB")
print(f"  ResNet50:    98 MB")
print(f"  MobileNetV2: 14 MB  â† Fits on phone!")

print("\nInference Speed (iPhone):")
print(f"  VGG16:      3.0 sec")
print(f"  ResNet50:   1.5 sec")
print(f"  MobileNetV2: 0.1 sec  â† Real-time!")

# For Maya's wildlife app: Perfect fit! âœ…
```

**Verdict for Maya:** âœ… MobileNetV2 is perfect for ranger's phone!

---

## Model Selection Guide (3 minutes)

### Quick Decision Flowchart

```
START: Which pre-trained model should I use?
â”‚
â”œâ”€ Are you LEARNING transfer learning?
â”‚  â””â”€ YES â†’ VGG16 (simplest architecture)
â”‚
â”œâ”€ Do you need to deploy on MOBILE/EDGE?
â”‚  â””â”€ YES â†’ MobileNetV2 (14 MB, fast)
â”‚
â”œâ”€ Do you need REAL-TIME inference?
â”‚  â””â”€ YES â†’ MobileNetV2 (0.1 sec)
â”‚
â”œâ”€ Do you have SPECIFIC constraints (size, speed)?
â”‚  â””â”€ YES â†’ Evaluate all options, benchmark
â”‚
â””â”€ DEFAULT â†’ ResNet50 â­
   (Best balance, industry standard)
```

---

### Complete Model Comparison Table

| Model | Params | Size | Top-5 Acc | Speed (CPU) | Use Case |
|-------|--------|------|-----------|-------------|----------|
| **VGG16** | 138M | 528 MB | 92.7% | 3.0 sec | Learning, education |
| **VGG19** | 144M | 549 MB | 92.8% | 3.5 sec | Feature visualization |
| **ResNet50** â­ | 25.6M | 98 MB | 93.3% | 1.5 sec | **Default choice** |
| **ResNet101** | 44.5M | 171 MB | 93.8% | 2.5 sec | High accuracy needed |
| **MobileNetV2** | 3.5M | 14 MB | 90.1% | 0.1 sec | Mobile deployment |
| **EfficientNetB0** | 5.3M | 29 MB | 93.3% | 0.5 sec | Modern default |

---

### The Three Stories - Final Decisions

**Dr. Anjali (Medical Imaging - 500 X-rays):**
- **Choice:** VGG16 feature extraction
- **Reason:** Small data, need reliability, not deploying to mobile
- **Result:** 92% accuracy

**Rohan (AgriTech - 5,000 crop images):**
- **Choice:** ResNet50 fine-tuning
- **Reason:** Medium data, need accuracy, cloud deployment
- **Result:** 92% accuracy

**Maya (Wildlife - 8,000 images, mobile app):**
- **Choice:** MobileNetV2 fine-tuning
- **Reason:** Mobile deployment, real-time inference required
- **Result:** 88% accuracy (acceptable trade-off for speed)

---

### Loading Models in TensorFlow/Keras

```python
from tensorflow.keras.applications import (
    VGG16, VGG19,
    ResNet50, ResNet101, ResNet152,
    MobileNetV2, MobileNetV3Small,
    EfficientNetB0, EfficientNetB7
)

# All models follow same API pattern:
base_model = ModelName(
    weights='imagenet',           # Pre-trained on ImageNet
    include_top=False,            # Remove classifier
    input_shape=(224, 224, 3)     # Standard input size
)

# Example with each model:
vgg = VGG16(weights='imagenet', include_top=False)
resnet = ResNet50(weights='imagenet', include_top=False)
mobile = MobileNetV2(weights='imagenet', include_top=False)

# All downloaded automatically on first use!
```

---

## ğŸ“ Part 3 Summary (Key Takeaways)

**Three Model Categories:**

1. **VGG16 - The Teacher** ğŸ“š
   - Best for: Learning, education
   - Size: 528 MB (huge)
   - Use when: Understanding transfer learning

2. **ResNet50 - The Workhorse** â­
   - Best for: Most applications (DEFAULT)
   - Size: 98 MB (reasonable)
   - Use when: No specific constraints

3. **MobileNetV2 - The Sprinter** ğŸƒ
   - Best for: Mobile, edge, real-time
   - Size: 14 MB (tiny)
   - Use when: Speed/size matters

**Decision Rule:**
```python
if learning:
    model = VGG16
elif mobile_deployment:
    model = MobileNetV2
else:
    model = ResNet50  # Default for 90% of cases
```

**All models:**
- âœ… Pre-trained on ImageNet (1.2M images)
- âœ… Learn universal visual features
- âœ… Free to download and use
- âœ… Same simple API in Keras

---

# ğŸ“ Complete Session Summary

## The Big Picture

**Transfer Learning in 3 Steps:**

```
Step 1: Choose Pre-trained Model
â”œâ”€â”€ VGG16 (learning)
â”œâ”€â”€ ResNet50 (default) â­
â””â”€â”€ MobileNetV2 (mobile)

Step 2: Choose Strategy
â”œâ”€â”€ Feature Extraction (<5K images) â„ï¸
â””â”€â”€ Fine-Tuning (>10K images) ğŸ”¥

Step 3: Train on YOUR Data
â”œâ”€â”€ Replace final classifier
â”œâ”€â”€ Train 10-20 epochs
â””â”€â”€ Get 85-95% accuracy! ğŸ‰
```

---

## Key Numbers to Remember

**Data Reduction:**
- From scratch: 1-10 million images needed
- Transfer learning: 1,000-10,000 images needed
- **Reduction: 100-1000Ã—**

**Time Reduction:**
- From scratch: 2-4 weeks training
- Transfer learning: 15 minutes - 2 hours
- **Reduction: 200-2000Ã—**

**Cost Reduction:**
- From scratch: $10,000-$15,000
- Transfer learning: $0-$100
- **Reduction: 100-150Ã—**

---

## Three Real-World Success Stories

| Person | Task | Dataset | Model | Strategy | Accuracy |
|--------|------|---------|-------|----------|----------|
| **Dr. Anjali** | Pneumonia detection | 500 X-rays | VGG16 | Feature Extraction | 92% |
| **Rohan** | Crop disease | 5,000 crops | ResNet50 | Fine-Tuning | 92% |
| **Maya** | Wildlife detection | 8,000 animals | MobileNetV2 | Fine-Tuning | 88% |

**Common Thread:** All used transfer learning to achieve production-grade results with limited data!

---

## Decision Matrices (Quick Reference)

### Strategy Selection
| Dataset Size | Compute | Strategy |
|-------------|---------|----------|
| <5K | CPU | Feature Extraction |
| 5K-20K | GPU | Feature Extraction â†’ Fine-Tuning |
| >20K | GPU | Fine-Tuning |

### Model Selection
| Constraint | Model |
|-----------|-------|
| Learning | VGG16 |
| Mobile | MobileNetV2 |
| Default | ResNet50 â­ |

---

## Code Templates (Copy-Paste Ready)

### Template 1: Feature Extraction
```python
from tensorflow.keras.applications import ResNet50
from tensorflow.keras import Sequential
from tensorflow.keras.layers import GlobalAveragePooling2D, Dense

# Load & freeze base
base = ResNet50(weights='imagenet', include_top=False)
base.trainable = False

# Add classifier
model = Sequential([
    base,
    GlobalAveragePooling2D(),
    Dense(num_classes, activation='softmax')
])

# Train
model.compile(optimizer='adam', loss='categorical_crossentropy')
model.fit(train_data, epochs=10)
```

### Template 2: Fine-Tuning
```python
# Load base
base = ResNet50(weights='imagenet', include_top=False)

# Freeze first 80%, unfreeze last 20%
base.trainable = True
for layer in base.layers[:40]:
    layer.trainable = False

# Build model
model = Sequential([base, GlobalAveragePooling2D(), Dense(num_classes, activation='softmax')])

# Train with LOWER learning rate
model.compile(optimizer=tf.keras.optimizers.Adam(1e-5), loss='categorical_crossentropy')
model.fit(train_data, epochs=20)
```

---

## Common Mistakes (Avoid These!)

1. âŒ Using high learning rate for fine-tuning
   - âœ… Use 10-100Ã— smaller LR (1e-5 instead of 1e-3)

2. âŒ Unfreezing all layers with small data
   - âœ… Freeze at least 50-80% of base model

3. âŒ Not using data augmentation
   - âœ… ALWAYS augment with transfer learning

4. âŒ Using VGG for mobile deployment
   - âœ… Use MobileNetV2 for mobile/edge

5. âŒ Skipping feature extraction and going straight to fine-tuning
   - âœ… Try feature extraction first (faster baseline)

---

## What's Next? (Preview of Tutorial T12)

**Next Session (Nov-10 Monday DO4):**
- **Tutorial T12:** Hands-on Transfer Learning
- **Dataset:** Cats vs Dogs (Kaggle)
- **Task:** Build classifier with both strategies
- **Compare:**
  - Train from scratch: 65% accuracy (overfits)
  - VGG16 feature extraction: 92% accuracy
  - ResNet50 fine-tuning: 95% accuracy

**Assignment:**
- Try transfer learning on YOUR own dataset
- Compare feature extraction vs fine-tuning
- Submit comparison report (10 points)

---

## FT2 Preparation (Nov-14)

**Transfer Learning Topics in FT2:**

**Expected MCQs:**
- When to use feature extraction vs fine-tuning?
- Which model for mobile deployment?
- Why does transfer learning reduce data requirements?
- ImageNet dataset details

**Expected SAQs (5 marks):**
- Explain transfer learning with example
- Compare VGG16 vs ResNet50 vs MobileNetV2
- Design transfer learning strategy for given scenario
- Code: Load and freeze pre-trained model

**Study Focus:**
- âœ… Decision matrices (strategy and model selection)
- âœ… Key model specifications (size, accuracy)
- âœ… Code syntax (loading models, freezing layers)
- âœ… Real-world use cases

---

## ğŸ“š Recommended Resources

### Official Documentation
- **TensorFlow Applications:** https://www.tensorflow.org/api_docs/python/tf/keras/applications
- **Keras Transfer Learning Guide:** https://keras.io/guides/transfer_learning/

### Tutorials
- **Google Colab Transfer Learning:** Search "TensorFlow transfer learning colab"
- **Fast.ai Practical Deep Learning:** https://course.fast.ai/

### Papers (Optional Advanced Reading)
- VGG: "Very Deep Convolutional Networks" (2014)
- ResNet: "Deep Residual Learning" (2015)
- MobileNetV2: "Inverted Residuals and Linear Bottlenecks" (2018)

### Datasets for Practice
- **Dogs vs Cats:** Kaggle (25,000 images)
- **Flowers:** TensorFlow Datasets (3,670 images)
- **Food-101:** TensorFlow Datasets (101 food categories)

---

## ğŸ¯ Final Thought

> **"Deep learning is no longer about who has the most data or the biggest GPU. Transfer learning democratized AI - now anyone with 1,000 images and a laptop can build production-grade models."**

**The Power of Transfer Learning:**
- Dr. Anjali saved lives with 500 X-rays
- Rohan helped farmers with 5,000 crop photos
- Maya protected wildlife with phone-based AI

**What will YOU build?**

---

**End of v2 Comprehensive Lecture Notes**

**Session Duration:** 60 minutes
**Total Length:** ~1,500 lines
**Format:** WHY â†’ WHAT â†’ HOW
**Philosophy:** 80-10-10 (Concepts-Code-Math)
**Characters:** Dr. Anjali, Rohan, Maya
**Models Covered:** VGG16, ResNet50, MobileNetV2
**Strategies:** Feature Extraction â„ï¸ + Fine-Tuning ğŸ”¥

**Status:** âœ… Ready for delivery on November 7, 2025

---

**Instructor Notes:**
- Adjust timing based on class engagement
- Prioritize hands-on code demos over theory
- Use character stories to maintain interest
- Emphasize practical decision-making (when to use what)
- Show model.summary() output for each model
- Prepare backup slides in case of time overrun

**Student Success Metrics:**
- Can explain transfer learning benefits
- Can choose appropriate strategy and model
- Can write 5 lines of code to load pre-trained model
- Can make informed decisions for their own projects
