# Transfer Learning - Interactive Notebooks

**Course:** 21CSE558T - Deep Neural Network Architectures
**Module 4:** CNNs & Transfer Learning (Week 12)
**Date:** November 7, 2025
**Instructor:** Prof. Ramesh Babu

---

## ğŸ“š Overview

This directory contains **4 interactive Google Colab notebooks** that teach transfer learning through hands-on experimentation. Students will progress from understanding the problem to mastering three different transfer learning strategies.

**Total Time:** 35-40 minutes of hands-on content
**Dataset:** TensorFlow Flowers (3,670 images, 5 classes)
**Models:** VGG16, ResNet50, MobileNetV2

---

## ğŸ““ Notebook Sequence

### **01_the_problem_why_transfer_learning.ipynb**
**Duration:** 5-7 minutes
**Difficulty:** Beginner

**What You'll Learn:**
- Why training from scratch fails with small datasets
- The data hunger problem in deep learning
- Visualization: Accuracy vs Dataset Size

**Key Demonstration:**
- Train small CNN from scratch: **45-55% accuracy** (overfits!)
- Motivation: "We need a better approach..."

**Run This:** [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/your-repo/01_the_problem_why_transfer_learning.ipynb)

---

### **02_feature_extraction_the_fast_solution.ipynb**
**Duration:** 8-10 minutes
**Difficulty:** Beginner

**What You'll Learn:**
- How to use pre-trained ResNet50
- Feature extraction strategy (freeze base model)
- Dramatic accuracy improvement with same data

**Key Demonstration:**
- Load ResNet50 pre-trained on ImageNet (1.2M images)
- Freeze 25M parameters, train only final classifier
- Result: **88-92% accuracy** in 3 minutes! ğŸ‰

**The "Magic Moment":** Watch accuracy jump from 45% â†’ 90%

**Run This:** [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/your-repo/02_feature_extraction.ipynb)

---

### **03_fine_tuning_going_deeper.ipynb**
**Duration:** 10-12 minutes
**Difficulty:** Intermediate

**What You'll Learn:**
- Fine-tuning strategy (unfreeze top layers)
- When to use fine-tuning vs feature extraction
- Importance of lower learning rates

**Key Demonstration:**
- Start with feature extraction results (90%)
- Unfreeze top 20% of ResNet50 layers
- Fine-tune with lower LR (1e-5)
- Result: **92-95% accuracy** (+2-5% improvement)

**Comparison:**
- From Scratch: 45% accuracy
- Feature Extraction: 90% accuracy
- Fine-Tuning: 93% accuracy â­

**Run This:** [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/your-repo/03_fine_tuning.ipynb)

---

### **04_model_zoo_choosing_the_right_model.ipynb**
**Duration:** 10-12 minutes
**Difficulty:** Intermediate

**What You'll Learn:**
- Compare VGG16, ResNet50, MobileNetV2
- Model selection based on constraints
- Speed vs accuracy tradeoffs

**Key Demonstration:**
- Train same task with all 3 models
- Compare: size, accuracy, training time, inference speed
- Decision flowchart: Which model for which scenario?

**Results Table:**
| Model | Size | Accuracy | Time | Best For |
|-------|------|----------|------|----------|
| VGG16 | 528 MB | 89% | 4 min | Learning |
| ResNet50 | 98 MB | 91% | 3 min | Default â­ |
| MobileNetV2 | 14 MB | 87% | 2 min | Mobile |

**Run This:** [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/your-repo/04_model_zoo.ipynb)

---

## ğŸ¯ Learning Path

```
START HERE
    â†“
[01] The Problem
    â”œâ”€ Why training from scratch fails?
    â””â”€ Result: 45% accuracy (bad!)
    â†“
[02] Feature Extraction â­ MOST IMPORTANT
    â”œâ”€ Freeze base, train classifier
    â””â”€ Result: 90% accuracy (great!)
    â†“
[03] Fine-Tuning
    â”œâ”€ Unfreeze some layers
    â””â”€ Result: 93% accuracy (even better!)
    â†“
[04] Model Zoo
    â”œâ”€ Compare different models
    â””â”€ Learn to choose right model
```

---

## ğŸš€ Quick Start Guide

### For Students:

**Option 1: Run All Notebooks (35-40 min)**
1. Open Notebook 01 â†’ Run all cells â†’ Understand the problem
2. Open Notebook 02 â†’ Run all cells â†’ See the solution
3. Open Notebook 03 â†’ Run all cells â†’ Learn advanced technique
4. Open Notebook 04 â†’ Run all cells â†’ Compare models

**Option 2: Essential Path (15 min)**
1. Notebook 01: Understand the problem (5 min)
2. Notebook 02: Learn feature extraction (10 min)
3. Read Notebooks 03-04 later

**Option 3: Homework Assignment**
- In Class: Watch instructor demo Notebooks 01-02
- Homework: Run Notebooks 03-04 yourself
- Submit: Screenshots + answers to questions

---

### For Instructors:

**Recommended Classroom Delivery:**

**Plan A: Full Demo (40 min)**
```
â”œâ”€â”€ 5 min: Notebook 01 (quick run, show the problem)
â”œâ”€â”€ 15 min: Notebook 02 (detailed walkthrough, live)
â”œâ”€â”€ 10 min: Notebook 03 (quick demo)
â””â”€â”€ 10 min: Notebook 04 (show results table)
```

**Plan B: Focused Demo (20 min) â­ RECOMMENDED**
```
â”œâ”€â”€ 5 min: Notebook 01 (show struggle of training from scratch)
â”œâ”€â”€ 15 min: Notebook 02 (live feature extraction demo)
â””â”€â”€ Assign: Notebooks 03-04 as homework
```

**Plan C: Lecture + Quick Demo (10 min)**
```
â”œâ”€â”€ 45 min: Use v2_comprehensive_lecture_notes.md
â””â”€â”€ 10 min: Run Notebook 02 live (show the "magic")
```

---

## ğŸ’» Technical Requirements

### Google Colab (FREE) âœ…
- No installation needed
- Free GPU available (Runtime â†’ Change runtime type â†’ GPU)
- All libraries pre-installed (TensorFlow, Keras, etc.)

### What Students Need:
- Google account (for Colab access)
- Chrome/Firefox browser
- Stable internet connection
- **No GPU required locally** (Colab provides free GPU)

### Download Times (First Run):
- ResNet50 weights: ~10 seconds (98 MB)
- TF Flowers dataset: ~5 seconds (218 MB)
- Total setup: <1 minute

---

## ğŸ“Š Expected Results

### Notebook 01: From Scratch
```
Model: Small CNN (3 Conv layers)
Dataset: 3,000 flower images
Epochs: 10
Result: 45-55% accuracy
Problem: Severe overfitting
Message: "Not enough data!"
```

### Notebook 02: Feature Extraction
```
Model: ResNet50 (frozen) + Classifier
Dataset: Same 3,000 images
Epochs: 5
Result: 88-92% accuracy â­
Improvement: +40-50% absolute!
Message: "Transfer learning works!"
```

### Notebook 03: Fine-Tuning
```
Model: ResNet50 (top 20% unfrozen)
Dataset: Same 3,000 images
Epochs: 10
Result: 92-95% accuracy â­â­
Improvement: +2-3% from feature extraction
Message: "Fine-tuning for final polish"
```

### Notebook 04: Model Comparison
```
All models trained with feature extraction:
â”œâ”€â”€ VGG16:      89% accuracy, 4 min training
â”œâ”€â”€ ResNet50:   91% accuracy, 3 min training â­
â””â”€â”€ MobileNetV2: 87% accuracy, 2 min training (fastest!)
```

---

## ğŸ“ Learning Outcomes

After completing all notebooks, students will be able to:

**Knowledge (CO-4, CO-5):**
- âœ… Explain why transfer learning is necessary
- âœ… Describe how pre-trained models work
- âœ… Differentiate feature extraction from fine-tuning
- âœ… Compare different pre-trained models

**Skills (CO-4, CO-5):**
- âœ… Load pre-trained models in TensorFlow/Keras
- âœ… Freeze/unfreeze layers programmatically
- âœ… Implement feature extraction in 5 lines of code
- âœ… Implement fine-tuning with proper learning rates
- âœ… Choose appropriate model for given constraints

**Application (CO-5):**
- âœ… Apply transfer learning to new datasets
- âœ… Select strategy based on data size
- âœ… Debug common transfer learning issues
- âœ… Optimize for accuracy vs speed tradeoffs

---

## ğŸ”§ Troubleshooting

### Common Issues:

**Issue 1: "Runtime disconnected"**
- **Solution:** Keep browser tab active, reconnect and re-run

**Issue 2: "Model download slow"**
- **Solution:** Be patient (first run only), or use MobileNetV2 (smaller)

**Issue 3: "Out of memory error"**
- **Solution:** Runtime â†’ Factory reset runtime, reduce batch size

**Issue 4: "Accuracy lower than expected"**
- **Solution:** Check GPU is enabled, verify data preprocessing

**Issue 5: "Can't access Colab"**
- **Solution:** Use personal Google account, check firewall settings

---

## ğŸ“ Assignment Ideas

### Assignment 1: Apply Transfer Learning (10 points)
**Task:** Use transfer learning on a different dataset
- Choose dataset: Cats vs Dogs, Food-101, or your own
- Implement feature extraction
- Compare with training from scratch
- Submit: Colab notebook + 1-page report

### Assignment 2: Strategy Comparison (15 points)
**Task:** Compare feature extraction vs fine-tuning
- Use same dataset (e.g., TF Flowers)
- Try both strategies
- Analyze: accuracy, training time, parameters
- Submit: Comparison table + insights

### Assignment 3: Model Selection (10 points)
**Task:** Choose best model for given scenario
- Test VGG16, ResNet50, MobileNetV2
- Measure: accuracy, speed, size
- Recommend model for mobile app deployment
- Justify your choice

### Assignment 4: Real-World Application (20 points)
**Task:** Build transfer learning classifier for domain of interest
- Medical, agriculture, wildlife, fashion, etc.
- Collect/use existing dataset (min 500 images)
- Train with transfer learning
- Submit: Working Colab + demo video

---

## ğŸ“š Additional Resources

### Official Documentation:
- **TensorFlow Transfer Learning Guide:** https://www.tensorflow.org/tutorials/images/transfer_learning
- **Keras Applications API:** https://keras.io/api/applications/
- **TensorFlow Datasets:** https://www.tensorflow.org/datasets

### Research Papers:
- **VGG:** "Very Deep Convolutional Networks" (Simonyan & Zisserman, 2014)
- **ResNet:** "Deep Residual Learning" (He et al., 2015)
- **MobileNetV2:** "Inverted Residuals and Linear Bottlenecks" (Sandler et al., 2018)

### Tutorials:
- **Google Colab Transfer Learning:** Search "TensorFlow transfer learning Colab"
- **Fast.ai Course:** https://course.fast.ai/ (Practical Deep Learning)
- **DeepLearning.AI:** Coursera TensorFlow Specialization

### Datasets for Practice:
- **TensorFlow Datasets Catalog:** https://www.tensorflow.org/datasets/catalog/overview
- **Kaggle Datasets:** https://www.kaggle.com/datasets
- **ImageNet:** https://www.image-net.org/ (for research)

---

## ğŸ¯ Assessment Alignment (FT2)

### Topics Covered in These Notebooks:

**From Formative Test 2 (Nov 14):**

**MCQ Topics:**
- âœ… Transfer learning benefits (data/time/cost reduction)
- âœ… Feature extraction vs fine-tuning comparison
- âœ… Pre-trained model selection (VGG/ResNet/MobileNet)
- âœ… When to freeze vs unfreeze layers
- âœ… Learning rate selection for fine-tuning

**SAQ Topics (5 marks each):**
- âœ… Explain transfer learning with example
- âœ… Compare transfer learning strategies
- âœ… Design transfer learning approach for scenario
- âœ… Model selection with justification

**Expected Questions:**
1. "Given 2,000 medical images, which strategy would you use and why?"
2. "Compare VGG16 and ResNet50 for mobile deployment"
3. "Explain why transfer learning reduces data requirements"
4. "Write code to load and freeze ResNet50"

**These notebooks prepare students for ALL transfer learning questions!**

---

## ğŸ“… Session Timeline

### Week 12 Schedule:

**Nov-7 Friday (DO3 - 100 min):**
- 8:00-8:40: Tutorial T11 - CIFAR-10 Regularization
- 8:40-9:00: Notebooks 01-02 Demo (20 min)
- 9:00-9:40: Transfer Learning Lecture (40 min)

**Nov-10 Monday (DO4 - 40 min):**
- Tutorial T12: Students run Notebooks 03-04
- Q&A and troubleshooting
- Homework assignment briefing

**Nov-14 Friday (DO3 - 100 min):**
- **FORMATIVE TEST 2 (FT2)**
- Modules 3-4 including Transfer Learning

---

## âœ… Checklist for Students

**Before Class (Nov-7):**
- [ ] Have Google account ready
- [ ] Test Colab access (open any public notebook)
- [ ] Bookmark this README
- [ ] Review Week 10-11 CNN concepts

**During Class (Nov-7):**
- [ ] Follow along with instructor demo
- [ ] Run Notebook 01 yourself
- [ ] Run Notebook 02 yourself
- [ ] Take notes on key concepts
- [ ] Ask questions if confused

**After Class (Nov-7-10):**
- [ ] Complete Notebooks 03-04 (homework)
- [ ] Experiment with different datasets
- [ ] Answer reflection questions
- [ ] Prepare questions for Nov-10 session

**Before FT2 (Nov-14):**
- [ ] Review all 4 notebooks
- [ ] Understand decision matrices
- [ ] Practice model selection scenarios
- [ ] Memorize key code patterns

---

## ğŸ† Success Metrics

**You've mastered transfer learning when you can:**

1. **Explain (in 30 seconds):**
   - "Transfer learning reuses features learned from millions of images, so we don't need millions of our own images. We just train the final layer on our small dataset."

2. **Implement (in 5 minutes):**
   ```python
   base = ResNet50(weights='imagenet', include_top=False)
   base.trainable = False
   model = Sequential([base, GlobalAveragePooling2D(), Dense(5, softmax)])
   model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')
   model.fit(train_data, epochs=5)
   ```

3. **Decide (with confidence):**
   - "I have 3,000 images â†’ use feature extraction"
   - "I need mobile deployment â†’ use MobileNetV2"
   - "I want best accuracy â†’ try fine-tuning ResNet50"

**If you can do these 3 things, you're ready for FT2 and real-world projects!**

---

## ğŸ“ Support

**Questions? Issues? Feedback?**

- **Instructor:** Prof. Ramesh Babu
- **Office Hours:** [TBD]
- **Email:** [TBD]
- **Discussion Forum:** [TBD]

**Common Questions:**
- "Which notebook should I start with?" â†’ Start with 01
- "Do I need GPU?" â†’ No, Colab provides free GPU
- "Can I use my own dataset?" â†’ Yes! After completing 01-04
- "How long will this take?" â†’ 35-40 min for all 4 notebooks

---

## ğŸ“„ File Listing

```
notebooks/
â”œâ”€â”€ README.md (this file)
â”œâ”€â”€ 01_the_problem_why_transfer_learning.ipynb
â”œâ”€â”€ 02_feature_extraction_the_fast_solution.ipynb
â”œâ”€â”€ 03_fine_tuning_going_deeper.ipynb
â””â”€â”€ 04_model_zoo_choosing_the_right_model.ipynb
```

**All notebooks are standalone** - you can run them in any order, but recommended sequence is 01 â†’ 02 â†’ 03 â†’ 04.

---

## ğŸ‰ Let's Get Started!

**Ready to experience the power of transfer learning?**

1. Open **Notebook 01** to see the problem
2. Open **Notebook 02** to see the solution (the "magic moment"!)
3. Continue with **Notebooks 03-04** to master advanced techniques

**Remember:** The goal is not just to run code, but to UNDERSTAND why transfer learning is revolutionary. Watch the accuracy numbers jump from 45% â†’ 90% and you'll see why this technique changed deep learning forever!

**Happy Learning! ğŸš€**

---

**Last Updated:** November 3, 2025
**Version:** 1.0
**Status:** âœ… Ready for Nov-7 class
