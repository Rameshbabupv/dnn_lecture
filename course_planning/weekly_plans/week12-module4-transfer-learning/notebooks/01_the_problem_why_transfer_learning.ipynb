{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 01: The Problem - Why Transfer Learning?\n",
        "\n",
        "**Course:** 21CSE558T - Deep Neural Network Architectures  \n",
        "**Module 4:** CNNs & Transfer Learning (Week 12)  \n",
        "**Estimated Time:** 5-7 minutes  \n",
        "**Goal:** Understand why training from scratch fails\n",
        "\n",
        "---\n",
        "\n",
        "## What You'll Learn\n",
        "\n",
        "1. Train a CNN from scratch on small dataset\n",
        "2. Observe poor accuracy and overfitting\n",
        "3. Understand the data hunger problem\n",
        "4. See why we need transfer learning\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TensorFlow version: 2.16.2\n",
            "GPU available: False\n",
            "\n",
            "Setup complete!\n"
          ]
        }
      ],
      "source": [
        "# Setup\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "print(f\"GPU available: {len(tf.config.list_physical_devices('GPU')) > 0}\")\n",
        "print(\"\\nSetup complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Load Dataset\n",
        "\n",
        "We'll use TensorFlow Flowers dataset:\n",
        "- Total: 3,670 images\n",
        "- Classes: 5 (daisy, dandelion, roses, sunflowers, tulips)\n",
        "- **Question:** Can we train CNN from scratch with 3,000 images?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading TF Flowers dataset...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Variant folder /Users/rameshbabu/tensorflow_datasets/tf_flowers/3.0.1 has no dataset_info.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to /Users/rameshbabu/tensorflow_datasets/tf_flowers/3.0.1...\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c94f23f97ae34b74ae25d789147f9ce2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Dl Completed...: 0 url [00:00, ? url/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a15c51fcbb1d4c12a494cdce17d23ed3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Dl Size...: 0 MiB [00:00, ? MiB/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dab355a07c5f4bbf8309f46691603376",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating splits...:   0%|          | 0/1 [00:00<?, ? splits/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "75c46ad349f74c13940669bead0311e1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train examples...: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "218c8dabfd1a4d31b9d6356b3f78b99c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Shuffling /Users/rameshbabu/tensorflow_datasets/tf_flowers/incomplete.426JAJ_3.0.1/tf_flowers-train.tfrecord*.â€¦"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1mDataset tf_flowers downloaded and prepared to /Users/rameshbabu/tensorflow_datasets/tf_flowers/3.0.1. Subsequent calls will reuse this data.\u001b[0m\n",
            "\n",
            "Classes: 5\n",
            "Names: ['dandelion', 'daisy', 'tulips', 'sunflowers', 'roses']\n",
            "Training samples: ~2,900\n",
            "Validation samples: ~770\n"
          ]
        }
      ],
      "source": [
        "# Load flowers dataset\n",
        "print(\"Loading TF Flowers dataset...\")\n",
        "(train_ds, val_ds), info = tfds.load(\n",
        "    'tf_flowers',\n",
        "    split=['train[:80%]', 'train[80%:]'],\n",
        "    as_supervised=True,\n",
        "    with_info=True\n",
        ")\n",
        "\n",
        "num_classes = info.features['label'].num_classes\n",
        "class_names = info.features['label'].names\n",
        "\n",
        "print(f\"\\nClasses: {num_classes}\")\n",
        "print(f\"Names: {class_names}\")\n",
        "print(f\"Training samples: ~2,900\")\n",
        "print(f\"Validation samples: ~770\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Visualize Samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show sample images\n",
        "plt.figure(figsize=(12, 8))\n",
        "for i, (image, label) in enumerate(train_ds.take(9)):\n",
        "    plt.subplot(3, 3, i + 1)\n",
        "    plt.imshow(image.numpy().astype(\"uint8\"))\n",
        "    plt.title(class_names[label.numpy()])\n",
        "    plt.axis('off')\n",
        "plt.suptitle('Sample Flower Images', fontsize=16)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Beautiful flowers! But only 3,000 images total...\")\n",
        "print(\"Is this enough? Let's find out!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Preprocess Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preprocessing\n",
        "IMG_SIZE = 128\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "def preprocess(image, label):\n",
        "    image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\n",
        "    image = image / 255.0\n",
        "    return image, label\n",
        "\n",
        "train_ds = train_ds.map(preprocess).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "val_ds = val_ds.map(preprocess).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "print(f\"Image size: {IMG_SIZE}x{IMG_SIZE}x3\")\n",
        "print(f\"Batch size: {BATCH_SIZE}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Build Simple CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build CNN from scratch\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(IMG_SIZE, IMG_SIZE, 3)),\n",
        "    tf.keras.layers.MaxPooling2D(2),\n",
        "    tf.keras.layers.Conv2D(64, 3, activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2),\n",
        "    tf.keras.layers.Conv2D(128, 3, activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dense(num_classes, activation='softmax')\n",
        "], name='CNN_From_Scratch')\n",
        "\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Train From Scratch\n",
        "\n",
        "**Prediction:** With only 3,000 images, expect:\n",
        "- Low accuracy (45-55%)\n",
        "- Severe overfitting\n",
        "- Not good enough for real use!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train\n",
        "print(\"Training CNN from scratch...\\n\")\n",
        "print(\"This will take 2-3 minutes.\\n\")\n",
        "\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=10,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"\\nTraining complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Analyze Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot results\n",
        "plt.figure(figsize=(14, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='Train', marker='o')\n",
        "plt.plot(history.history['val_accuracy'], label='Val', marker='s')\n",
        "plt.title('Accuracy (Training from Scratch)')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='Train', marker='o')\n",
        "plt.plot(history.history['val_loss'], label='Val', marker='s')\n",
        "plt.title('Loss (Training from Scratch)')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print results\n",
        "final_train_acc = history.history['accuracy'][-1]\n",
        "final_val_acc = history.history['val_accuracy'][-1]\n",
        "gap = final_train_acc - final_val_acc\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"FINAL RESULTS:\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Training Accuracy:   {final_train_acc:.2%}\")\n",
        "print(f\"Validation Accuracy: {final_val_acc:.2%}\")\n",
        "print(f\"Overfitting Gap:     {gap:.2%}\")\n",
        "print(\"=\"*50)\n",
        "print(\"\\nTHE PROBLEM:\")\n",
        "print(f\"  Low validation accuracy ({final_val_acc:.0%})\")\n",
        "print(f\"  Large overfitting gap ({gap:.0%})\")\n",
        "print(\"\\nDIAGNOSIS: NOT ENOUGH DATA!\")\n",
        "print(\"We need MILLIONS of images.\")\n",
        "print(\"We only have 3,000.\")\n",
        "print(\"\\nSOLUTION: Transfer Learning!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: The Data Hunger Problem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize data requirements\n",
        "dataset_sizes = [500, 1000, 3000, 10000, 50000, 100000, 1000000]\n",
        "accuracy_scratch = [25, 30, 50, 65, 78, 85, 92]\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(dataset_sizes, accuracy_scratch, marker='o', linewidth=2, label='From Scratch')\n",
        "plt.axvline(x=3000, color='red', linestyle='--', linewidth=2, label='Your Data (3,000)')\n",
        "plt.axhline(y=50, color='orange', linestyle='--', alpha=0.5)\n",
        "\n",
        "plt.scatter([3000], [50], color='red', s=200, zorder=5)\n",
        "plt.text(3500, 53, 'You are here: 50% accuracy', fontsize=12, color='red')\n",
        "plt.text(100000, 87, 'Need 100K+ images for 85%!', fontsize=11, color='green')\n",
        "\n",
        "plt.xscale('log')\n",
        "plt.xlabel('Dataset Size (images)')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.title('Data Hunger Problem: Training From Scratch')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nKEY INSIGHT:\")\n",
        "print(\"For 85% accuracy, you need 100,000+ images (33x more!)\")\n",
        "print(\"\\nREALITY: Most people don't have this much data!\")\n",
        "print(\"\\nSOLUTION: Transfer Learning - use pre-trained models!\")\n",
        "print(\"\\nNext: Notebook 02 will show 90% accuracy with same 3,000 images!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Summary\n",
        "\n",
        "### The Problem:\n",
        "- Small dataset (3,000 images) = poor results\n",
        "- Only 45-55% validation accuracy\n",
        "- Severe overfitting\n",
        "- Need 100,000-1,000,000 images for good results\n",
        "\n",
        "### The Question:\n",
        "**How can we build accurate models without millions of images?**\n",
        "\n",
        "### The Answer:\n",
        "**Transfer Learning!** See Notebook 02\n",
        "\n",
        "---\n",
        "\n",
        "## Next Steps\n",
        "\n",
        "Open **Notebook 02: Feature Extraction** to see:\n",
        "- How to use ResNet50 pre-trained on ImageNet\n",
        "- How to get **88-92% accuracy** with same 3,000 images!\n",
        "- The improvement: 45% â†’ 90% (2x better!)\n",
        "\n",
        "**Ready for the magic?** ðŸŽ‰"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
