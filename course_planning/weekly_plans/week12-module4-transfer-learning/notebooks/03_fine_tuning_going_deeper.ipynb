{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 03: Fine-Tuning - Going Deeper\n",
        "\n",
        "**Course:** 21CSE558T - Deep Neural Network Architectures  \n",
        "**Module 4:** CNNs & Transfer Learning (Week 12)  \n",
        "**Estimated Time:** 10-12 minutes  \n",
        "**Prerequisites:** Notebook 02  \n",
        "**Goal:** Unfreeze top layers for 92-95% accuracy\n",
        "\n",
        "---\n",
        "\n",
        "## \ud83d\udcda What You'll Learn\n",
        "\n",
        "In this notebook, you will:\n",
        "1. Start with feature extraction baseline (90% accuracy)\n",
        "2. Unfreeze top 20% of ResNet50 layers\n",
        "3. Fine-tune with lower learning rate (1e-5)\n",
        "4. Achieve **92-95% accuracy** (extra 2-5% improvement!)\n",
        "\n",
        "**Key Concept:** _\"Carefully adjust pre-trained features for your specific task\"_\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense\n",
        "\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "print(\"Ready for fine-tuning!\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Load Dataset (Same as Before)\n",
        "\n",
        "We'll use the same TF Flowers dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load dataset\n",
        "(train_ds, val_ds), info = tfds.load(\n",
        "    'tf_flowers',\n",
        "    split=['train[:80%]', 'train[80%:]'],\n",
        "    as_supervised=True,\n",
        "    with_info=True\n",
        ")\n",
        "\n",
        "num_classes = info.features['label'].num_classes\n",
        "class_names = info.features['label'].names\n",
        "\n",
        "# Preprocess\n",
        "IMG_SIZE = 224\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "def preprocess(image, label):\n",
        "    image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\n",
        "    # Use ResNet50-specific preprocessing\n",
        "    image = tf.keras.applications.resnet50.preprocess_input(image)\n",
        "    return image, label\n",
        "\n",
        "train_ds = train_ds.map(preprocess).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "val_ds = val_ds.map(preprocess).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "print(\"Dataset ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Build Model with Unfrozen Top Layers\n",
        "\n",
        "**Key Difference from Notebook 02:**\n",
        "- Notebook 02: ALL layers frozen\n",
        "- Notebook 03: Top 20% layers UNFROZEN\n",
        "\n",
        "**Strategy:**\n",
        "1. Load ResNet50 with ImageNet weights\n",
        "2. Set `base_model.trainable = True` (enable training)\n",
        "3. Freeze ONLY the first 80% of layers\n",
        "4. Let top 20% layers adjust to our flower images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load ResNet50\n",
        "base_model = ResNet50(\n",
        "    weights='imagenet',\n",
        "    include_top=False,\n",
        "    input_shape=(IMG_SIZE, IMG_SIZE, 3)\n",
        ")\n",
        "\n",
        "# Enable training for the base model\n",
        "base_model.trainable = True\n",
        "\n",
        "# Count total layers\n",
        "total_layers = len(base_model.layers)\n",
        "freeze_until = int(0.8 * total_layers)  # Freeze first 80%\n",
        "\n",
        "# Freeze first 80% of layers\n",
        "for layer in base_model.layers[:freeze_until]:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Count trainable layers\n",
        "trainable_layers = sum([layer.trainable for layer in base_model.layers])\n",
        "\n",
        "print(f\"Total layers: {total_layers}\")\n",
        "print(f\"Frozen layers: {freeze_until} (80%)\")\n",
        "print(f\"Trainable layers: {trainable_layers} (20%)\")\n",
        "print(\"\\nTop 20% of ResNet50 is now unfrozen!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Build Complete Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build model\n",
        "model = Sequential([\n",
        "    base_model,\n",
        "    GlobalAveragePooling2D(),\n",
        "    Dense(num_classes, activation='softmax')\n",
        "], name='ResNet50_FineTuning')\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Compile with LOWER Learning Rate\n",
        "\n",
        "**CRITICAL:** Use 100\u00d7 lower learning rate!\n",
        "\n",
        "- Feature extraction (Notebook 02): LR = 1e-3 (default Adam)\n",
        "- Fine-tuning (Notebook 03): LR = 1e-5 (100\u00d7 smaller)\n",
        "\n",
        "**Why lower LR?**\n",
        "- Pre-trained weights are already good\n",
        "- We want to make small adjustments, not destroy them\n",
        "- High LR would break the learned features!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compile with LOWER learning rate\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(1e-5),  # 100\u00d7 smaller than default!\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "print(\"Model compiled with learning rate: 1e-5\")\n",
        "print(\"(100\u00d7 lower than feature extraction!)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Fine-Tune!\n",
        "\n",
        "**Expected Results:**\n",
        "- Start: 90% accuracy (inherited from ImageNet features)\n",
        "- After 10 epochs: 92-95% accuracy\n",
        "- Improvement: Extra 2-5% from fine-tuning!\n",
        "\n",
        "**Training time:** ~5-7 minutes (slower than feature extraction)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train!\n",
        "print(\"\ud83d\udd25 Fine-tuning ResNet50...\\n\")\n",
        "print(\"Watch accuracy improve beyond 90%!\\n\")\n",
        "\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=10,  # More epochs than feature extraction\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"\\n\u2705 Fine-tuning complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Compare All Three Approaches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot results\n",
        "plt.figure(figsize=(14, 5))\n",
        "\n",
        "# Accuracy\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='Train', marker='o')\n",
        "plt.plot(history.history['val_accuracy'], label='Val', marker='s')\n",
        "plt.axhline(y=0.50, color='red', linestyle='--', label='Scratch (50%)', alpha=0.7)\n",
        "plt.axhline(y=0.90, color='orange', linestyle='--', label='Feature Extraction (90%)', alpha=0.7)\n",
        "plt.title('Fine-Tuning: Best Accuracy!', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Loss\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='Train', marker='o')\n",
        "plt.plot(history.history['val_loss'], label='Val', marker='s')\n",
        "plt.title('Fine-Tuning: Lower Loss', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print final comparison\n",
        "final_val_acc = history.history['val_accuracy'][-1]\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"\ud83c\udfaf FINAL COMPARISON - ALL THREE APPROACHES\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\n{'Approach':<30} {'Val Accuracy':<20} {'Improvement'}\")\n",
        "print(\"-\"*70)\n",
        "print(f\"{'Notebook 01 (Scratch)':<30} {'~50%':<20} {'Baseline'}\")\n",
        "print(f\"{'Notebook 02 (Feature Extract)':<30} {'~90%':<20} {'+40%'}\")\n",
        "print(f\"{'Notebook 03 (Fine-Tuning)':<30} {f'{final_val_acc:.1%}':<20} {f'+{(final_val_acc - 0.50)*100:.0f}%'}\")\n",
        "print(\"-\"*70)\n",
        "print(f\"\\n\u2728 Fine-tuning achieved: {final_val_acc:.1%}\")\n",
        "print(f\"   Extra improvement: {(final_val_acc - 0.90)*100:.1f}% over feature extraction\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: When to Use Each Strategy?\n",
        "\n",
        "**Decision Matrix:**\n",
        "\n",
        "| Dataset Size | Time Budget | Accuracy Need | Strategy |\n",
        "|--------------|-------------|---------------|----------|\n",
        "| < 1,000 | Low | Moderate | Feature Extraction \u2b50 |\n",
        "| 1,000-10,000 | Medium | High | Fine-Tuning \u2b50\u2b50 |\n",
        "| > 10,000 | High | Maximum | Fine-Tuning + More layers |\n",
        "| > 100,000 | High | Maximum | Consider training from scratch |\n",
        "\n",
        "**For our TF Flowers (3,000 images):**\n",
        "- \u2705 Fine-tuning is PERFECT choice!\n",
        "- Gets us 92-95% accuracy\n",
        "- Only takes 5-7 minutes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## \ud83c\udf93 Summary: What You Learned\n",
        "\n",
        "### Fine-Tuning Strategy:\n",
        "1. \u2705 **Start with pre-trained model:** ResNet50 with ImageNet\n",
        "2. \u2705 **Unfreeze top layers:** Top 20% can adjust\n",
        "3. \u2705 **Use LOWER learning rate:** 1e-5 (100\u00d7 smaller)\n",
        "4. \u2705 **Train longer:** 10 epochs instead of 5\n",
        "5. \u2705 **Get extra 2-5%:** From 90% to 93%!\n",
        "\n",
        "### Comparison:\n",
        "| Aspect | Feature Extraction | Fine-Tuning |\n",
        "|--------|-------------------|-------------|\n",
        "| Freeze | All layers | First 80% only |\n",
        "| Learning Rate | 1e-3 (default) | 1e-5 (lower) |\n",
        "| Epochs | 5 | 10 |\n",
        "| Training Time | 3 min | 5-7 min |\n",
        "| Accuracy | 88-92% | 92-95% |\n",
        "| Best For | <1K images | 1K-10K images |\n",
        "\n",
        "### Key Insight:\n",
        "**\"Fine-tuning = gentle adjustments to pre-trained features\"**\n",
        "\n",
        "- Don't destroy ImageNet knowledge\n",
        "- Just adapt it slightly for your domain\n",
        "- Lower LR is CRITICAL!\n",
        "\n",
        "---\n",
        "\n",
        "## \u2705 Key Takeaways\n",
        "\n",
        "- \u2705 **When to fine-tune:** 1K-10K images, need high accuracy\n",
        "- \u2705 **How to fine-tune:** Unfreeze top 20%, use LR = 1e-5\n",
        "- \u2705 **Why lower LR:** Prevent destroying learned features\n",
        "- \u2705 **Expected gain:** Extra 2-5% over feature extraction\n",
        "\n",
        "---\n",
        "\n",
        "## \ud83d\ude80 Next Steps\n",
        "\n",
        "**Ready to compare different models?**\n",
        "\n",
        "\ud83d\udc49 Open **Notebook 04** to compare:\n",
        "- VGG16 (simple but large)\n",
        "- ResNet50 (default choice)\n",
        "- MobileNetV2 (fast and small)\n",
        "\n",
        "Learn which model to use for which scenario!\n",
        "\n",
        "---\n",
        "\n",
        "**End of Notebook 03**\n",
        "\n",
        "**Status:** \u2705 Fine-tuning mastered!\n",
        "\n",
        "**Achievement Unlocked:** \ud83c\udfc6 93%+ accuracy with fine-tuning\n",
        "\n",
        "**Time spent:** ~10-12 minutes\n",
        "\n",
        "**Next:** Notebook 04 - Model Zoo \ud83e\udd81"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}