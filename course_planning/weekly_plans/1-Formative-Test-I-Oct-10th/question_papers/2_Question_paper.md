# Question Paper 2 - Formative Test I
**Course Code**: 21CSE558T  
**Course Title**: Deep Neural Network Architectures  
**Test Duration**: 50 minutes  
**Total Marks**: 25  
**Coverage**: Modules 1-2 (Weeks 1-6)

---

## Instructions:
1. Answer ALL questions in Part A (MCQs)
2. Answer ANY THREE questions from Part B (SAQs)
3. Each MCQ carries 1 mark, each SAQ carries 5 marks
4. Total marks: 25 (Part A: 10 marks + Part B: 15 marks)

---

# PART A: Multiple Choice Questions (10 × 1 = 10 marks)

### Q1. In TensorFlow, which data structure is the fundamental building block?
**Module**: 1 | **Difficulty**: Easy | **CO**: CO-1 | **Source**: MCQ Q2

a) Array  
b) Matrix  
c) Tensor  
d) Vector

---

### Q2. Which activation function can output negative values?
**Module**: 1 | **Difficulty**: Easy | **CO**: CO-1 | **Source**: MCQ Q5

a) Sigmoid  
b) ReLU  
c) Tanh  
d) Softmax

---

### Q3. The backpropagation algorithm uses which mathematical concept?
**Module**: 1 | **Difficulty**: Moderate | **CO**: CO-1 | **Source**: MCQ Q12

a) Integration  
b) Chain rule of differentiation  
c) Matrix multiplication only  
d) Linear algebra only

---

### Q4. In a multilayer perceptron, the universal approximation theorem states:
**Module**: 1 | **Difficulty**: Moderate | **CO**: CO-1 | **Source**: MCQ Q15

a) Any function can be approximated with infinite layers  
b) A single hidden layer can approximate any continuous function  
c) Only linear functions can be approximated  
d) Approximation is impossible with finite neurons

---

### Q5. Learning rate determines:
**Module**: 2 | **Difficulty**: Easy | **CO**: CO-2 | **Source**: MCQ Q22

a) Number of epochs  
b) Size of steps toward minimum  
c) Number of hidden layers  
d) Batch size

---

### Q6. Which regularization technique randomly sets some neurons to zero during training?
**Module**: 2 | **Difficulty**: Easy | **CO**: CO-2 | **Source**: MCQ Q24

a) L1 regularization  
b) L2 regularization  
c) Dropout  
d) Early stopping

---

### Q7. L2 regularization adds which penalty to the loss function?
**Module**: 2 | **Difficulty**: Moderate | **CO**: CO-2 | **Source**: MCQ Q33

a) Sum of absolute values of weights  
b) Sum of squared weights  
c) Product of weights  
d) Maximum weight value

---

### Q8. The momentum parameter in gradient descent:
**Module**: 2 | **Difficulty**: Moderate | **CO**: CO-2 | **Source**: MCQ Q36

a) Increases the learning rate  
b) Accelerates convergence in relevant directions  
c) Prevents overfitting  
d) Normalizes the inputs

---

### Q9. The bias-variance tradeoff is related to:
**Module**: 2 | **Difficulty**: Difficult | **CO**: CO-2 | **Source**: MCQ Q43

a) Computational complexity  
b) Overfitting and underfitting  
c) Memory usage  
d) Training time

---

### Q10. Which technique can help with both vanishing and exploding gradients?
**Module**: 2 | **Difficulty**: Difficult | **CO**: CO-2 | **Source**: MCQ Q44

a) Dropout  
b) Residual connections  
c) L2 regularization  
d) Early stopping

---

# PART B: Short Answer Questions (Answer any 3 × 5 = 15 marks)

### Q11. You build a deep neural network but use only linear activation functions. Despite having multiple layers, the network performs like a simple linear model. Explain why this happens.
**Module**: 1 | **Week**: 3 | **Difficulty**: Easy | **CO**: CO-1 | **Source**: 7-SAQ Q2

---

### Q12. You replace sigmoid activations with ReLU in your deep network and observe faster training and better performance. Explain what causes this improvement.
**Module**: 1 | **Week**: 3 | **Difficulty**: Moderate | **CO**: CO-1 | **Source**: 7-SAQ Q3

---

### Q13. Your model achieves 99% accuracy on training data but only 65% on test data. Explain what problem this indicates and why it occurs.
**Module**: 2 | **Week**: 6 | **Difficulty**: Easy | **CO**: CO-2 | **Source**: 7-SAQ Q4

---

### Q14. You switch from batch gradient descent to stochastic gradient descent and observe that training becomes noisier but sometimes achieves better final results. Explain this trade-off.
**Module**: 2 | **Week**: 4 | **Difficulty**: Moderate | **CO**: CO-2 | **Source**: 7-SAQ Q5

---

### Q15. You apply L1 regularization to your network and find that many weights become exactly zero, while L2 regularization only makes weights smaller. Explain what causes this fundamental difference.
**Module**: 2 | **Week**: 6 | **Difficulty**: Moderate | **CO**: CO-2 | **Source**: 7-SAQ Q6

---

## Distribution Summary:
- **MCQs**: Module 1: 4, Module 2: 6 | Easy: 4, Moderate: 4, Difficult: 2
- **SAQs**: Module 1: 2, Module 2: 3 | Easy: 2, Moderate: 3, Difficult: 0
- **Course Outcomes**: CO-1: 6, CO-2: 9
- **Total Marks**: 25 | **Time**: 50 minutes
