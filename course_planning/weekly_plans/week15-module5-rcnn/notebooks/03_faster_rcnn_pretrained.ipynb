{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 03: Faster R-CNN Pre-trained Implementation\n",
    "\n",
    "**Week 15 - Module 5: Object Detection - Tutorial T15**\n",
    "\n",
    "**Duration:** ~20 minutes\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Use pre-trained Faster R-CNN for object detection\n",
    "- Understand Region Proposal Network (RPN) outputs\n",
    "- Analyze detection results and confidence scores\n",
    "- Compare Faster R-CNN with YOLO performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Faster R-CNN Architecture Overview\n",
    "\n",
    "### Complete Pipeline:\n",
    "\n",
    "```\n",
    "Input Image (H × W × 3)\n",
    "    |\n",
    "    v\n",
    "Backbone CNN (ResNet50 + FPN)\n",
    "    ├─ C2: 256×256 feature map\n",
    "    ├─ C3: 128×128 feature map  \n",
    "    ├─ C4: 64×64 feature map\n",
    "    └─ C5: 32×32 feature map\n",
    "    |\n",
    "    v\n",
    "Region Proposal Network (RPN)\n",
    "    ├─ 3×3 conv → 512 channels\n",
    "    ├─ 1×1 conv → Objectness (2k scores)\n",
    "    └─ 1×1 conv → Box deltas (4k values)\n",
    "    |\n",
    "    v\n",
    "Proposal Generation\n",
    "    ├─ Generate ~20,000 anchor boxes\n",
    "    ├─ Filter by objectness score\n",
    "    ├─ Apply NMS → ~2,000 proposals\n",
    "    └─ Select top 300 for detection\n",
    "    |\n",
    "    v\n",
    "ROI Align (7×7×512 per proposal)\n",
    "    |\n",
    "    v\n",
    "Detection Head\n",
    "    ├─ Fully connected layers\n",
    "    ├─ Classification branch → C+1 classes\n",
    "    └─ Box regression branch → 4 coordinates\n",
    "    |\n",
    "    v\n",
    "Post-processing\n",
    "    ├─ Filter by confidence threshold\n",
    "    ├─ Apply class-wise NMS\n",
    "    └─ Output final detections\n",
    "```\n",
    "\n",
    "### Key Components:\n",
    "\n",
    "#### 1. Backbone (ResNet50 + FPN)\n",
    "- **ResNet50**: Deep residual network for feature extraction\n",
    "- **FPN (Feature Pyramid Network)**: Multi-scale features\n",
    "  - Helps detect objects at different sizes\n",
    "  - P2 (high-res) for small objects\n",
    "  - P5 (low-res) for large objects\n",
    "\n",
    "#### 2. Region Proposal Network (RPN)\n",
    "- **Anchor boxes**: 3 scales × 3 ratios = 9 anchors per position\n",
    "  - Scales: {32², 64², 128²} pixels\n",
    "  - Ratios: {1:1, 1:2, 2:1}\n",
    "- **Objectness score**: Binary classification (object vs background)\n",
    "- **Box regression**: Refine anchor positions\n",
    "\n",
    "#### 3. ROI Align\n",
    "- Improved version of ROI Pooling\n",
    "- Uses bilinear interpolation (no quantization)\n",
    "- Outputs fixed 7×7 feature map per proposal\n",
    "\n",
    "#### 4. Detection Head\n",
    "- **Classification**: Softmax over C+1 classes (C classes + background)\n",
    "- **Box refinement**: Further adjust proposal coordinates\n",
    "- **Output**: (class, confidence, box) per detection\n",
    "\n",
    "### Training Details:\n",
    "\n",
    "**RPN Loss**:\n",
    "```\n",
    "L_rpn = L_cls(objectness) + λ * L_reg(box_deltas)\n",
    "```\n",
    "\n",
    "**Detection Loss**:\n",
    "```\n",
    "L_det = L_cls(class) + λ * L_reg(box_refinement)\n",
    "```\n",
    "\n",
    "**Total Loss**:\n",
    "```\n",
    "L_total = L_rpn + L_det\n",
    "```\n",
    "\n",
    "### Pre-trained Model:\n",
    "\n",
    "We'll use **fasterrcnn_resnet50_fpn** pre-trained on COCO dataset:\n",
    "- **80 object classes**: person, car, dog, etc.\n",
    "- **118k training images**\n",
    "- **mAP**: ~37% on COCO validation\n",
    "- **Speed**: ~5-10 FPS on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and Imports\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "import time\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Torchvision version: {torchvision.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# COCO class names (80 classes)\n",
    "COCO_CLASSES = [\n",
    "    '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n",
    "    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A', 'stop sign',\n",
    "    'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n",
    "    'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack', 'umbrella', 'N/A', 'N/A',\n",
    "    'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n",
    "    'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',\n",
    "    'bottle', 'N/A', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl',\n",
    "    'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n",
    "    'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table',\n",
    "    'N/A', 'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',\n",
    "    'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A', 'book',\n",
    "    'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n",
    "]\n",
    "\n",
    "print(f\"\\nCOCO dataset has {len(COCO_CLASSES)-1} object classes (excluding background)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Pre-trained Faster R-CNN Model\n",
    "\n",
    "print(\"Loading pre-trained Faster R-CNN (ResNet50 + FPN)...\")\n",
    "print(\"This may take a moment to download (~160 MB)\\n\")\n",
    "\n",
    "# Load model with pre-trained weights\n",
    "model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "# Set to evaluation mode (disables dropout, batch norm training mode)\n",
    "model.eval()\n",
    "\n",
    "# Move to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "print(f\"✓ Model loaded successfully on {device}\")\n",
    "print(f\"\\nModel Architecture:\")\n",
    "print(f\"  Backbone: ResNet50 with Feature Pyramid Network (FPN)\")\n",
    "print(f\"  RPN: Region Proposal Network with anchor boxes\")\n",
    "print(f\"  ROI Head: ROI Align + Detection head\")\n",
    "print(f\"  Classes: {len(COCO_CLASSES)-1} COCO categories\")\n",
    "\n",
    "# Model statistics\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nModel Statistics:\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"  Model size: ~160 MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Preprocessing Functions\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    \"\"\"\n",
    "    Load and preprocess image for Faster R-CNN\n",
    "    \n",
    "    Args:\n",
    "        image_path: Path to input image\n",
    "    \n",
    "    Returns:\n",
    "        img_tensor: Preprocessed tensor [C, H, W]\n",
    "        img_rgb: Original image in RGB format for visualization\n",
    "    \"\"\"\n",
    "    # Read image\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        raise ValueError(f\"Could not read image: {image_path}\")\n",
    "    \n",
    "    # Convert BGR to RGB\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Convert to tensor [C, H, W] and normalize to [0, 1]\n",
    "    img_tensor = torch.from_numpy(img_rgb).permute(2, 0, 1).float() / 255.0\n",
    "    \n",
    "    return img_tensor, img_rgb\n",
    "\n",
    "\n",
    "def create_sample_image():\n",
    "    \"\"\"\n",
    "    Create a sample test image with simple shapes\n",
    "    \"\"\"\n",
    "    img = np.ones((480, 640, 3), dtype=np.uint8) * 255\n",
    "    \n",
    "    # Draw some shapes\n",
    "    cv2.rectangle(img, (50, 50), (200, 200), (255, 0, 0), -1)\n",
    "    cv2.rectangle(img, (300, 100), (500, 250), (0, 255, 0), -1)\n",
    "    cv2.circle(img, (400, 350), 80, (0, 0, 255), -1)\n",
    "    \n",
    "    # Convert to tensor\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img_tensor = torch.from_numpy(img_rgb).permute(2, 0, 1).float() / 255.0\n",
    "    \n",
    "    return img_tensor, img_rgb\n",
    "\n",
    "# Test preprocessing\n",
    "print(\"Testing image preprocessing...\")\n",
    "test_tensor, test_rgb = create_sample_image()\n",
    "print(f\"  Tensor shape: {test_tensor.shape}\")\n",
    "print(f\"  Tensor dtype: {test_tensor.dtype}\")\n",
    "print(f\"  Value range: [{test_tensor.min():.2f}, {test_tensor.max():.2f}]\")\n",
    "print(f\"  RGB array shape: {test_rgb.shape}\")\n",
    "print(\"✓ Preprocessing working correctly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Object Detection\n",
    "\n",
    "def detect_objects(model, img_tensor, device, conf_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Run Faster R-CNN detection on image\n",
    "    \n",
    "    Args:\n",
    "        model: Faster R-CNN model\n",
    "        img_tensor: Preprocessed image tensor [C, H, W]\n",
    "        device: torch device (cuda or cpu)\n",
    "        conf_threshold: Confidence threshold for filtering detections\n",
    "    \n",
    "    Returns:\n",
    "        predictions: Dictionary with boxes, labels, scores\n",
    "        inference_time: Time taken for inference (seconds)\n",
    "    \"\"\"\n",
    "    # Move image to device\n",
    "    img_tensor = img_tensor.to(device)\n",
    "    \n",
    "    # Run inference\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        predictions = model([img_tensor])\n",
    "    inference_time = time.time() - start_time\n",
    "    \n",
    "    # Extract predictions for first (and only) image\n",
    "    pred = predictions[0]\n",
    "    \n",
    "    # Filter by confidence threshold\n",
    "    keep_idx = pred['scores'] > conf_threshold\n",
    "    filtered_pred = {\n",
    "        'boxes': pred['boxes'][keep_idx].cpu().numpy(),\n",
    "        'labels': pred['labels'][keep_idx].cpu().numpy(),\n",
    "        'scores': pred['scores'][keep_idx].cpu().numpy()\n",
    "    }\n",
    "    \n",
    "    return filtered_pred, inference_time\n",
    "\n",
    "# Test detection\n",
    "print(\"Running test detection...\")\n",
    "img_tensor, img_rgb = create_sample_image()\n",
    "predictions, inf_time = detect_objects(model, img_tensor, device, conf_threshold=0.5)\n",
    "\n",
    "print(f\"\\nDetection Results:\")\n",
    "print(f\"  Inference time: {inf_time*1000:.1f} ms\")\n",
    "print(f\"  FPS: {1/inf_time:.1f}\")\n",
    "print(f\"  Detections (conf > 0.5): {len(predictions['boxes'])}\")\n",
    "\n",
    "if len(predictions['boxes']) > 0:\n",
    "    print(f\"\\nTop detections:\")\n",
    "    for i in range(min(5, len(predictions['boxes']))):\n",
    "        label = COCO_CLASSES[predictions['labels'][i]]\n",
    "        score = predictions['scores'][i]\n",
    "        box = predictions['boxes'][i]\n",
    "        print(f\"  {i+1}. {label}: {score:.3f} - Box: [{box[0]:.0f}, {box[1]:.0f}, {box[2]:.0f}, {box[3]:.0f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization Function\n",
    "\n",
    "def visualize_detections(img_rgb, predictions, title=\"Faster R-CNN Detections\"):\n",
    "    \"\"\"\n",
    "    Visualize detection results with bounding boxes and labels\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(14, 10))\n",
    "    ax.imshow(img_rgb)\n",
    "    \n",
    "    # Color map for different classes\n",
    "    colors = plt.cm.hsv(np.linspace(0, 1, len(COCO_CLASSES)))\n",
    "    \n",
    "    # Draw each detection\n",
    "    for i in range(len(predictions['boxes'])):\n",
    "        box = predictions['boxes'][i]\n",
    "        label_id = predictions['labels'][i]\n",
    "        score = predictions['scores'][i]\n",
    "        label = COCO_CLASSES[label_id]\n",
    "        \n",
    "        # Get box coordinates\n",
    "        x1, y1, x2, y2 = box\n",
    "        width = x2 - x1\n",
    "        height = y2 - y1\n",
    "        \n",
    "        # Draw rectangle\n",
    "        color = colors[label_id]\n",
    "        rect = Rectangle((x1, y1), width, height, \n",
    "                        linewidth=2, edgecolor=color, facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "        \n",
    "        # Add label with background\n",
    "        label_text = f'{label}: {score:.2f}'\n",
    "        ax.text(x1, y1-5, label_text, \n",
    "               bbox=dict(boxstyle='round,pad=0.3', facecolor=color, alpha=0.7),\n",
    "               fontsize=10, fontweight='bold', color='white')\n",
    "    \n",
    "    ax.set_title(f'{title}\\n({len(predictions[\"boxes\"])} detections)', \n",
    "                fontsize=14, fontweight='bold')\n",
    "    ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize test results\n",
    "if len(predictions['boxes']) > 0:\n",
    "    visualize_detections(img_rgb, predictions, \n",
    "                        title=\"Faster R-CNN Detection Test\")\n",
    "else:\n",
    "    print(\"No detections to visualize (try lowering confidence threshold)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confidence Threshold Tuning\n",
    "\n",
    "print(\"Testing different confidence thresholds...\\n\")\n",
    "\n",
    "thresholds = [0.3, 0.5, 0.7, 0.9]\n",
    "img_tensor, img_rgb = create_sample_image()\n",
    "\n",
    "# Get raw predictions (no filtering)\n",
    "with torch.no_grad():\n",
    "    raw_predictions = model([img_tensor.to(device)])[0]\n",
    "\n",
    "# Test each threshold\n",
    "results = []\n",
    "for thresh in thresholds:\n",
    "    keep_idx = raw_predictions['scores'] > thresh\n",
    "    n_detections = keep_idx.sum().item()\n",
    "    results.append(n_detections)\n",
    "    print(f\"Threshold {thresh:.1f}: {n_detections} detections\")\n",
    "\n",
    "# Visualize threshold impact\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, thresh in enumerate(thresholds):\n",
    "    # Filter predictions\n",
    "    keep_idx = raw_predictions['scores'] > thresh\n",
    "    filtered_pred = {\n",
    "        'boxes': raw_predictions['boxes'][keep_idx].cpu().numpy(),\n",
    "        'labels': raw_predictions['labels'][keep_idx].cpu().numpy(),\n",
    "        'scores': raw_predictions['scores'][keep_idx].cpu().numpy()\n",
    "    }\n",
    "    \n",
    "    # Visualize\n",
    "    axes[idx].imshow(img_rgb)\n",
    "    \n",
    "    colors = plt.cm.hsv(np.linspace(0, 1, len(COCO_CLASSES)))\n",
    "    for i in range(len(filtered_pred['boxes'])):\n",
    "        box = filtered_pred['boxes'][i]\n",
    "        label_id = filtered_pred['labels'][i]\n",
    "        score = filtered_pred['scores'][i]\n",
    "        label = COCO_CLASSES[label_id]\n",
    "        \n",
    "        x1, y1, x2, y2 = box\n",
    "        width = x2 - x1\n",
    "        height = y2 - y1\n",
    "        \n",
    "        color = colors[label_id]\n",
    "        rect = Rectangle((x1, y1), width, height,\n",
    "                        linewidth=2, edgecolor=color, facecolor='none')\n",
    "        axes[idx].add_patch(rect)\n",
    "        \n",
    "        label_text = f'{label}: {score:.2f}'\n",
    "        axes[idx].text(x1, y1-5, label_text,\n",
    "                      bbox=dict(boxstyle='round,pad=0.3', facecolor=color, alpha=0.7),\n",
    "                      fontsize=9, fontweight='bold', color='white')\n",
    "    \n",
    "    axes[idx].set_title(f'Threshold = {thresh:.1f}\\n({len(filtered_pred[\"boxes\"])} detections)',\n",
    "                       fontsize=12, fontweight='bold')\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.suptitle('Impact of Confidence Threshold on Detections', \n",
    "            fontsize=14, fontweight='bold', y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.savefig('threshold_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Observations:\")\n",
    "print(\"  Low threshold (0.3): More detections, but may include false positives\")\n",
    "print(\"  Medium threshold (0.5-0.7): Balanced precision-recall\")\n",
    "print(\"  High threshold (0.9): Very confident detections only, may miss objects\")\n",
    "print(\"\\nRecommendation: Use 0.5 as default, adjust based on application needs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Speed Benchmark - CPU vs GPU\n",
    "\n",
    "print(\"Running speed benchmark...\\n\")\n",
    "\n",
    "img_tensor, img_rgb = create_sample_image()\n",
    "n_iterations = 20\n",
    "\n",
    "# GPU benchmark (if available)\n",
    "if torch.cuda.is_available():\n",
    "    model_gpu = model.to('cuda')\n",
    "    img_gpu = img_tensor.to('cuda')\n",
    "    \n",
    "    # Warmup\n",
    "    for _ in range(5):\n",
    "        with torch.no_grad():\n",
    "            _ = model_gpu([img_gpu])\n",
    "    \n",
    "    # Benchmark\n",
    "    gpu_times = []\n",
    "    for _ in range(n_iterations):\n",
    "        start = time.time()\n",
    "        with torch.no_grad():\n",
    "            _ = model_gpu([img_gpu])\n",
    "        torch.cuda.synchronize()  # Wait for GPU to finish\n",
    "        gpu_times.append(time.time() - start)\n",
    "    \n",
    "    print(f\"GPU Performance ({n_iterations} iterations):\")\n",
    "    print(f\"  Average: {np.mean(gpu_times)*1000:.1f} ms\")\n",
    "    print(f\"  Std: {np.std(gpu_times)*1000:.1f} ms\")\n",
    "    print(f\"  FPS: {1/np.mean(gpu_times):.1f}\")\n",
    "\n",
    "# CPU benchmark\n",
    "model_cpu = model.to('cpu')\n",
    "img_cpu = img_tensor.to('cpu')\n",
    "\n",
    "# Warmup\n",
    "for _ in range(2):\n",
    "    with torch.no_grad():\n",
    "        _ = model_cpu([img_cpu])\n",
    "\n",
    "# Benchmark (fewer iterations for CPU)\n",
    "cpu_times = []\n",
    "for _ in range(5):\n",
    "    start = time.time()\n",
    "    with torch.no_grad():\n",
    "        _ = model_cpu([img_cpu])\n",
    "    cpu_times.append(time.time() - start)\n",
    "\n",
    "print(f\"\\nCPU Performance (5 iterations):\")\n",
    "print(f\"  Average: {np.mean(cpu_times)*1000:.1f} ms\")\n",
    "print(f\"  Std: {np.std(cpu_times)*1000:.1f} ms\")\n",
    "print(f\"  FPS: {1/np.mean(cpu_times):.2f}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    speedup = np.mean(cpu_times) / np.mean(gpu_times)\n",
    "    print(f\"\\nGPU Speedup: {speedup:.1f}×\")\n",
    "    \n",
    "    # Visualization\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Timing comparison\n",
    "    devices = ['GPU', 'CPU']\n",
    "    times_ms = [np.mean(gpu_times)*1000, np.mean(cpu_times)*1000]\n",
    "    colors = ['#27ae60', '#e74c3c']\n",
    "    \n",
    "    bars = ax1.bar(devices, times_ms, color=colors, alpha=0.7, edgecolor='black')\n",
    "    ax1.set_ylabel('Inference Time (ms)', fontsize=12, fontweight='bold')\n",
    "    ax1.set_title('GPU vs CPU Performance', fontsize=14, fontweight='bold')\n",
    "    ax1.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    for bar, t in zip(bars, times_ms):\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height(),\n",
    "                f'{t:.1f} ms', ha='center', va='bottom', \n",
    "                fontsize=11, fontweight='bold')\n",
    "    \n",
    "    # FPS comparison\n",
    "    fps_values = [1/np.mean(gpu_times), 1/np.mean(cpu_times)]\n",
    "    bars2 = ax2.bar(devices, fps_values, color=colors, alpha=0.7, edgecolor='black')\n",
    "    ax2.set_ylabel('Frames Per Second (FPS)', fontsize=12, fontweight='bold')\n",
    "    ax2.set_title('Throughput Comparison', fontsize=14, fontweight='bold')\n",
    "    ax2.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    for bar, fps in zip(bars2, fps_values):\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height(),\n",
    "                f'{fps:.1f} FPS', ha='center', va='bottom',\n",
    "                fontsize=11, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('gpu_vs_cpu.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Move model back to original device\n",
    "model.to(device)\n",
    "print(f\"\\n✓ Model back on {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. RPN Analysis (Advanced)\n",
    "\n",
    "### Understanding RPN Outputs:\n",
    "\n",
    "The Region Proposal Network generates proposals BEFORE the final detection. Let's analyze:\n",
    "\n",
    "**RPN Pipeline:**\n",
    "1. Generate ~20,000 anchor boxes across all feature map locations\n",
    "2. Predict objectness score for each anchor\n",
    "3. Predict box deltas (refinements) for each anchor\n",
    "4. Filter by objectness score (top 12,000 pre-NMS)\n",
    "5. Apply Non-Maximum Suppression (NMS)\n",
    "6. Select top 2,000 proposals (training) or 1,000 (inference)\n",
    "7. Further filter to top 300 for detection head\n",
    "\n",
    "**Key Concepts:**\n",
    "\n",
    "**Objectness Score:**\n",
    "- Binary classification: \"Is there any object here?\"\n",
    "- NOT class-specific (\"car\" vs \"dog\")\n",
    "- Just \"object\" vs \"background\"\n",
    "- Trained with IOU threshold:\n",
    "  - Positive: IOU > 0.7 with ground truth\n",
    "  - Negative: IOU < 0.3 with all ground truth\n",
    "\n",
    "**Box Deltas:**\n",
    "- Refinements to anchor positions\n",
    "- Format: (Δx, Δy, Δw, Δh)\n",
    "- Applied as:\n",
    "  ```\n",
    "  x_pred = x_anchor + Δx * w_anchor\n",
    "  y_pred = y_anchor + Δy * h_anchor\n",
    "  w_pred = w_anchor * exp(Δw)\n",
    "  h_pred = h_anchor * exp(Δh)\n",
    "  ```\n",
    "\n",
    "**NMS (Non-Maximum Suppression):**\n",
    "- Removes duplicate proposals\n",
    "- Keeps highest-scoring box, removes overlapping boxes (IOU > 0.7)\n",
    "- Essential for reducing redundancy\n",
    "\n",
    "### Why RPN Works:\n",
    "\n",
    "1. **Shared Computation**: Uses same features as detector\n",
    "2. **Learned**: Adapts to dataset characteristics\n",
    "3. **Fast**: GPU-accelerated, parallel processing\n",
    "4. **Accurate**: Better proposals than Selective Search\n",
    "5. **End-to-End**: Gradient flows from detection loss to RPN\n",
    "\n",
    "### RPN Training:\n",
    "\n",
    "**Loss Function:**\n",
    "```\n",
    "L_rpn = (1/N_cls) * Σ L_cls(p_i, p_i*) + λ * (1/N_reg) * Σ p_i* * L_reg(t_i, t_i*)\n",
    "```\n",
    "\n",
    "Where:\n",
    "- L_cls: Binary cross-entropy (object vs background)\n",
    "- L_reg: Smooth L1 loss for box regression\n",
    "- p_i: Predicted objectness\n",
    "- p_i*: Ground truth label (1=object, 0=background)\n",
    "- t_i: Predicted box deltas\n",
    "- t_i*: Target box deltas\n",
    "- λ: Balance parameter (typically 10)\n",
    "\n",
    "### Note:\n",
    "Accessing internal RPN outputs requires modifying the model forward pass, which is beyond this tutorial's scope. In practice, you typically just use the final detections. Understanding RPN conceptually is what matters for exam preparation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Mask R-CNN Preview (Instance Segmentation)\n",
    "\n",
    "print(\"Loading Mask R-CNN (extension of Faster R-CNN)...\\n\")\n",
    "\n",
    "from torchvision.models.detection import maskrcnn_resnet50_fpn\n",
    "\n",
    "# Load Mask R-CNN\n",
    "mask_rcnn = maskrcnn_resnet50_fpn(pretrained=True)\n",
    "mask_rcnn.eval()\n",
    "mask_rcnn.to(device)\n",
    "\n",
    "print(\"✓ Mask R-CNN loaded\")\n",
    "print(\"\\nMask R-CNN = Faster R-CNN + Instance Segmentation\")\n",
    "print(\"  Additional output: Pixel-level masks for each detected object\")\n",
    "print(\"  Same RPN and detection pipeline\")\n",
    "print(\"  Extra head: FCN (Fully Convolutional Network) for masks\")\n",
    "\n",
    "# Run detection + segmentation\n",
    "img_tensor, img_rgb = create_sample_image()\n",
    "with torch.no_grad():\n",
    "    mask_predictions = mask_rcnn([img_tensor.to(device)])[0]\n",
    "\n",
    "# Filter predictions\n",
    "keep_idx = mask_predictions['scores'] > 0.5\n",
    "n_detections = keep_idx.sum().item()\n",
    "\n",
    "print(f\"\\nMask R-CNN Results:\")\n",
    "print(f\"  Detections: {n_detections}\")\n",
    "print(f\"  Output keys: {list(mask_predictions.keys())}\")\n",
    "print(f\"  Note: 'masks' key contains segmentation masks!\")\n",
    "\n",
    "if n_detections > 0:\n",
    "    first_mask = mask_predictions['masks'][0, 0].cpu().numpy()\n",
    "    print(f\"  First mask shape: {first_mask.shape}\")\n",
    "    print(f\"  Mask values: [0, 1] (probability of belonging to object)\")\n",
    "\n",
    "print(\"\\nMask R-CNN Applications:\")\n",
    "print(\"  - Instance segmentation\")\n",
    "print(\"  - Object counting with precise boundaries\")\n",
    "print(\"  - Image editing and composition\")\n",
    "print(\"  - Medical imaging (tumor segmentation)\")\n",
    "print(\"  - Autonomous driving (pedestrian/vehicle boundaries)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Exercise: Detect Objects in Your Images\n",
    "\n",
    "**Task**: Apply Faster R-CNN to your own images and analyze results.\n",
    "\n",
    "### Instructions:\n",
    "\n",
    "1. **Load your image:**\n",
    "   ```python\n",
    "   img_tensor, img_rgb = preprocess_image('path/to/your/image.jpg')\n",
    "   ```\n",
    "\n",
    "2. **Run detection:**\n",
    "   ```python\n",
    "   predictions, inf_time = detect_objects(model, img_tensor, device, conf_threshold=0.5)\n",
    "   ```\n",
    "\n",
    "3. **Visualize results:**\n",
    "   ```python\n",
    "   visualize_detections(img_rgb, predictions)\n",
    "   ```\n",
    "\n",
    "4. **Experiment with:**\n",
    "   - Different confidence thresholds\n",
    "   - Different types of images (indoor, outdoor, crowded, simple)\n",
    "   - Comparing with YOLO results (if you did Week 14 notebooks)\n",
    "\n",
    "### Analysis Questions:\n",
    "\n",
    "1. **How accurate are the detections?**\n",
    "   - Are bounding boxes tight around objects?\n",
    "   - Are all objects detected?\n",
    "   - Any false positives?\n",
    "\n",
    "2. **How does performance vary with image complexity?**\n",
    "   - Simple scene (few objects)\n",
    "   - Crowded scene (many objects)\n",
    "   - Small objects vs large objects\n",
    "\n",
    "3. **What's the inference speed on your hardware?**\n",
    "   - GPU vs CPU difference\n",
    "   - Impact of image size\n",
    "\n",
    "4. **How does Faster R-CNN compare to YOLO?**\n",
    "   - Accuracy differences\n",
    "   - Speed differences\n",
    "   - When would you choose each?\n",
    "\n",
    "### Starter Code:\n",
    "\n",
    "```python\n",
    "# Your image path\n",
    "image_path = 'your_image.jpg'\n",
    "\n",
    "# Load and detect\n",
    "img_tensor, img_rgb = preprocess_image(image_path)\n",
    "predictions, inf_time = detect_objects(model, img_tensor, device)\n",
    "\n",
    "# Analyze\n",
    "print(f\"Detected {len(predictions['boxes'])} objects in {inf_time*1000:.1f} ms\")\n",
    "visualize_detections(img_rgb, predictions)\n",
    "\n",
    "# Try different thresholds\n",
    "for thresh in [0.3, 0.5, 0.7, 0.9]:\n",
    "    pred, _ = detect_objects(model, img_tensor, device, conf_threshold=thresh)\n",
    "    print(f\"Threshold {thresh}: {len(pred['boxes'])} detections\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Summary & Comparison with YOLO\n",
    "\n",
    "### What We Learned:\n",
    "\n",
    "1. **Faster R-CNN Architecture**:\n",
    "   - Backbone (ResNet50 + FPN)\n",
    "   - Region Proposal Network (RPN)\n",
    "   - ROI Align\n",
    "   - Detection Head\n",
    "\n",
    "2. **Using Pre-trained Models**:\n",
    "   - Load with torchvision\n",
    "   - Preprocess images correctly\n",
    "   - Run inference\n",
    "   - Filter and visualize results\n",
    "\n",
    "3. **Performance Analysis**:\n",
    "   - Confidence threshold tuning\n",
    "   - GPU vs CPU benchmarking\n",
    "   - Understanding RPN outputs\n",
    "\n",
    "### Faster R-CNN vs YOLO (from Week 14):\n",
    "\n",
    "| Aspect | Faster R-CNN | YOLO v8 |\n",
    "|--------|--------------|----------|\n",
    "| **Paradigm** | Two-stage | Single-shot |\n",
    "| **Speed (GPU)** | ~5-10 FPS | ~60-100 FPS |\n",
    "| **Accuracy (COCO)** | ~42% mAP | ~50% mAP (v8m) |\n",
    "| **Small Objects** | Better | Good |\n",
    "| **Localization** | Very precise | Good |\n",
    "| **Model Size** | ~160 MB | ~52 MB |\n",
    "| **GPU Memory** | ~2 GB | ~1.5 GB |\n",
    "| **Real-time?** | Borderline (5-10 FPS) | Yes (60+ FPS) |\n",
    "| **Use Cases** | Precision-critical | Real-time apps |\n",
    "\n",
    "### When to Choose Each:\n",
    "\n",
    "**Choose Faster R-CNN when:**\n",
    "- Accuracy is paramount (medical imaging, quality inspection)\n",
    "- Small objects matter (satellite imagery, microscopy)\n",
    "- Offline analysis is acceptable (video post-processing)\n",
    "- Need instance segmentation (use Mask R-CNN)\n",
    "\n",
    "**Choose YOLO when:**\n",
    "- Real-time performance required (autonomous driving, surveillance)\n",
    "- Deployment on edge devices (mobile, embedded)\n",
    "- Large objects in clear scenes\n",
    "- Need high throughput (process many images quickly)\n",
    "\n",
    "### Next Notebook Preview:\n",
    "\n",
    "**Notebook 04**: YOLO vs R-CNN Benchmark\n",
    "- Direct head-to-head comparison\n",
    "- Same test images\n",
    "- Speed and accuracy metrics\n",
    "- Detailed analysis\n",
    "\n",
    "---\n",
    "\n",
    "**Estimated completion time**: 20 minutes\n",
    "\n",
    "**Tutorial T15 Complete!** You can now use pre-trained Faster R-CNN for object detection and understand its architecture and trade-offs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
