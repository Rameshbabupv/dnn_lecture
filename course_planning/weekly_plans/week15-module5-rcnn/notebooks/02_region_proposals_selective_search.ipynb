{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 02: Region Proposals & Selective Search\n",
    "\n",
    "**Week 15 - Module 5: Object Detection**\n",
    "\n",
    "**Duration:** ~15 minutes\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Understand the region proposal concept and why it matters\n",
    "- Learn how Selective Search algorithm works\n",
    "- Implement Selective Search hands-on\n",
    "- Understand why Region Proposal Network (RPN) replaced it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What are Region Proposals?\n",
    "\n",
    "### The Object Detection Challenge:\n",
    "\n",
    "Given an image, we need to find all objects. But...\n",
    "\n",
    "**Naive Approach - Sliding Window:**\n",
    "- Try every possible bounding box position\n",
    "- Multiple scales: 50×50, 100×100, 200×200, ...\n",
    "- Multiple aspect ratios: 1:1, 1:2, 2:1, ...\n",
    "- **Result**: Millions of candidate boxes per image!\n",
    "\n",
    "**Example Calculation:**\n",
    "- Image: 640×480 pixels\n",
    "- Scales: 10 different sizes\n",
    "- Aspect ratios: 3 ratios\n",
    "- Stride: 16 pixels\n",
    "- **Total boxes**: (640/16) × (480/16) × 10 × 3 = **36,000 boxes**\n",
    "\n",
    "Running a CNN classifier 36,000 times per image is impractical!\n",
    "\n",
    "### Region Proposals Solution:\n",
    "\n",
    "**Key Idea**: Pre-filter candidate regions before expensive classification\n",
    "\n",
    "**Goals:**\n",
    "1. **Reduce search space**: Millions → Thousands\n",
    "2. **High recall**: Don't miss actual objects (recall ≥ 95%)\n",
    "3. **Class-agnostic**: Find \"object-like\" regions regardless of category\n",
    "4. **Fast generation**: < 1 second per image\n",
    "\n",
    "**Analogy**: \n",
    "- Region proposals = \"Where might objects be?\"\n",
    "- Like a metal detector scanning beach before digging\n",
    "- Focus expensive computation where it matters\n",
    "\n",
    "### Historical Methods:\n",
    "\n",
    "1. **Objectness (2010)**: Generic object saliency\n",
    "2. **Selective Search (2011-2013)**: Hierarchical grouping (used in R-CNN)\n",
    "3. **EdgeBoxes (2014)**: Contour-based proposals\n",
    "4. **RPN (2015)**: Learned neural network proposals (Faster R-CNN) ← Current standard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Selective Search Algorithm\n",
    "\n",
    "**Paper**: \"Selective Search for Object Recognition\" (2013)\n",
    "\n",
    "**Authors**: J.R.R. Uijlings et al.\n",
    "\n",
    "### Core Philosophy:\n",
    "\n",
    "Objects can appear at different scales and have various characteristics. Use **multiple strategies** and combine them:\n",
    "\n",
    "1. **Hierarchical Grouping**: Start small, merge similar regions\n",
    "2. **Multiple Similarity Measures**: Color, texture, size, fill\n",
    "3. **Multiple Color Spaces**: RGB, HSV, Lab, etc.\n",
    "4. **Diversification**: Capture all possible object-like regions\n",
    "\n",
    "### Algorithm Steps:\n",
    "\n",
    "```\n",
    "Step 1: Initial Segmentation\n",
    "  ├─ Use graph-based segmentation (Felzenszwalb)\n",
    "  ├─ Over-segment image into ~1000 small regions\n",
    "  └─ Each region = potential object part\n",
    "\n",
    "Step 2: Calculate Similarity\n",
    "  ├─ Color similarity (histogram intersection)\n",
    "  ├─ Texture similarity (Gaussian derivatives)\n",
    "  ├─ Size similarity (prefer merging small regions)\n",
    "  └─ Fill similarity (how well regions fit together)\n",
    "\n",
    "Step 3: Hierarchical Grouping\n",
    "  ├─ Merge most similar neighboring regions\n",
    "  ├─ Recalculate similarities\n",
    "  ├─ Repeat until entire image is one region\n",
    "  └─ Record all merge steps\n",
    "\n",
    "Step 4: Generate Proposals\n",
    "  ├─ Each region in hierarchy = one proposal\n",
    "  ├─ Compute bounding box for each region\n",
    "  └─ Output: ~2000 region proposals\n",
    "```\n",
    "\n",
    "### Similarity Metrics:\n",
    "\n",
    "**1. Color Similarity** (s_color):\n",
    "- 25-bin histogram per color channel\n",
    "- Histogram intersection: s_color(ri, rj) = Σ min(ci, cj)\n",
    "\n",
    "**2. Texture Similarity** (s_texture):\n",
    "- SIFT-like features: Gaussian derivatives in 8 orientations\n",
    "- 10-bin histogram per channel per orientation\n",
    "\n",
    "**3. Size Similarity** (s_size):\n",
    "- s_size(ri, rj) = 1 - (size(ri) + size(rj)) / size(image)\n",
    "- Encourages merging small regions first\n",
    "\n",
    "**4. Fill Similarity** (s_fill):\n",
    "- s_fill(ri, rj) = 1 - (size(BB_merge) - size(ri) - size(rj)) / size(image)\n",
    "- BB_merge = bounding box of merged region\n",
    "- Prefers regions that fit together well\n",
    "\n",
    "**Combined Similarity**:\n",
    "```\n",
    "s(ri, rj) = a1*s_color + a2*s_texture + a3*s_size + a4*s_fill\n",
    "```\n",
    "\n",
    "### Fast vs Quality Modes:\n",
    "\n",
    "**Fast Mode** (default):\n",
    "- Fewer color spaces (just RGB)\n",
    "- Fewer starting regions\n",
    "- ~1000 proposals in 1 second\n",
    "\n",
    "**Quality Mode**:\n",
    "- Multiple color spaces (RGB, HSV, Lab, etc.)\n",
    "- More diversified strategies\n",
    "- ~2000 proposals in 2-3 seconds\n",
    "- Better recall (fewer missed objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "import time\n",
    "\n",
    "print(\"OpenCV version:\", cv2.__version__)\n",
    "print(\"\\nNote: Selective Search requires opencv-contrib-python\")\n",
    "print(\"Install: pip install opencv-contrib-python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to create sample image\n",
    "def create_sample_image():\n",
    "    \"\"\"Create a simple test image with colored rectangles\"\"\"\n",
    "    img = np.ones((400, 600, 3), dtype=np.uint8) * 255  # White background\n",
    "    \n",
    "    # Draw colored rectangles (simulating objects)\n",
    "    cv2.rectangle(img, (50, 50), (150, 150), (255, 0, 0), -1)     # Blue square\n",
    "    cv2.rectangle(img, (200, 100), (400, 200), (0, 255, 0), -1)   # Green rectangle\n",
    "    cv2.rectangle(img, (450, 250), (550, 350), (0, 0, 255), -1)   # Red square\n",
    "    cv2.circle(img, (300, 300), 50, (255, 255, 0), -1)            # Yellow circle\n",
    "    \n",
    "    return img\n",
    "\n",
    "# Create and display sample image\n",
    "img = create_sample_image()\n",
    "img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(img_rgb)\n",
    "plt.title('Sample Image with 4 Objects', fontsize=14, fontweight='bold')\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Image shape: {img.shape}\")\n",
    "print(f\"Number of actual objects: 4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement Selective Search\n",
    "\n",
    "def run_selective_search(image, mode='fast'):\n",
    "    \"\"\"\n",
    "    Run Selective Search on image\n",
    "    \n",
    "    Args:\n",
    "        image: Input image (BGR format)\n",
    "        mode: 'fast' or 'quality'\n",
    "    \n",
    "    Returns:\n",
    "        proposals: Array of bounding boxes [x, y, w, h]\n",
    "        elapsed_time: Processing time in seconds\n",
    "    \"\"\"\n",
    "    # Create Selective Search object\n",
    "    ss = cv2.ximgproc.segmentation.createSelectiveSearchSegmentation()\n",
    "    ss.setBaseImage(image)\n",
    "    \n",
    "    # Choose mode\n",
    "    if mode == 'fast':\n",
    "        ss.switchToSelectiveSearchFast()\n",
    "    else:\n",
    "        ss.switchToSelectiveSearchQuality()\n",
    "    \n",
    "    # Run Selective Search\n",
    "    start_time = time.time()\n",
    "    proposals = ss.process()\n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    return proposals, elapsed_time\n",
    "\n",
    "# Run on sample image\n",
    "print(\"Running Selective Search (Fast mode)...\")\n",
    "proposals_fast, time_fast = run_selective_search(img, mode='fast')\n",
    "\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"  Number of proposals: {len(proposals_fast)}\")\n",
    "print(f\"  Processing time: {time_fast:.3f} seconds\")\n",
    "print(f\"  Proposals per second: {len(proposals_fast)/time_fast:.1f}\")\n",
    "\n",
    "# Show first few proposals\n",
    "print(f\"\\nFirst 5 proposals (x, y, w, h):\")\n",
    "for i, (x, y, w, h) in enumerate(proposals_fast[:5]):\n",
    "    print(f\"  {i+1}. [{x:4d}, {y:4d}, {w:4d}, {h:4d}] - Area: {w*h:6d} pixels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Region Proposals\n",
    "\n",
    "def visualize_proposals(image, proposals, max_proposals=100, title=\"Region Proposals\"):\n",
    "    \"\"\"\n",
    "    Visualize region proposals on image\n",
    "    \"\"\"\n",
    "    img_rgb = cv2.cvtColor(image.copy(), cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    ax.imshow(img_rgb)\n",
    "    \n",
    "    # Draw proposals\n",
    "    for i, (x, y, w, h) in enumerate(proposals[:max_proposals]):\n",
    "        rect = Rectangle((x, y), w, h, linewidth=1, \n",
    "                        edgecolor='lime', facecolor='none', alpha=0.4)\n",
    "        ax.add_patch(rect)\n",
    "    \n",
    "    ax.set_title(f'{title}\\n(Showing {min(max_proposals, len(proposals))}/{len(proposals)} proposals)',\n",
    "                fontsize=14, fontweight='bold')\n",
    "    ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize top 100 proposals\n",
    "visualize_proposals(img, proposals_fast, max_proposals=100, \n",
    "                   title=\"Selective Search - Top 100 Proposals\")\n",
    "\n",
    "print(\"Observation: Notice how proposals cover objects at different scales\")\n",
    "print(\"Green boxes = potential object regions identified by algorithm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze Proposals - Size Distribution\n",
    "\n",
    "# Calculate proposal statistics\n",
    "areas = [w * h for (x, y, w, h) in proposals_fast]\n",
    "aspect_ratios = [w / h if h > 0 else 0 for (x, y, w, h) in proposals_fast]\n",
    "widths = [w for (x, y, w, h) in proposals_fast]\n",
    "heights = [h for (x, y, w, h) in proposals_fast]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Area distribution\n",
    "axes[0, 0].hist(areas, bins=50, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "axes[0, 0].set_xlabel('Area (pixels²)', fontsize=11, fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Frequency', fontsize=11, fontweight='bold')\n",
    "axes[0, 0].set_title('Proposal Area Distribution', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].axvline(np.median(areas), color='red', linestyle='--', \n",
    "                   label=f'Median: {np.median(areas):.0f}')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "# Aspect ratio distribution\n",
    "axes[0, 1].hist(aspect_ratios, bins=50, color='lightcoral', edgecolor='black', alpha=0.7)\n",
    "axes[0, 1].set_xlabel('Aspect Ratio (w/h)', fontsize=11, fontweight='bold')\n",
    "axes[0, 1].set_ylabel('Frequency', fontsize=11, fontweight='bold')\n",
    "axes[0, 1].set_title('Aspect Ratio Distribution', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].axvline(1.0, color='green', linestyle='--', label='Square (1:1)')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "axes[0, 1].set_xlim([0, 5])  # Limit x-axis for better visualization\n",
    "\n",
    "# Width vs Height scatter\n",
    "axes[1, 0].scatter(widths, heights, alpha=0.3, s=10, color='purple')\n",
    "axes[1, 0].set_xlabel('Width (pixels)', fontsize=11, fontweight='bold')\n",
    "axes[1, 0].set_ylabel('Height (pixels)', fontsize=11, fontweight='bold')\n",
    "axes[1, 0].set_title('Width vs Height', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].plot([0, max(widths)], [0, max(heights)], 'r--', alpha=0.5, label='Square line')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(alpha=0.3)\n",
    "\n",
    "# Cumulative area coverage\n",
    "sorted_areas = sorted(areas, reverse=True)\n",
    "cumulative = np.cumsum(sorted_areas) / sum(areas) * 100\n",
    "axes[1, 1].plot(cumulative, color='darkgreen', linewidth=2)\n",
    "axes[1, 1].set_xlabel('Number of Proposals', fontsize=11, fontweight='bold')\n",
    "axes[1, 1].set_ylabel('Cumulative Area Coverage (%)', fontsize=11, fontweight='bold')\n",
    "axes[1, 1].set_title('Cumulative Area Coverage', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].axhline(80, color='red', linestyle='--', alpha=0.5, label='80% coverage')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('proposal_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nProposal Statistics:\")\n",
    "print(f\"  Total proposals: {len(proposals_fast)}\")\n",
    "print(f\"  Area range: {min(areas)} - {max(areas)} pixels²\")\n",
    "print(f\"  Median area: {np.median(areas):.0f} pixels²\")\n",
    "print(f\"  Aspect ratio range: {min(aspect_ratios):.2f} - {max(aspect_ratios):.2f}\")\n",
    "print(f\"  Median aspect ratio: {np.median(aspect_ratios):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quality vs Speed Mode Comparison\n",
    "\n",
    "print(\"Comparing Fast vs Quality modes...\\n\")\n",
    "\n",
    "# Run both modes\n",
    "proposals_fast, time_fast = run_selective_search(img, mode='fast')\n",
    "proposals_quality, time_quality = run_selective_search(img, mode='quality')\n",
    "\n",
    "# Comparison table\n",
    "comparison_data = {\n",
    "    'Mode': ['Fast', 'Quality'],\n",
    "    'Proposals': [len(proposals_fast), len(proposals_quality)],\n",
    "    'Time (s)': [f'{time_fast:.3f}', f'{time_quality:.3f}'],\n",
    "    'FPS': [f'{1/time_fast:.2f}', f'{1/time_quality:.2f}']\n",
    "}\n",
    "\n",
    "print(\"Mode Comparison:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Mode':<12} {'Proposals':<12} {'Time (s)':<12} {'FPS':<12}\")\n",
    "print(\"=\"*60)\n",
    "for i in range(len(comparison_data['Mode'])):\n",
    "    print(f\"{comparison_data['Mode'][i]:<12} \"\n",
    "          f\"{comparison_data['Proposals'][i]:<12} \"\n",
    "          f\"{comparison_data['Time (s)'][i]:<12} \"\n",
    "          f\"{comparison_data['FPS'][i]:<12}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Fast mode\n",
    "img_fast = cv2.cvtColor(img.copy(), cv2.COLOR_BGR2RGB)\n",
    "axes[0].imshow(img_fast)\n",
    "for i, (x, y, w, h) in enumerate(proposals_fast[:50]):\n",
    "    rect = Rectangle((x, y), w, h, linewidth=1, \n",
    "                    edgecolor='lime', facecolor='none', alpha=0.4)\n",
    "    axes[0].add_patch(rect)\n",
    "axes[0].set_title(f'Fast Mode\\n{len(proposals_fast)} proposals in {time_fast:.3f}s', \n",
    "                 fontsize=12, fontweight='bold')\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Quality mode\n",
    "img_quality = cv2.cvtColor(img.copy(), cv2.COLOR_BGR2RGB)\n",
    "axes[1].imshow(img_quality)\n",
    "for i, (x, y, w, h) in enumerate(proposals_quality[:50]):\n",
    "    rect = Rectangle((x, y), w, h, linewidth=1, \n",
    "                    edgecolor='cyan', facecolor='none', alpha=0.4)\n",
    "    axes[1].add_patch(rect)\n",
    "axes[1].set_title(f'Quality Mode\\n{len(proposals_quality)} proposals in {time_quality:.3f}s', \n",
    "                 fontsize=12, fontweight='bold')\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('fast_vs_quality.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nKey Differences:\")\n",
    "print(f\"  Quality mode generates {len(proposals_quality) - len(proposals_fast)} more proposals\")\n",
    "print(f\"  Quality mode is {time_quality/time_fast:.1f}× slower\")\n",
    "print(f\"  Trade-off: Better recall vs processing time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Problems with Selective Search\n",
    "\n",
    "Despite being the foundation of R-CNN and Fast R-CNN, Selective Search has fundamental limitations:\n",
    "\n",
    "### Problem 1: Not Learned from Data\n",
    "- **Fixed algorithm**: Hand-crafted rules, can't adapt to dataset\n",
    "- **No optimization**: Parameters manually tuned, not learned\n",
    "- **Generic**: Doesn't leverage domain knowledge\n",
    "- **Example**: Same strategy for faces, cars, text - but these need different approaches!\n",
    "\n",
    "### Problem 2: Slow Processing\n",
    "- **CPU-only**: No GPU acceleration\n",
    "- **1-2 seconds per image**: 87% of total detection time in Fast R-CNN!\n",
    "- **Bottleneck**: Can't leverage parallel processing\n",
    "- **Compare**: Modern RPN runs in milliseconds on GPU\n",
    "\n",
    "### Problem 3: Low-Quality Proposals\n",
    "- **Many redundant boxes**: High overlap between proposals\n",
    "- **Poor localization**: Bounding boxes not tight around objects\n",
    "- **Recall-precision trade-off**: Need 2000 proposals to get 95% recall\n",
    "- **Missed objects**: Small or unusual objects often missed\n",
    "\n",
    "### Problem 4: Not End-to-End Trainable\n",
    "- **Separate preprocessing step**: Can't train jointly with detector\n",
    "- **No gradient flow**: Can't optimize for detection task\n",
    "- **Fixed during training**: Detector adapts around proposal limitations\n",
    "\n",
    "### Problem 5: No Task-Specific Optimization\n",
    "- **Generic \"objectness\"**: Not optimized for specific classes\n",
    "- **Fixed hierarchy**: Can't learn which merges matter\n",
    "- **No feedback**: Detection errors don't improve proposals\n",
    "\n",
    "### Quantitative Limitations:\n",
    "\n",
    "| Metric | Selective Search | RPN (Faster R-CNN) |\n",
    "|--------|------------------|--------------------|\n",
    "| **Processing Time** | 1-2 seconds | 10 milliseconds |\n",
    "| **Proposals Needed** | ~2000 | ~300 |\n",
    "| **Recall @ 95%** | 2000 proposals | 300 proposals |\n",
    "| **Trainable** | No | Yes |\n",
    "| **GPU Accelerated** | No | Yes |\n",
    "| **Localization Quality** | Moderate | High |\n",
    "| **Adaptable** | No | Yes (learns from data) |\n",
    "\n",
    "### Real-World Impact:\n",
    "\n",
    "**Fast R-CNN timing breakdown:**\n",
    "- Selective Search: **2.0 seconds** (87%)\n",
    "- CNN forward pass: 0.2 seconds (9%)\n",
    "- ROI pooling + detection: 0.1 seconds (4%)\n",
    "- **Total: 2.3 seconds per image**\n",
    "\n",
    "**Faster R-CNN timing breakdown:**\n",
    "- RPN: **0.01 seconds** (5%)\n",
    "- CNN forward pass: 0.15 seconds (75%)\n",
    "- ROI pooling + detection: 0.04 seconds (20%)\n",
    "- **Total: 0.2 seconds per image**\n",
    "\n",
    "**Result**: 11× speedup just by replacing Selective Search with RPN!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Solution: Region Proposal Network (RPN)\n",
    "\n",
    "Faster R-CNN introduced RPN to solve all Selective Search problems:\n",
    "\n",
    "### RPN Key Ideas:\n",
    "\n",
    "#### 1. Learned from Data\n",
    "```python\n",
    "# RPN is a neural network trained end-to-end\n",
    "# Input: Feature map from backbone CNN\n",
    "# Output: Objectness scores + bounding box coordinates\n",
    "\n",
    "class RPN(nn.Module):\n",
    "    def __init__(self):\n",
    "        self.conv = nn.Conv2d(512, 512, 3, padding=1)  # 3×3 conv\n",
    "        self.cls_layer = nn.Conv2d(512, 2*k, 1)  # k anchors × 2 (obj/not)\n",
    "        self.reg_layer = nn.Conv2d(512, 4*k, 1)  # k anchors × 4 (x,y,w,h)\n",
    "    \n",
    "    def forward(self, features):\n",
    "        x = F.relu(self.conv(features))\n",
    "        objectness = self.cls_layer(x)  # Is there an object?\n",
    "        bbox_deltas = self.reg_layer(x)  # Where is it?\n",
    "        return objectness, bbox_deltas\n",
    "```\n",
    "\n",
    "#### 2. Anchor Boxes Concept\n",
    "- Pre-defined reference boxes at each feature map location\n",
    "- Multiple scales: {128², 256², 512²}\n",
    "- Multiple aspect ratios: {1:1, 1:2, 2:1}\n",
    "- Total: 3 × 3 = 9 anchors per position\n",
    "\n",
    "#### 3. Shared Computation\n",
    "- Uses same CNN features as detector\n",
    "- No separate image processing\n",
    "- GPU-accelerated, parallel processing\n",
    "\n",
    "#### 4. End-to-End Training\n",
    "- Trained jointly with detection network\n",
    "- Gradient flows from detection loss to RPN\n",
    "- Learns what proposals help detection\n",
    "\n",
    "### RPN vs Selective Search:\n",
    "\n",
    "**Speed:**\n",
    "- Selective Search: 2000ms\n",
    "- RPN: 10ms\n",
    "- **Speedup: 200×**\n",
    "\n",
    "**Quality:**\n",
    "- Selective Search: 2000 proposals for 95% recall\n",
    "- RPN: 300 proposals for 96% recall\n",
    "- **Efficiency: 6.7× fewer proposals, better recall**\n",
    "\n",
    "**Adaptability:**\n",
    "- Selective Search: Fixed for all datasets\n",
    "- RPN: Learns optimal proposals for each dataset\n",
    "\n",
    "### Why RPN Won:\n",
    "\n",
    "1. **Speed**: 200× faster\n",
    "2. **Quality**: Better proposals with fewer candidates\n",
    "3. **Integration**: Seamless with detection pipeline\n",
    "4. **Flexibility**: Adapts to any dataset/task\n",
    "5. **Simplicity**: One network vs complex preprocessing\n",
    "\n",
    "### Legacy:\n",
    "\n",
    "RPN's anchor box concept influenced:\n",
    "- **YOLO** (grid-based anchors)\n",
    "- **SSD** (multi-scale anchors)\n",
    "- **RetinaNet** (focal loss with anchors)\n",
    "- **Modern detectors**: Most use learned proposal mechanisms\n",
    "\n",
    "**Preview**: In Notebook 03, we'll implement Faster R-CNN with RPN!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Exercise: Generate Proposals for Your Image\n",
    "\n",
    "**Task**: Apply Selective Search to your own image and analyze results.\n",
    "\n",
    "### Instructions:\n",
    "\n",
    "1. Load your image (or use provided test image)\n",
    "2. Run Selective Search (both Fast and Quality modes)\n",
    "3. Visualize top 100 proposals\n",
    "4. Analyze:\n",
    "   - How many proposals contain actual objects?\n",
    "   - How many are redundant?\n",
    "   - Quality of bounding boxes (tight fit?)\n",
    "\n",
    "### Questions:\n",
    "\n",
    "1. **What percentage of proposals are useful?**\n",
    "   - Count proposals with objects vs total\n",
    "   \n",
    "2. **How does image complexity affect proposal count?**\n",
    "   - Simple scene vs cluttered scene\n",
    "   \n",
    "3. **Is Quality mode worth the extra time?**\n",
    "   - Compare recall improvement vs speed cost\n",
    "   \n",
    "4. **How would you filter proposals before detection?**\n",
    "   - Size thresholds?\n",
    "   - Non-maximum suppression?\n",
    "\n",
    "### Starter Code:\n",
    "\n",
    "```python\n",
    "# Load your image\n",
    "img = cv2.imread('your_image.jpg')\n",
    "\n",
    "# Run Selective Search\n",
    "proposals, time_taken = run_selective_search(img, mode='fast')\n",
    "\n",
    "# Visualize\n",
    "visualize_proposals(img, proposals, max_proposals=100)\n",
    "\n",
    "# Analyze\n",
    "print(f\"Total proposals: {len(proposals)}\")\n",
    "print(f\"Processing time: {time_taken:.3f}s\")\n",
    "\n",
    "# Your analysis here...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary\n",
    "\n",
    "### What We Learned:\n",
    "\n",
    "1. **Region Proposals Concept**:\n",
    "   - Pre-filter candidate regions before expensive classification\n",
    "   - Reduce search space from millions to thousands\n",
    "   - Goal: High recall with manageable number of proposals\n",
    "\n",
    "2. **Selective Search Algorithm**:\n",
    "   - Hierarchical grouping based on color, texture, size, fill\n",
    "   - Generates ~2000 diverse proposals\n",
    "   - Fast vs Quality modes (speed vs recall trade-off)\n",
    "\n",
    "3. **Selective Search Limitations**:\n",
    "   - Not learned (fixed algorithm)\n",
    "   - Slow (1-2 seconds per image)\n",
    "   - Low-quality proposals (many redundant)\n",
    "   - Not end-to-end trainable\n",
    "\n",
    "4. **RPN Solution**:\n",
    "   - Learned neural network for proposals\n",
    "   - 200× faster (10ms vs 2000ms)\n",
    "   - Better quality with fewer proposals\n",
    "   - End-to-end trainable\n",
    "\n",
    "### Historical Context:\n",
    "\n",
    "- **2011-2013**: Selective Search state-of-the-art\n",
    "- **2014**: R-CNN uses Selective Search → 53% mAP\n",
    "- **2015**: Fast R-CNN still uses Selective Search → bottleneck identified\n",
    "- **2015**: Faster R-CNN replaces with RPN → 200× speedup!\n",
    "- **2015-present**: Learned proposals become standard\n",
    "\n",
    "### Key Takeaway:\n",
    "\n",
    "**Selective Search** was crucial for early object detection success, but **learned proposals (RPN)** proved far superior. Understanding both helps appreciate why modern detectors are designed the way they are.\n",
    "\n",
    "### Next Notebook Preview:\n",
    "\n",
    "**Notebook 03**: Faster R-CNN Pre-trained Implementation\n",
    "- Load pre-trained Faster R-CNN\n",
    "- Run detection on images\n",
    "- Analyze RPN outputs\n",
    "- Compare with YOLO performance\n",
    "\n",
    "---\n",
    "\n",
    "**Estimated completion time**: 15 minutes\n",
    "\n",
    "**Key insight**: The evolution from Selective Search to RPN shows how deep learning replaced hand-crafted algorithms in computer vision. This pattern repeated across many CV tasks (features, detection, segmentation, etc.)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
