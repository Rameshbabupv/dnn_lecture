{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 01: R-CNN Family Evolution\n",
    "\n",
    "**Week 15 - Module 5: Object Detection**\n",
    "\n",
    "**Duration:** ~10 minutes\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Understand R-CNN evolution timeline (R-CNN → Fast R-CNN → Faster R-CNN)\n",
    "- Compare architectures and key improvements at each stage\n",
    "- Learn the two-stage detection paradigm and why it matters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Two-Stage Detection?\n",
    "\n",
    "Before we dive into the R-CNN family, let's understand why two-stage detectors exist:\n",
    "\n",
    "### Advantages:\n",
    "- **Higher Accuracy**: Separate stages allow for refined localization and classification\n",
    "- **Better for Small Objects**: Region proposals help focus on challenging areas\n",
    "- **Academic Importance**: Pioneered modern object detection (2014-2015)\n",
    "- **Precision-Critical Applications**: Medical imaging, autonomous driving decision-making\n",
    "\n",
    "### Two-Stage Paradigm:\n",
    "1. **Stage 1**: Generate region proposals (\"Where might objects be?\")\n",
    "2. **Stage 2**: Classify and refine each proposal (\"What is it exactly?\")\n",
    "\n",
    "### Trade-off:\n",
    "- **Gain**: Higher mAP (mean Average Precision)\n",
    "- **Cost**: Slower inference (historically 1-2 FPS vs 30+ FPS for YOLO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. R-CNN (2014) - The Pioneer\n",
    "\n",
    "**Authors:** Ross Girshick et al. (UC Berkeley)\n",
    "\n",
    "**Paper:** \"Rich feature hierarchies for accurate object detection and semantic segmentation\"\n",
    "\n",
    "### Architecture:\n",
    "\n",
    "```\n",
    "Input Image\n",
    "    ↓\n",
    "Selective Search → ~2000 region proposals\n",
    "    ↓\n",
    "Warp each region to 227×227\n",
    "    ↓\n",
    "CNN (AlexNet) → Extract features (2000 times!)\n",
    "    ↓\n",
    "SVM Classifier → Classify each region\n",
    "    ↓\n",
    "Bounding Box Regressor → Refine box coordinates\n",
    "    ↓\n",
    "Output: Detected objects with boxes\n",
    "```\n",
    "\n",
    "### Key Components:\n",
    "1. **Selective Search**: Generates ~2000 candidate regions per image\n",
    "2. **CNN Feature Extraction**: AlexNet extracts 4096-dim features per region\n",
    "3. **SVM Classification**: Separate SVM for each object class\n",
    "4. **Bounding Box Regression**: Linear regressor to refine box coordinates\n",
    "\n",
    "### Performance:\n",
    "- **mAP on PASCAL VOC 2012**: 53.3%\n",
    "- **Speed**: 47 seconds per image (GPU)\n",
    "- **Breakthrough**: 30% improvement over previous methods!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. R-CNN Problems\n",
    "\n",
    "Despite its groundbreaking accuracy, R-CNN had critical issues:\n",
    "\n",
    "### Problem 1: Extremely Slow\n",
    "- **47 seconds per image** is unusable for real applications\n",
    "- Why? CNN forward pass for each of 2000 regions\n",
    "\n",
    "### Problem 2: Redundant Computation\n",
    "- Same image features computed 2000 times with heavy overlap\n",
    "- Neighboring regions share 80%+ of pixels but recomputed independently\n",
    "\n",
    "### Problem 3: Multi-Stage Training\n",
    "- **Stage 1**: Train CNN on ImageNet\n",
    "- **Stage 2**: Fine-tune CNN on detection data\n",
    "- **Stage 3**: Train SVMs\n",
    "- **Stage 4**: Train bounding box regressors\n",
    "- Not end-to-end trainable!\n",
    "\n",
    "### Problem 4: Storage Requirements\n",
    "- Must cache features for all regions to disk (hundreds of GB)\n",
    "- Training takes 2.5 GPU-days\n",
    "\n",
    "**Solution needed**: Eliminate redundant computation while maintaining accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fast R-CNN (2015) - Major Speedup\n",
    "\n",
    "**Author:** Ross Girshick (Microsoft Research)\n",
    "\n",
    "**Paper:** \"Fast R-CNN\"\n",
    "\n",
    "### Key Innovation: Share Computation!\n",
    "\n",
    "```\n",
    "Input Image\n",
    "    ↓\n",
    "CNN (Single forward pass!) → Feature map\n",
    "    ↓\n",
    "Selective Search → ~2000 region proposals\n",
    "    ↓\n",
    "ROI Pooling Layer → Extract fixed-size features per region\n",
    "    ↓\n",
    "Fully Connected Layers\n",
    "    ↓\n",
    "Parallel Branches:\n",
    "  ├─ Softmax Classifier (C+1 classes)\n",
    "  └─ Bounding Box Regressor (4 coordinates)\n",
    "    ↓\n",
    "Output: Detected objects\n",
    "```\n",
    "\n",
    "### Major Improvements:\n",
    "\n",
    "#### 1. Single CNN Forward Pass\n",
    "- Process entire image once\n",
    "- Extract region features from shared feature map\n",
    "\n",
    "#### 2. ROI (Region of Interest) Pooling\n",
    "- Maps arbitrary-sized regions to fixed-size (7×7) features\n",
    "- Enables batch processing\n",
    "\n",
    "#### 3. Multi-Task Loss\n",
    "- Simultaneous classification and localization\n",
    "- L = L_cls + λ * L_bbox\n",
    "- End-to-end training (except region proposals)\n",
    "\n",
    "#### 4. No Disk Storage\n",
    "- Features computed on-the-fly\n",
    "\n",
    "### Performance:\n",
    "- **mAP on PASCAL VOC 2012**: 66.1% (+12.8%)\n",
    "- **Speed**: 2 seconds per image (GPU)\n",
    "- **Speedup**: 25× faster training, 146× faster testing!\n",
    "- **Training**: Single-stage (9 hours vs 2.5 days)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Fast R-CNN Remaining Bottleneck\n",
    "\n",
    "Fast R-CNN solved the CNN computation problem, but...\n",
    "\n",
    "### The Bottleneck: Selective Search\n",
    "\n",
    "**Timing Breakdown:**\n",
    "- Selective Search: **2 seconds per image**\n",
    "- CNN + Detection: 0.3 seconds per image\n",
    "\n",
    "**Problem**: Selective Search takes 87% of total time!\n",
    "\n",
    "### Why Selective Search is Problematic:\n",
    "\n",
    "1. **Not Learned**: Fixed algorithm, can't adapt to your dataset\n",
    "2. **Slow**: CPU-only, 1-2 seconds per image\n",
    "3. **Low Quality**: Generates many low-quality proposals\n",
    "4. **Not End-to-End**: Can't train the full system jointly\n",
    "\n",
    "### The Question:\n",
    "Can we **learn** to generate region proposals?\n",
    "\n",
    "**Answer**: Yes! → Region Proposal Network (RPN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Faster R-CNN (2015) - The Breakthrough\n",
    "\n",
    "**Authors:** Shaoqing Ren, Kaiming He, Ross Girshick, Jian Sun (Microsoft Research)\n",
    "\n",
    "**Paper:** \"Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks\"\n",
    "\n",
    "### Revolutionary Idea: Region Proposal Network (RPN)\n",
    "\n",
    "```\n",
    "Input Image\n",
    "    ↓\n",
    "Backbone CNN (ResNet/VGG) → Shared Feature Map\n",
    "    ↓\n",
    "    ├─────────────────────┐\n",
    "    ↓                     ↓\n",
    "Region Proposal Network   ROI Pooling + Detection Head\n",
    "(RPN)                     (Fast R-CNN)\n",
    "    ↓                     ↓\n",
    "~300 proposals   →   Classification + Box Refinement\n",
    "                          ↓\n",
    "                    Output: Detected objects\n",
    "```\n",
    "\n",
    "### Region Proposal Network (RPN) Details:\n",
    "\n",
    "#### Concept:\n",
    "- Slide a small network over feature map\n",
    "- At each position, predict:\n",
    "  - **Objectness**: Is there an object? (2 scores: yes/no)\n",
    "  - **Box Coordinates**: Where is it? (4 values: x, y, w, h)\n",
    "\n",
    "#### Anchor Boxes:\n",
    "- Pre-defined boxes at multiple scales and aspect ratios\n",
    "- Typically: 3 scales × 3 ratios = 9 anchors per position\n",
    "- Example: {128², 256², 512²} × {1:1, 1:2, 2:1}\n",
    "\n",
    "#### Training:\n",
    "- **Positive anchors**: IOU > 0.7 with ground truth\n",
    "- **Negative anchors**: IOU < 0.3 with all ground truth\n",
    "- Loss: L_rpn = L_cls + L_bbox\n",
    "\n",
    "### End-to-End Training:\n",
    "1. Train RPN\n",
    "2. Train Fast R-CNN using RPN proposals\n",
    "3. Fine-tune RPN\n",
    "4. Fine-tune Fast R-CNN\n",
    "- **Result**: Fully trainable detection pipeline!\n",
    "\n",
    "### Performance:\n",
    "- **mAP on PASCAL VOC 2007**: 73.2%\n",
    "- **Speed**: 0.2 seconds per image (5 FPS on GPU)\n",
    "- **Speedup**: 10× faster than Fast R-CNN!\n",
    "- **Accuracy**: Slightly better than Fast R-CNN\n",
    "- **Game-changer**: First \"nearly real-time\" two-stage detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timeline Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Data\n",
    "models = ['R-CNN\\n(2014)', 'Fast R-CNN\\n(2015)', 'Faster R-CNN\\n(2015)']\n",
    "speed = [47, 2, 0.2]  # seconds per image\n",
    "fps = [1/s for s in speed]  # frames per second\n",
    "mAP = [53.3, 66.1, 73.2]  # mean Average Precision\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Speed comparison\n",
    "colors = ['#e74c3c', '#f39c12', '#27ae60']\n",
    "bars1 = ax1.bar(models, speed, color=colors, alpha=0.7, edgecolor='black')\n",
    "ax1.set_ylabel('Seconds per Image (lower is better)', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Speed Evolution: 235× Speedup!', fontsize=14, fontweight='bold')\n",
    "ax1.set_yscale('log')\n",
    "ax1.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "\n",
    "# Add FPS annotations\n",
    "for bar, f in zip(bars1, fps):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, height,\n",
    "             f'{f:.1f} FPS', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "# Accuracy comparison\n",
    "bars2 = ax2.bar(models, mAP, color=colors, alpha=0.7, edgecolor='black')\n",
    "ax2.set_ylabel('mAP (%) on PASCAL VOC (higher is better)', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Accuracy Evolution: +20% mAP Improvement', fontsize=14, fontweight='bold')\n",
    "ax2.set_ylim([0, 80])\n",
    "ax2.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "\n",
    "# Add value annotations\n",
    "for bar, m in zip(bars2, mAP):\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, height,\n",
    "             f'{m:.1f}%', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('rcnn_evolution.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Evolution Summary:\")\n",
    "print(f\"Speed improvement: {speed[0]/speed[2]:.0f}× faster (47s → 0.2s)\")\n",
    "print(f\"Accuracy improvement: +{mAP[2]-mAP[0]:.1f}% mAP (53.3% → 73.2%)\")\n",
    "print(f\"\\nFaster R-CNN achieved BOTH speed AND accuracy improvements!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Architecture Comparison Table\n",
    "\n",
    "| Feature | R-CNN (2014) | Fast R-CNN (2015) | Faster R-CNN (2015) |\n",
    "|---------|--------------|-------------------|---------------------|\n",
    "| **Region Proposals** | Selective Search | Selective Search | **RPN (learned)** |\n",
    "| **CNN Forward Pass** | 2000× per image | **1× per image** | **1× per image** |\n",
    "| **Feature Extraction** | Per-region warping | ROI Pooling | ROI Pooling/Align |\n",
    "| **Classification** | SVM (separate) | Softmax (integrated) | Softmax (integrated) |\n",
    "| **Bounding Box** | Linear regressor | Multi-task loss | Multi-task loss |\n",
    "| **Training** | 3-stage pipeline | 2-stage (Selective Search separate) | **End-to-end** |\n",
    "| **Training Time** | 84 hours (2.5 GPU-days) | 9 hours | 12 hours |\n",
    "| **Speed (seconds/img)** | 47s | 2s | **0.2s** |\n",
    "| **FPS** | 0.02 | 0.5 | **5** |\n",
    "| **mAP (PASCAL VOC)** | 53.3% | 66.1% | **73.2%** |\n",
    "| **Storage Required** | Hundreds of GB | Minimal | Minimal |\n",
    "| **Main Bottleneck** | CNN computation | Selective Search | NMS post-processing |\n",
    "| **Innovation** | CNN for detection | Shared computation | Learned proposals |\n",
    "\n",
    "### Key Takeaways:\n",
    "1. **R-CNN**: Pioneered CNN-based detection but impractical speed\n",
    "2. **Fast R-CNN**: Solved CNN redundancy, 25× speedup\n",
    "3. **Faster R-CNN**: Solved proposal generation, end-to-end trainable, 10× speedup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Key Innovations Summary\n",
    "\n",
    "### R-CNN (2014) Innovations:\n",
    "1. **Apply CNNs to object detection** (first major success)\n",
    "2. Pre-training on ImageNet + fine-tuning on detection data\n",
    "3. Bounding box regression for precise localization\n",
    "\n",
    "### Fast R-CNN (2015) Innovations:\n",
    "1. **Shared convolutional computation** across regions\n",
    "2. **ROI Pooling layer** for efficient feature extraction\n",
    "3. **Multi-task loss** for joint training\n",
    "4. Single-stage training (except proposals)\n",
    "\n",
    "### Faster R-CNN (2015) Innovations:\n",
    "1. **Region Proposal Network (RPN)** - learned proposals\n",
    "2. **Anchor boxes** concept (later used in YOLO, SSD)\n",
    "3. **End-to-end trainable** detection system\n",
    "4. **Attention mechanism** (RPN focuses on likely object regions)\n",
    "\n",
    "### Impact on Field:\n",
    "- **R-CNN**: Proved CNNs work for detection (2014)\n",
    "- **Fast R-CNN**: Made two-stage detection practical (2015)\n",
    "- **Faster R-CNN**: Became the standard for high-accuracy detection (2015-2020)\n",
    "- **Legacy**: Anchor boxes, FPN, Mask R-CNN all built on Faster R-CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Exercise: Compare R-CNN Family\n",
    "\n",
    "**Scenario**: You're explaining object detection evolution to a colleague.\n",
    "\n",
    "### Questions:\n",
    "\n",
    "1. **Why was R-CNN groundbreaking despite being slow?**\n",
    "   - *Hint: Think about accuracy improvement and approach*\n",
    "\n",
    "2. **What single change made Fast R-CNN 25× faster?**\n",
    "   - *Hint: Where was computation redundant?*\n",
    "\n",
    "3. **Why is Faster R-CNN called \"end-to-end trainable\"?**\n",
    "   - *Hint: Compare training pipelines*\n",
    "\n",
    "4. **When would you choose Faster R-CNN over YOLO?**\n",
    "   - *Hint: Consider accuracy vs speed trade-off*\n",
    "\n",
    "5. **What concept from Faster R-CNN did YOLO adopt?**\n",
    "   - *Hint: Think about proposal generation*\n",
    "\n",
    "### Answers:\n",
    "*(Think first, then expand)*\n",
    "\n",
    "<details>\n",
    "<summary>Click to reveal answers</summary>\n",
    "\n",
    "1. **R-CNN breakthrough**: \n",
    "   - First to apply CNNs successfully to detection\n",
    "   - 30% mAP improvement over previous methods\n",
    "   - Showed transfer learning works (ImageNet → detection)\n",
    "\n",
    "2. **Fast R-CNN speedup**:\n",
    "   - Single CNN forward pass for entire image\n",
    "   - ROI Pooling extracts features from shared map\n",
    "   - Eliminated 2000× redundant CNN computation\n",
    "\n",
    "3. **End-to-end trainable**:\n",
    "   - All components trained with gradients\n",
    "   - RPN + detector trained jointly\n",
    "   - No separate Selective Search or SVM stages\n",
    "\n",
    "4. **Choose Faster R-CNN when**:\n",
    "   - Small objects critical (medical imaging)\n",
    "   - Accuracy more important than speed\n",
    "   - Offline analysis acceptable\n",
    "   - Need instance segmentation (Mask R-CNN)\n",
    "\n",
    "5. **YOLO adopted**:\n",
    "   - Anchor boxes concept from RPN\n",
    "   - Multiple scales/ratios per grid cell\n",
    "   - Direct box prediction from features\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary & Next Steps\n",
    "\n",
    "### What We Learned:\n",
    "1. **Two-stage detection paradigm**: Proposals → Classification\n",
    "2. **R-CNN evolution**: Each version solved the previous bottleneck\n",
    "3. **Key innovations**: Shared computation, ROI pooling, learned proposals\n",
    "4. **Performance**: 235× speedup + 20% mAP improvement (2014-2015)\n",
    "\n",
    "### Timeline:\n",
    "- **2014**: R-CNN (pioneering but slow)\n",
    "- **2015**: Fast R-CNN (shared computation)\n",
    "- **2015**: Faster R-CNN (learned proposals) ← Still widely used!\n",
    "- **2017**: Mask R-CNN (added segmentation)\n",
    "- **2019**: Cascade R-CNN (iterative refinement)\n",
    "\n",
    "### Next Notebook Preview:\n",
    "**Notebook 02**: Region Proposals & Selective Search\n",
    "- How does Selective Search work?\n",
    "- Hands-on implementation\n",
    "- Why RPN replaced it\n",
    "\n",
    "---\n",
    "\n",
    "**Estimated completion time**: 10 minutes\n",
    "\n",
    "**Key takeaway**: Faster R-CNN's success came from eliminating bottlenecks systematically while maintaining accuracy. Understanding this evolution helps you choose the right detector for your application!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
