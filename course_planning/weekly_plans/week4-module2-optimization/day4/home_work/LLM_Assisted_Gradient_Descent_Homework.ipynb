{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# ü§ñ LLM-Assisted Gradient Descent Learning\n**Course:** Deep Neural Network Architectures (21CSE558T)  \n**Module:** 2 - Optimization and Regularization  \n**Assignment:** Week 4, Day 4 - Interactive LLM Homework  \n**Due:** Before Week 5, Day 1  \n\n---\n\n**¬© 2025 Prof. Ramesh Babu | SRM University | Data Science and Business Systems (DSBS)**  \n*Course Materials for 21CSE558T - Deep Neural Network Architectures*\n\n---\n\n## üéØ Learning Revolution: AI-Assisted Programming\n\nWelcome to the future of learning! In this assignment, you'll master gradient descent concepts while learning to effectively collaborate with Large Language Models (LLMs) like Claude, ChatGPT, or Gemini.\n\n### Why This Approach?\n- **Real-world skill**: Professional developers increasingly use AI assistance\n- **Deeper understanding**: Explaining requirements to LLMs reinforces your knowledge\n- **Prompt engineering**: Critical skill for the AI era\n- **Code evaluation**: Learn to critically assess AI-generated solutions\n\n### Learning Objectives\nBy completing this assignment, you will:\n1. **Master gradient descent** through hands-on LLM collaboration\n2. **Develop prompt engineering skills** for technical tasks\n3. **Learn to verify and debug** AI-generated code\n4. **Practice iterative refinement** of solutions\n5. **Build confidence** in AI-assisted development\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Prompt Engineering Guide for Technical Tasks\n",
    "\n",
    "### üèÜ The CLEAR Framework for LLM Prompts\n",
    "\n",
    "**C** - **Context**: Provide background and domain  \n",
    "**L** - **Language**: Specify programming language and libraries  \n",
    "**E** - **Examples**: Include input/output examples when helpful  \n",
    "**A** - **Approach**: Mention preferred methods or constraints  \n",
    "**R** - **Requirements**: Be specific about what you need  \n",
    "\n",
    "### ‚úÖ Good Prompt Examples:\n",
    "\n",
    "**Good Prompt:**\n",
    "```\n",
    "I'm learning gradient descent for neural networks. Can you write a Python function \n",
    "that implements batch gradient descent for linear regression? \n",
    "\n",
    "Requirements:\n",
    "- Use NumPy for matrix operations\n",
    "- Function should take X (features), y (targets), learning_rate, and epochs\n",
    "- Return the learned weights and cost history\n",
    "- Include comments explaining each step\n",
    "- Use Mean Squared Error as the cost function\n",
    "\n",
    "Please also explain why we divide gradients by the number of samples.\n",
    "```\n",
    "\n",
    "**‚ùå Poor Prompt:**\n",
    "```\n",
    "Write gradient descent code\n",
    "```\n",
    "\n",
    "### üõ†Ô∏è Iterative Improvement Strategy\n",
    "1. **Start broad** ‚Üí Get basic implementation\n",
    "2. **Add specifics** ‚Üí Request improvements or modifications\n",
    "3. **Ask for explanations** ‚Üí Understand the \"why\" behind the code\n",
    "4. **Request variations** ‚Üí Compare different approaches\n",
    "\n",
    "### üîç Code Verification Checklist\n",
    "- [ ] **Does it run without errors?**\n",
    "- [ ] **Are variable names meaningful?**\n",
    "- [ ] **Is the math correct?** (gradients, cost function)\n",
    "- [ ] **Does it handle edge cases?** (empty data, wrong dimensions)\n",
    "- [ ] **Are there helpful comments?**\n",
    "- [ ] **Does it match the requirements?**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Getting Started: Choose Your LLM Partner\n",
    "\n",
    "### Recommended LLMs for This Assignment:\n",
    "1. **Claude** (claude.ai) - Excellent for educational explanations\n",
    "2. **ChatGPT** (chat.openai.com) - Great for iterative coding\n",
    "3. **Gemini** (gemini.google.com) - Good for mathematical explanations\n",
    "\n",
    "### Setup Instructions:\n",
    "1. Open your chosen LLM in a new browser tab\n",
    "2. Keep this Colab notebook open alongside\n",
    "3. Copy prompts from this notebook to your LLM\n",
    "4. Copy LLM responses back to the code cells below\n",
    "5. Test and verify each solution\n",
    "\n",
    "**‚ú® Pro Tip:** You can use multiple LLMs to compare approaches!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì¶ Initial Setup - Run this cell first\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from sklearn.datasets import make_regression, load_boston\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"üéâ Setup complete! Ready to start LLM-assisted learning.\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(\"\\nüí° Remember: Copy prompts to your LLM, then paste the code responses here!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# üéì Exercise 1: Basic Gradient Descent Foundation\n",
    "\n",
    "## üéØ Learning Goal\n",
    "Understand the core mechanics of gradient descent by implementing it for simple linear regression.\n",
    "\n",
    "## üìù Prompt for Your LLM\n",
    "\n",
    "**Copy this prompt to your chosen LLM:**\n",
    "\n",
    "```\n",
    "I'm a M.Tech student learning gradient descent for deep learning. I need to implement \n",
    "basic gradient descent for linear regression from scratch.\n",
    "\n",
    "Requirements:\n",
    "- Write a Python function called `simple_gradient_descent`\n",
    "- Use only NumPy (no sklearn for the optimization part)\n",
    "- Function parameters: X (features), y (targets), learning_rate=0.01, epochs=1000\n",
    "- Return: final weights, bias, and cost_history list\n",
    "- Use Mean Squared Error as cost function\n",
    "- Include detailed comments explaining the math\n",
    "- Add print statements every 100 epochs to show progress\n",
    "\n",
    "Also create a simple test with synthetic data (y = 3x + 2 + noise) and \n",
    "visualize the cost function over epochs.\n",
    "\n",
    "Please explain why we use the derivative of MSE for gradients.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ü§ñ PASTE LLM RESPONSE HERE - Exercise 1\n",
    "# Copy the gradient descent function from your LLM response below:\n",
    "\n",
    "# YOUR LLM-GENERATED CODE GOES HERE\n",
    "\n",
    "\n",
    "# Test the function here (if LLM provided test code):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß™ Verification Cell - Exercise 1\n",
    "# This cell will help you verify your LLM-generated code works correctly\n",
    "\n",
    "print(\"=== Exercise 1 Verification ===\")\n",
    "\n",
    "# Create test data\n",
    "np.random.seed(42)\n",
    "X_test = np.random.randn(100, 1)\n",
    "y_test = 3 * X_test.squeeze() + 2 + 0.1 * np.random.randn(100)\n",
    "\n",
    "try:\n",
    "    # Test your function (uncomment when you have the function)\n",
    "    # weights, bias, costs = simple_gradient_descent(X_test, y_test, learning_rate=0.01, epochs=500)\n",
    "    \n",
    "    # Verification checks\n",
    "    print(\"‚úÖ Function runs without errors\")\n",
    "    # print(f\"‚úÖ Final weight: {weights[0]:.3f} (should be close to 3.0)\")\n",
    "    # print(f\"‚úÖ Final bias: {bias:.3f} (should be close to 2.0)\")\n",
    "    # print(f\"‚úÖ Cost decreased: {costs[0]:.3f} ‚Üí {costs[-1]:.3f}\")\n",
    "    \n",
    "    print(\"\\nüéØ If weights ‚âà 3.0 and bias ‚âà 2.0, your implementation is correct!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "    print(\"üí° Check your function definition and try again\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§î Reflection Questions - Exercise 1\n",
    "\n",
    "**Answer these after completing Exercise 1:**\n",
    "\n",
    "1. **How did your LLM explain the gradient calculation?**\n",
    "   - Your answer: *(Write what you learned about why we use MSE derivatives)*\n",
    "\n",
    "2. **What was the most helpful part of the LLM's explanation?**\n",
    "   - Your answer: \n",
    "\n",
    "3. **Did you need to refine your prompt? How?**\n",
    "   - Your answer: \n",
    "\n",
    "4. **How close were your final weights to the true values (3.0 and 2.0)?**\n",
    "   - Your answer: \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéì Exercise 2: Batch vs Stochastic Gradient Descent\n",
    "\n",
    "## üéØ Learning Goal\n",
    "Compare batch and stochastic gradient descent to understand their trade-offs.\n",
    "\n",
    "## üìù Prompt for Your LLM\n",
    "\n",
    "```\n",
    "I need to compare Batch Gradient Descent with Stochastic Gradient Descent for my deep learning course.\n",
    "\n",
    "Please create two functions:\n",
    "\n",
    "1. `batch_gradient_descent(X, y, learning_rate=0.01, epochs=1000)`\n",
    "   - Uses all samples to compute gradients each iteration\n",
    "   - Returns weights, bias, cost_history\n",
    "\n",
    "2. `stochastic_gradient_descent(X, y, learning_rate=0.01, epochs=1000)`\n",
    "   - Uses one sample at a time for gradient computation\n",
    "   - Shuffle data each epoch\n",
    "   - Returns weights, bias, cost_history\n",
    "\n",
    "Requirements:\n",
    "- Use NumPy for all operations\n",
    "- Both should work with multi-feature datasets\n",
    "- Include timing measurements\n",
    "- Add clear comments explaining the differences\n",
    "- Create a comparison test that shows convergence patterns\n",
    "\n",
    "Also explain:\n",
    "- Why SGD might converge faster initially\n",
    "- Why BGD gives smoother convergence\n",
    "- When to use each method\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ü§ñ PASTE LLM RESPONSE HERE - Exercise 2\n",
    "# Copy both functions from your LLM response:\n",
    "\n",
    "# YOUR LLM-GENERATED CODE FOR BATCH GD:\n",
    "\n",
    "\n",
    "# YOUR LLM-GENERATED CODE FOR STOCHASTIC GD:\n",
    "\n",
    "\n",
    "# Any test/comparison code from LLM:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß™ Verification Cell - Exercise 2\n",
    "print(\"=== Exercise 2 Verification ===\")\n",
    "\n",
    "# Create a more complex dataset\n",
    "X_multi = np.random.randn(500, 3)  # 3 features\n",
    "true_weights = np.array([2, -1, 0.5])\n",
    "y_multi = X_multi @ true_weights + 1 + 0.1 * np.random.randn(500)\n",
    "\n",
    "try:\n",
    "    print(\"Testing Batch Gradient Descent...\")\n",
    "    start_time = time.time()\n",
    "    # bgd_weights, bgd_bias, bgd_costs = batch_gradient_descent(X_multi, y_multi, epochs=200)\n",
    "    bgd_time = time.time() - start_time\n",
    "    \n",
    "    print(\"\\nTesting Stochastic Gradient Descent...\")\n",
    "    start_time = time.time()\n",
    "    # sgd_weights, sgd_bias, sgd_costs = stochastic_gradient_descent(X_multi, y_multi, epochs=200)\n",
    "    sgd_time = time.time() - start_time\n",
    "    \n",
    "    # Comparison\n",
    "    print(f\"\\nüìä Timing Comparison:\")\n",
    "    print(f\"   BGD: {bgd_time:.3f} seconds\")\n",
    "    print(f\"   SGD: {sgd_time:.3f} seconds\")\n",
    "    \n",
    "    # Weight comparison\n",
    "    print(f\"\\nüéØ Weight Accuracy:\")\n",
    "    print(f\"   True weights: {true_weights}\")\n",
    "    # print(f\"   BGD weights: {bgd_weights}\")\n",
    "    # print(f\"   SGD weights: {sgd_weights}\")\n",
    "    \n",
    "    print(\"\\n‚úÖ Both methods implemented successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "    print(\"üí° Check your function implementations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Visualization Cell - Exercise 2\n",
    "# Create convergence comparison plot\n",
    "\n",
    "try:\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    # Plot BGD convergence\n",
    "    plt.subplot(1, 2, 1)\n",
    "    # plt.plot(bgd_costs, 'b-', linewidth=2, label='Batch GD')\n",
    "    plt.title('Batch Gradient Descent')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Cost')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot SGD convergence\n",
    "    plt.subplot(1, 2, 2)\n",
    "    # plt.plot(sgd_costs, 'r-', alpha=0.7, label='Stochastic GD')\n",
    "    plt.title('Stochastic Gradient Descent')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Cost')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"üìà Notice the difference in convergence smoothness!\")\n",
    "    \n",
    "except:\n",
    "    print(\"üìä Run your functions first to see the comparison plots\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§î Reflection Questions - Exercise 2\n",
    "\n",
    "1. **Which method converged faster? Why?**\n",
    "   - Your answer: \n",
    "\n",
    "2. **What did you notice about the smoothness of convergence?**\n",
    "   - Your answer: \n",
    "\n",
    "3. **How did the LLM explain the trade-offs between BGD and SGD?**\n",
    "   - Your answer: \n",
    "\n",
    "4. **Which method would you choose for a very large dataset? Why?**\n",
    "   - Your answer: \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéì Exercise 3: Mini-batch Gradient Descent\n",
    "\n",
    "## üéØ Learning Goal\n",
    "Implement mini-batch GD and explore the effect of different batch sizes.\n",
    "\n",
    "## üìù Prompt for Your LLM\n",
    "\n",
    "```\n",
    "I need to implement Mini-batch Gradient Descent that combines the benefits of \n",
    "both Batch and Stochastic Gradient Descent.\n",
    "\n",
    "Create a function: `mini_batch_gradient_descent(X, y, batch_size=32, learning_rate=0.01, epochs=1000)`\n",
    "\n",
    "Requirements:\n",
    "- Process data in small batches of specified size\n",
    "- Shuffle data at the beginning of each epoch\n",
    "- Handle cases where data doesn't divide evenly by batch_size\n",
    "- Track cost after each epoch (not each batch)\n",
    "- Include timing measurements\n",
    "- Return weights, bias, cost_history, and batches_per_epoch\n",
    "\n",
    "Also create a comparison function that tests different batch sizes: [1, 16, 32, 64, 128, 'full']\n",
    "and plots their convergence patterns on the same graph.\n",
    "\n",
    "Explain:\n",
    "- How batch size affects convergence speed and stability\n",
    "- Why mini-batch is often preferred in deep learning\n",
    "- How to choose an appropriate batch size\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ü§ñ PASTE LLM RESPONSE HERE - Exercise 3\n",
    "# Copy the mini-batch GD function and comparison code:\n",
    "\n",
    "# YOUR LLM-GENERATED MINI-BATCH GD FUNCTION:\n",
    "\n",
    "\n",
    "# YOUR LLM-GENERATED BATCH SIZE COMPARISON CODE:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß™ Verification Cell - Exercise 3\n",
    "print(\"=== Exercise 3 Verification ===\")\n",
    "\n",
    "# Create test dataset\n",
    "X_test = np.random.randn(1000, 2)\n",
    "y_test = X_test @ np.array([1.5, -2.0]) + 0.5 + 0.1 * np.random.randn(1000)\n",
    "\n",
    "batch_sizes_to_test = [1, 16, 32, 64, 128, len(X_test)]  # 1=SGD, len(X_test)=BGD\n",
    "\n",
    "try:\n",
    "    results = {}\n",
    "    \n",
    "    for bs in batch_sizes_to_test:\n",
    "        print(f\"\\nTesting batch size: {bs}\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Test your mini-batch function\n",
    "        # weights, bias, costs, batches_per_epoch = mini_batch_gradient_descent(\n",
    "        #     X_test, y_test, batch_size=bs, epochs=100\n",
    "        # )\n",
    "        \n",
    "        training_time = time.time() - start_time\n",
    "        \n",
    "        # results[bs] = {\n",
    "        #     'weights': weights,\n",
    "        #     'costs': costs,\n",
    "        #     'time': training_time,\n",
    "        #     'final_cost': costs[-1]\n",
    "        # }\n",
    "        \n",
    "        # print(f\"   Final cost: {costs[-1]:.4f}, Time: {training_time:.3f}s\")\n",
    "    \n",
    "    print(\"\\n‚úÖ Mini-batch GD testing complete!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "    print(\"üí° Check your mini-batch function implementation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§î Reflection Questions - Exercise 3\n",
    "\n",
    "1. **Which batch size gave the best balance of speed and stability?**\n",
    "   - Your answer: \n",
    "\n",
    "2. **How did very small vs very large batch sizes perform?**\n",
    "   - Your answer: \n",
    "\n",
    "3. **What did the LLM suggest for choosing batch size in practice?**\n",
    "   - Your answer: \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéì Exercise 4: Learning Rate Experiments\n",
    "\n",
    "## üéØ Learning Goal\n",
    "Understand the critical role of learning rate in gradient descent optimization.\n",
    "\n",
    "## üìù Prompt for Your LLM\n",
    "\n",
    "```\n",
    "I need to understand how learning rate affects gradient descent convergence. \n",
    "\n",
    "Create a function `learning_rate_experiment(X, y, learning_rates, epochs=500)` that:\n",
    "- Tests multiple learning rates: [0.001, 0.01, 0.05, 0.1, 0.2, 0.5, 1.0]\n",
    "- Uses mini-batch gradient descent with batch_size=32\n",
    "- Handles divergence (when cost becomes very large or NaN)\n",
    "- Returns results dictionary with convergence info for each learning rate\n",
    "- Creates a visualization showing:\n",
    "  1. Convergence curves for all stable learning rates\n",
    "  2. Final cost vs learning rate plot\n",
    "  3. Training time comparison\n",
    "\n",
    "Also implement adaptive learning rate that decreases over time:\n",
    "`adaptive_learning_rate_gd(X, y, initial_lr=0.1, decay_rate=0.95, epochs=500)`\n",
    "\n",
    "Explain:\n",
    "- What happens with too small learning rates\n",
    "- What happens with too large learning rates  \n",
    "- How to identify the optimal learning rate\n",
    "- Benefits of adaptive learning rates\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ü§ñ PASTE LLM RESPONSE HERE - Exercise 4\n",
    "# Copy the learning rate experiment functions:\n",
    "\n",
    "# YOUR LLM-GENERATED LEARNING RATE EXPERIMENT FUNCTION:\n",
    "\n",
    "\n",
    "# YOUR LLM-GENERATED ADAPTIVE LEARNING RATE FUNCTION:\n",
    "\n",
    "\n",
    "# Any visualization code from LLM:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß™ Verification Cell - Exercise 4\n",
    "print(\"=== Exercise 4 Verification ===\")\n",
    "\n",
    "# Create challenging dataset\n",
    "X_lr = np.random.randn(800, 3)\n",
    "y_lr = X_lr @ np.array([2, -1, 0.5]) + 1 + 0.2 * np.random.randn(800)\n",
    "\n",
    "try:\n",
    "    print(\"üî¨ Running learning rate experiments...\")\n",
    "    \n",
    "    # Test learning rate experiment function\n",
    "    # lr_results = learning_rate_experiment(X_lr, y_lr)\n",
    "    \n",
    "    print(\"\\nüîÑ Testing adaptive learning rate...\")\n",
    "    \n",
    "    # Test adaptive learning rate\n",
    "    # adaptive_weights, adaptive_costs = adaptive_learning_rate_gd(X_lr, y_lr)\n",
    "    \n",
    "    print(\"\\n‚úÖ Learning rate experiments complete!\")\n",
    "    print(\"üìä Check the visualizations to understand learning rate effects\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "    print(\"üí° Check your learning rate experiment functions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§î Reflection Questions - Exercise 4\n",
    "\n",
    "1. **What was the optimal learning rate for your dataset?**\n",
    "   - Your answer: \n",
    "\n",
    "2. **Which learning rates caused divergence? What did that look like?**\n",
    "   - Your answer: \n",
    "\n",
    "3. **How did adaptive learning rate compare to fixed learning rate?**\n",
    "   - Your answer: \n",
    "\n",
    "4. **What strategy would you use to find optimal learning rate in practice?**\n",
    "   - Your answer: \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéì Exercise 5: Real Dataset Application\n",
    "\n",
    "## üéØ Learning Goal\n",
    "Apply all gradient descent variants to a real-world dataset and compare performance.\n",
    "\n",
    "## üìù Prompt for Your LLM\n",
    "\n",
    "```\n",
    "Now I want to apply everything I've learned to the Boston Housing dataset for real-world validation.\n",
    "\n",
    "Create a comprehensive comparison function `boston_housing_comparison()` that:\n",
    "\n",
    "1. Loads and preprocesses Boston Housing data:\n",
    "   - Standardizes features (mean=0, std=1)\n",
    "   - Splits into train/test (80/20)\n",
    "   - Adds bias column for intercept\n",
    "\n",
    "2. Implements and compares all methods:\n",
    "   - Batch Gradient Descent\n",
    "   - Stochastic Gradient Descent  \n",
    "   - Mini-batch GD (batch sizes: 16, 32, 64)\n",
    "   - Adaptive learning rate mini-batch GD\n",
    "\n",
    "3. Evaluates performance:\n",
    "   - Training time for each method\n",
    "   - Final training cost\n",
    "   - Test set Mean Squared Error\n",
    "   - R-squared score\n",
    "\n",
    "4. Creates comprehensive visualizations:\n",
    "   - Convergence comparison plot\n",
    "   - Performance metrics bar chart\n",
    "   - Predictions vs actual scatter plot for best method\n",
    "\n",
    "Also provide practical recommendations for:\n",
    "- Which method to use for different dataset sizes\n",
    "- How to tune hyperparameters efficiently\n",
    "- When each approach is most suitable\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ü§ñ PASTE LLM RESPONSE HERE - Exercise 5\n",
    "# Copy the Boston Housing comparison function:\n",
    "\n",
    "# YOUR LLM-GENERATED BOSTON HOUSING COMPARISON FUNCTION:\n",
    "\n",
    "\n",
    "# Any additional helper functions from LLM:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß™ Verification Cell - Exercise 5\n",
    "print(\"=== Exercise 5 Verification - Boston Housing ===\")\n",
    "\n",
    "try:\n",
    "    print(\"üè† Loading Boston Housing dataset...\")\n",
    "    \n",
    "    # Load the actual dataset\n",
    "    boston = load_boston()\n",
    "    X_boston, y_boston = boston.data, boston.target\n",
    "    \n",
    "    print(f\"   Dataset shape: {X_boston.shape}\")\n",
    "    print(f\"   Target range: ${y_boston.min():.1f}k - ${y_boston.max():.1f}k\")\n",
    "    \n",
    "    # Run your comprehensive comparison\n",
    "    print(\"\\nüìä Running comprehensive gradient descent comparison...\")\n",
    "    # results = boston_housing_comparison()\n",
    "    \n",
    "    print(\"\\n‚úÖ Real dataset analysis complete!\")\n",
    "    print(\"üéØ Check the results to see which method performed best\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "    print(\"üí° Make sure your boston_housing_comparison function is implemented\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§î Reflection Questions - Exercise 5\n",
    "\n",
    "1. **Which gradient descent variant performed best on the Boston Housing dataset?**\n",
    "   - Your answer: \n",
    "\n",
    "2. **How did real-world performance compare to synthetic data experiments?**\n",
    "   - Your answer: \n",
    "\n",
    "3. **What practical insights did you gain about choosing optimization methods?**\n",
    "   - Your answer: \n",
    "\n",
    "4. **What would you do differently if you had 10x more data?**\n",
    "   - Your answer: \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéì Exercise 6: Advanced Optimization (Bonus)\n",
    "\n",
    "## üéØ Learning Goal\n",
    "Explore advanced optimization techniques that build on gradient descent.\n",
    "\n",
    "## üìù Prompt for Your LLM\n",
    "\n",
    "```\n",
    "I want to explore advanced optimization techniques that improve upon basic gradient descent.\n",
    "\n",
    "Please implement two advanced optimizers:\n",
    "\n",
    "1. `momentum_gradient_descent(X, y, learning_rate=0.01, momentum=0.9, epochs=1000)`:\n",
    "   - Implements momentum to accelerate convergence\n",
    "   - Maintains velocity from previous updates\n",
    "   - Helps escape local minima and speeds up training\n",
    "\n",
    "2. `adam_optimizer(X, y, learning_rate=0.001, beta1=0.9, beta2=0.999, epochs=1000)`:\n",
    "   - Implements Adam optimizer (adaptive moments)\n",
    "   - Combines momentum with adaptive learning rates\n",
    "   - Industry standard for deep learning\n",
    "\n",
    "Create a comparison function that:\n",
    "- Tests all methods (basic GD, momentum, Adam) on the same data\n",
    "- Shows convergence curves side by side\n",
    "- Compares final performance and training time\n",
    "- Demonstrates why these advanced methods are preferred\n",
    "\n",
    "Explain:\n",
    "- How momentum helps with convergence\n",
    "- Why Adam is so popular in deep learning\n",
    "- When to use each optimization method\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ü§ñ PASTE LLM RESPONSE HERE - Exercise 6 (Bonus)\n",
    "# Copy the advanced optimizer implementations:\n",
    "\n",
    "# YOUR LLM-GENERATED MOMENTUM GD FUNCTION:\n",
    "\n",
    "\n",
    "# YOUR LLM-GENERATED ADAM OPTIMIZER FUNCTION:\n",
    "\n",
    "\n",
    "# YOUR LLM-GENERATED COMPARISON FUNCTION:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß™ Verification Cell - Exercise 6 (Bonus)\n",
    "print(\"=== Exercise 6 Verification - Advanced Optimization ===\")\n",
    "\n",
    "# Create a challenging optimization landscape\n",
    "X_advanced = np.random.randn(600, 4)\n",
    "y_advanced = X_advanced @ np.array([1, -2, 0.5, 1.5]) + 0.5 + 0.1 * np.random.randn(600)\n",
    "\n",
    "try:\n",
    "    print(\"üöÄ Testing advanced optimization methods...\")\n",
    "    \n",
    "    # Test momentum GD\n",
    "    print(\"\\nüìà Testing Momentum Gradient Descent...\")\n",
    "    # momentum_weights, momentum_costs = momentum_gradient_descent(X_advanced, y_advanced)\n",
    "    \n",
    "    # Test Adam optimizer\n",
    "    print(\"\\nüß† Testing Adam Optimizer...\")\n",
    "    # adam_weights, adam_costs = adam_optimizer(X_advanced, y_advanced)\n",
    "    \n",
    "    print(\"\\n‚úÖ Advanced optimization methods implemented successfully!\")\n",
    "    print(\"üéØ Compare convergence speeds to see the improvements\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "    print(\"üí° Check your advanced optimizer implementations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§î Reflection Questions - Exercise 6 (Bonus)\n",
    "\n",
    "1. **How much faster did momentum and Adam converge compared to basic GD?**\n",
    "   - Your answer: \n",
    "\n",
    "2. **What did the LLM explain about why Adam is so popular?**\n",
    "   - Your answer: \n",
    "\n",
    "3. **In what scenarios would you still prefer basic gradient descent?**\n",
    "   - Your answer: \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéâ Final Reflection: LLM-Assisted Learning Experience\n",
    "\n",
    "## ü§ñ Prompt Engineering Skills Assessment\n",
    "\n",
    "**Rate your improvement in these areas (1-5 scale):**\n",
    "\n",
    "1. **Writing clear, specific prompts**: ___/5\n",
    "2. **Iteratively refining requests**: ___/5  \n",
    "3. **Asking for explanations, not just code**: ___/5\n",
    "4. **Verifying and debugging LLM responses**: ___/5\n",
    "5. **Understanding when to use different LLMs**: ___/5\n",
    "\n",
    "## üéØ Technical Knowledge Gained\n",
    "\n",
    "**What's the most important insight you gained about:**\n",
    "\n",
    "1. **Gradient Descent Fundamentals:**\n",
    "   - Your answer: \n",
    "\n",
    "2. **Batch vs Stochastic vs Mini-batch Trade-offs:**\n",
    "   - Your answer: \n",
    "\n",
    "3. **Learning Rate Selection:**\n",
    "   - Your answer: \n",
    "\n",
    "4. **Real-world Application Considerations:**\n",
    "   - Your answer: \n",
    "\n",
    "## üöÄ LLM Collaboration Insights\n",
    "\n",
    "1. **Best Prompt Strategy:** What worked best for getting useful code?\n",
    "   - Your answer: \n",
    "\n",
    "2. **Most Helpful LLM Feature:** Explanations, code generation, debugging help?\n",
    "   - Your answer: \n",
    "\n",
    "3. **Biggest Challenge:** What was hardest about working with LLMs?\n",
    "   - Your answer: \n",
    "\n",
    "4. **Future Application:** How will you use LLMs in your future projects?\n",
    "   - Your answer: \n",
    "\n",
    "## üìà Learning Efficiency\n",
    "\n",
    "**Compared to traditional textbook/lecture learning:**\n",
    "\n",
    "- **Faster concept exploration:** Yes/No - Why?\n",
    "- **Deeper understanding:** Yes/No - Why?\n",
    "- **Better retention:** Yes/No - Why?\n",
    "- **More engaging:** Yes/No - Why?\n",
    "\n",
    "## üéØ Recommendations for Future Students\n",
    "\n",
    "**Your top 3 tips for effective LLM-assisted learning:**\n",
    "\n",
    "1. \n",
    "2. \n",
    "3. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìã Submission Checklist\n",
    "\n",
    "## ‚úÖ Completion Requirements\n",
    "\n",
    "**Core Exercises (Required):**\n",
    "- [ ] Exercise 1: Basic Gradient Descent ‚úì\n",
    "- [ ] Exercise 2: Batch vs Stochastic Comparison ‚úì\n",
    "- [ ] Exercise 3: Mini-batch Implementation ‚úì\n",
    "- [ ] Exercise 4: Learning Rate Experiments ‚úì\n",
    "- [ ] Exercise 5: Boston Housing Application ‚úì\n",
    "\n",
    "**Bonus Exercise (Optional):**\n",
    "- [ ] Exercise 6: Advanced Optimization (Momentum/Adam) ‚úì\n",
    "\n",
    "**Reflection Components:**\n",
    "- [ ] All reflection questions answered ‚úì\n",
    "- [ ] Final LLM learning assessment completed ‚úì\n",
    "- [ ] Prompt engineering insights documented ‚úì\n",
    "\n",
    "## üéØ Quality Checklist\n",
    "\n",
    "- [ ] **All code cells run without errors**\n",
    "- [ ] **LLM-generated code has been tested and verified**\n",
    "- [ ] **Visualizations display correctly**\n",
    "- [ ] **Reflection answers are thoughtful and complete**\n",
    "- [ ] **At least 3 different LLM prompts were refined iteratively**\n",
    "- [ ] **Evidence of critical evaluation of LLM responses**\n",
    "\n",
    "## üìä Grading Rubric (Total: 100 points)\n",
    "\n",
    "### Technical Implementation (50 points)\n",
    "- **Exercise 1-3**: 30 points (10 points each)\n",
    "- **Exercise 4-5**: 20 points (10 points each)\n",
    "- **Bonus Exercise 6**: +10 extra credit points\n",
    "\n",
    "### Prompt Engineering (25 points)\n",
    "- **Prompt quality and iteration**: 15 points\n",
    "- **Effective use of LLM capabilities**: 10 points\n",
    "\n",
    "### Analysis and Reflection (25 points)\n",
    "- **Technical understanding demonstrated**: 15 points\n",
    "- **Learning process reflection quality**: 10 points\n",
    "\n",
    "## üöÄ Ready for the Future!\n",
    "\n",
    "**Congratulations!** You've not only mastered gradient descent optimization but also developed crucial AI collaboration skills for the modern era of programming.\n",
    "\n",
    "**Skills you've gained:**\n",
    "- ‚úÖ Deep understanding of optimization algorithms\n",
    "- ‚úÖ Effective prompt engineering techniques\n",
    "- ‚úÖ Critical evaluation of AI-generated code\n",
    "- ‚úÖ Iterative problem-solving with LLMs\n",
    "- ‚úÖ Real-world application of theoretical concepts\n",
    "\n",
    "**Keep exploring!** These LLM collaboration skills will serve you well in all future programming projects. üéâ\n",
    "\n",
    "---\n",
    "**Submission Deadline:** Before Week 5, Day 1  \n",
    "**Submit:** This completed Colab notebook with all exercises and reflections"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}