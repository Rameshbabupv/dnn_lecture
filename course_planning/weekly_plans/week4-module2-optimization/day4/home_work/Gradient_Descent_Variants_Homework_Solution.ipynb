{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Gradient Descent Variants: Implementation & Analysis\n**Course:** Deep Neural Network Architectures (21CSE558T)  \n**Module:** 2 - Optimization and Regularization  \n**Assignment:** Week 4, Day 4 Homework  \n**Due:** Before Week 5, Day 1  \n\n---\n\n**© 2025 Prof. Ramesh Babu | SRM University | Data Science and Business Systems (DSBS)**  \n*Course Materials for 21CSE558T - Deep Neural Network Architectures*\n\n---\n\n## Learning Objectives\nBy completing this assignment, you will:\n1. **Implement** all three gradient descent variants on a real dataset\n2. **Compare** convergence patterns and computational trade-offs\n3. **Analyze** the impact of batch size on optimization performance\n4. **Develop** practical intuition for choosing optimization algorithms\n\n## Assignment Structure\n- **Part 1:** Implementation Challenge (60%)\n- **Part 2:** Experimental Analysis (30%)\n- **Part 3:** Written Analysis (10%)\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Data Loading\n",
    "We'll use the Boston Housing dataset - a classic regression problem with real-world characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and explore the Boston Housing dataset\n",
    "boston = load_boston()\n",
    "X, y = boston.data, boston.target\n",
    "\n",
    "print(\"=== Boston Housing Dataset ===\")\n",
    "print(f\"Features: {X.shape[1]} (dimensions)\")\n",
    "print(f\"Samples: {X.shape[0]} (houses)\")\n",
    "print(f\"Target: Housing prices in $1000s\")\n",
    "print(f\"\\nFeature names: {boston.feature_names}\")\n",
    "print(f\"\\nPrice statistics:\")\n",
    "print(f\"  Min: ${y.min():.1f}k\")\n",
    "print(f\"  Max: ${y.max():.1f}k\")\n",
    "print(f\"  Mean: ${y.mean():.1f}k\")\n",
    "print(f\"  Std: ${y.std():.1f}k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing\n",
    "# 1. Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 2. Standardize features (crucial for gradient descent)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 3. Add bias column (intercept term)\n",
    "X_train_bias = np.column_stack([np.ones(X_train_scaled.shape[0]), X_train_scaled])\n",
    "X_test_bias = np.column_stack([np.ones(X_test_scaled.shape[0]), X_test_scaled])\n",
    "\n",
    "print(\"=== Data Preprocessing Complete ===\")\n",
    "print(f\"Training set: {X_train_bias.shape[0]} samples, {X_train_bias.shape[1]} features (including bias)\")\n",
    "print(f\"Test set: {X_test_bias.shape[0]} samples\")\n",
    "print(f\"Feature scaling: Mean ≈ 0, Std ≈ 1\")\n",
    "print(f\"Training data shape: {X_train_bias.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 1: Implementation Challenge (60%)\n",
    "Implement all three gradient descent variants with proper metrics tracking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Batch Gradient Descent Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_gradient_descent(X, y, learning_rate=0.01, epochs=1000, tolerance=1e-6):\n",
    "    \"\"\"\n",
    "    Batch Gradient Descent Implementation\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix with bias column (m × n+1)\n",
    "        y: Target values (m,)\n",
    "        learning_rate: Step size for parameter updates\n",
    "        epochs: Maximum number of iterations\n",
    "        tolerance: Convergence threshold\n",
    "    \n",
    "    Returns:\n",
    "        weights: Learned parameters\n",
    "        costs: Cost function values over epochs\n",
    "        training_time: Time taken for training\n",
    "    \"\"\"\n",
    "    m, n = X.shape\n",
    "    \n",
    "    # Initialize weights randomly\n",
    "    weights = np.random.normal(0, 0.01, n)\n",
    "    \n",
    "    # Track metrics\n",
    "    costs = []\n",
    "    prev_cost = float('inf')\n",
    "    \n",
    "    # Start timing\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Forward pass: compute predictions for ALL examples\n",
    "        predictions = X @ weights\n",
    "        \n",
    "        # Compute cost (Mean Squared Error)\n",
    "        cost = np.mean((predictions - y) ** 2)\n",
    "        costs.append(cost)\n",
    "        \n",
    "        # Compute gradients using ALL examples\n",
    "        gradients = (2/m) * X.T @ (predictions - y)\n",
    "        \n",
    "        # Update weights\n",
    "        weights -= learning_rate * gradients\n",
    "        \n",
    "        # Check for convergence\n",
    "        if abs(prev_cost - cost) < tolerance:\n",
    "            print(f\"BGD converged at epoch {epoch}\")\n",
    "            break\n",
    "        prev_cost = cost\n",
    "        \n",
    "        # Progress reporting\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"BGD Epoch {epoch}: Cost = {cost:.6f}\")\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    return weights, costs, training_time\n",
    "\n",
    "# Test BGD implementation\n",
    "print(\"=== Testing Batch Gradient Descent ===\")\n",
    "weights_bgd, costs_bgd, time_bgd = batch_gradient_descent(\n",
    "    X_train_bias, y_train, learning_rate=0.1, epochs=1000\n",
    ")\n",
    "\n",
    "print(f\"\\nBGD Results:\")\n",
    "print(f\"Final cost: {costs_bgd[-1]:.6f}\")\n",
    "print(f\"Training time: {time_bgd:.3f} seconds\")\n",
    "print(f\"Total epochs: {len(costs_bgd)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Stochastic Gradient Descent Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_gradient_descent(X, y, learning_rate=0.01, epochs=1000, tolerance=1e-6):\n",
    "    \"\"\"\n",
    "    Stochastic Gradient Descent Implementation\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix with bias column (m × n+1)\n",
    "        y: Target values (m,)\n",
    "        learning_rate: Step size for parameter updates\n",
    "        epochs: Maximum number of iterations\n",
    "        tolerance: Convergence threshold\n",
    "    \n",
    "    Returns:\n",
    "        weights: Learned parameters\n",
    "        costs: Cost function values over epochs\n",
    "        training_time: Time taken for training\n",
    "    \"\"\"\n",
    "    m, n = X.shape\n",
    "    \n",
    "    # Initialize weights randomly\n",
    "    weights = np.random.normal(0, 0.01, n)\n",
    "    \n",
    "    # Track metrics\n",
    "    costs = []\n",
    "    \n",
    "    # Start timing\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_cost = 0\n",
    "        \n",
    "        # Shuffle data each epoch for better convergence\n",
    "        indices = np.random.permutation(m)\n",
    "        \n",
    "        for i in indices:\n",
    "            # Forward pass: compute prediction for SINGLE example\n",
    "            x_i = X[i:i+1]  # Keep as 2D array\n",
    "            y_i = y[i]\n",
    "            \n",
    "            prediction = x_i @ weights\n",
    "            \n",
    "            # Compute cost for this example\n",
    "            cost = (prediction - y_i) ** 2\n",
    "            epoch_cost += cost\n",
    "            \n",
    "            # Compute gradients using SINGLE example\n",
    "            gradient = 2 * x_i.T @ (prediction - y_i)\n",
    "            \n",
    "            # Update weights after each example\n",
    "            weights -= learning_rate * gradient.flatten()\n",
    "        \n",
    "        # Average cost for the epoch\n",
    "        avg_cost = epoch_cost / m\n",
    "        costs.append(avg_cost)\n",
    "        \n",
    "        # Progress reporting\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"SGD Epoch {epoch}: Cost = {avg_cost:.6f}\")\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    return weights, costs, training_time\n",
    "\n",
    "# Test SGD implementation\n",
    "print(\"=== Testing Stochastic Gradient Descent ===\")\n",
    "weights_sgd, costs_sgd, time_sgd = stochastic_gradient_descent(\n",
    "    X_train_bias, y_train, learning_rate=0.01, epochs=500  # Lower LR and epochs for SGD\n",
    ")\n",
    "\n",
    "print(f\"\\nSGD Results:\")\n",
    "print(f\"Final cost: {costs_sgd[-1]:.6f}\")\n",
    "print(f\"Training time: {time_sgd:.3f} seconds\")\n",
    "print(f\"Total epochs: {len(costs_sgd)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Mini-batch Gradient Descent Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mini_batch_gradient_descent(X, y, batch_size=32, learning_rate=0.01, epochs=1000, tolerance=1e-6):\n",
    "    \"\"\"\n",
    "    Mini-batch Gradient Descent Implementation\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix with bias column (m × n+1)\n",
    "        y: Target values (m,)\n",
    "        batch_size: Number of examples per batch\n",
    "        learning_rate: Step size for parameter updates\n",
    "        epochs: Maximum number of iterations\n",
    "        tolerance: Convergence threshold\n",
    "    \n",
    "    Returns:\n",
    "        weights: Learned parameters\n",
    "        costs: Cost function values over epochs\n",
    "        training_time: Time taken for training\n",
    "    \"\"\"\n",
    "    m, n = X.shape\n",
    "    \n",
    "    # Initialize weights randomly\n",
    "    weights = np.random.normal(0, 0.01, n)\n",
    "    \n",
    "    # Track metrics\n",
    "    costs = []\n",
    "    \n",
    "    # Start timing\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_cost = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        # Shuffle data each epoch\n",
    "        indices = np.random.permutation(m)\n",
    "        \n",
    "        # Create mini-batches\n",
    "        for i in range(0, m, batch_size):\n",
    "            batch_indices = indices[i:i+batch_size]\n",
    "            X_batch = X[batch_indices]\n",
    "            y_batch = y[batch_indices]\n",
    "            \n",
    "            # Forward pass: compute predictions for BATCH\n",
    "            predictions = X_batch @ weights\n",
    "            \n",
    "            # Compute cost for this batch\n",
    "            batch_cost = np.mean((predictions - y_batch) ** 2)\n",
    "            epoch_cost += batch_cost * len(X_batch)  # Weight by batch size\n",
    "            \n",
    "            # Compute gradients using BATCH\n",
    "            gradients = (2/len(X_batch)) * X_batch.T @ (predictions - y_batch)\n",
    "            \n",
    "            # Update weights after each batch\n",
    "            weights -= learning_rate * gradients\n",
    "            \n",
    "            num_batches += 1\n",
    "        \n",
    "        # Average cost for the epoch\n",
    "        avg_cost = epoch_cost / m\n",
    "        costs.append(avg_cost)\n",
    "        \n",
    "        # Progress reporting\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Mini-batch GD (bs={batch_size}) Epoch {epoch}: Cost = {avg_cost:.6f}\")\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    return weights, costs, training_time\n",
    "\n",
    "# Test Mini-batch GD with different batch sizes\n",
    "batch_sizes = [16, 32, 64, 128]\n",
    "mb_results = {}\n",
    "\n",
    "print(\"=== Testing Mini-batch Gradient Descent ===\")\n",
    "for bs in batch_sizes:\n",
    "    print(f\"\\nTesting batch size {bs}:\")\n",
    "    weights, costs, training_time = mini_batch_gradient_descent(\n",
    "        X_train_bias, y_train, batch_size=bs, learning_rate=0.05, epochs=500\n",
    "    )\n",
    "    \n",
    "    mb_results[bs] = {\n",
    "        'weights': weights,\n",
    "        'costs': costs,\n",
    "        'time': training_time,\n",
    "        'final_cost': costs[-1]\n",
    "    }\n",
    "    \n",
    "    print(f\"  Final cost: {costs[-1]:.6f}\")\n",
    "    print(f\"  Training time: {training_time:.3f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 2: Experimental Analysis (30%)\n",
    "Compare performance metrics and create visualizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Convergence Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive convergence comparison\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Plot 1: All algorithms together\n",
    "plt.subplot(2, 3, 1)\n",
    "plt.plot(costs_bgd[:200], 'b-', linewidth=2, label='Batch GD', alpha=0.8)\n",
    "plt.plot(costs_sgd[:200], 'r-', linewidth=1, label='Stochastic GD', alpha=0.7)\n",
    "for bs in [32, 64]:\n",
    "    plt.plot(mb_results[bs]['costs'][:200], '--', linewidth=1.5, label=f'Mini-batch (bs={bs})')\n",
    "plt.title('Convergence Comparison (First 200 Epochs)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Cost (MSE)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.yscale('log')\n",
    "\n",
    "# Plot 2: Batch GD detail\n",
    "plt.subplot(2, 3, 2)\n",
    "plt.plot(costs_bgd, 'b-', linewidth=2)\n",
    "plt.title('Batch Gradient Descent')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Cost (MSE)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.yscale('log')\n",
    "\n",
    "# Plot 3: SGD detail\n",
    "plt.subplot(2, 3, 3)\n",
    "plt.plot(costs_sgd, 'r-', linewidth=1, alpha=0.7)\n",
    "plt.title('Stochastic Gradient Descent')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Cost (MSE)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.yscale('log')\n",
    "\n",
    "# Plot 4: Mini-batch comparison\n",
    "plt.subplot(2, 3, 4)\n",
    "colors = ['green', 'orange', 'purple', 'brown']\n",
    "for i, bs in enumerate(batch_sizes):\n",
    "    plt.plot(mb_results[bs]['costs'], color=colors[i], linewidth=1.5, \n",
    "             label=f'Batch size {bs}', alpha=0.8)\n",
    "plt.title('Mini-batch GD: Batch Size Comparison')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Cost (MSE)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.yscale('log')\n",
    "\n",
    "# Plot 5: Training time comparison\n",
    "plt.subplot(2, 3, 5)\n",
    "methods = ['BGD', 'SGD'] + [f'MB-{bs}' for bs in batch_sizes]\n",
    "times = [time_bgd, time_sgd] + [mb_results[bs]['time'] for bs in batch_sizes]\n",
    "colors_bar = ['blue', 'red'] + ['green', 'orange', 'purple', 'brown']\n",
    "\n",
    "bars = plt.bar(methods, times, color=colors_bar, alpha=0.7)\n",
    "plt.title('Training Time Comparison')\n",
    "plt.ylabel('Time (seconds)')\n",
    "plt.xticks(rotation=45)\n",
    "# Add value labels on bars\n",
    "for bar, time_val in zip(bars, times):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "             f'{time_val:.2f}s', ha='center', va='bottom')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 6: Final cost comparison\n",
    "plt.subplot(2, 3, 6)\n",
    "final_costs = [costs_bgd[-1], costs_sgd[-1]] + [mb_results[bs]['final_cost'] for bs in batch_sizes]\n",
    "bars = plt.bar(methods, final_costs, color=colors_bar, alpha=0.7)\n",
    "plt.title('Final Cost Comparison')\n",
    "plt.ylabel('Final MSE')\n",
    "plt.xticks(rotation=45)\n",
    "# Add value labels on bars\n",
    "for bar, cost_val in zip(bars, final_costs):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, \n",
    "             f'{cost_val:.1f}', ha='center', va='bottom')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"=== Convergence Analysis Summary ===\")\n",
    "print(f\"Batch GD: {len(costs_bgd)} epochs, final cost: {costs_bgd[-1]:.4f}, time: {time_bgd:.2f}s\")\n",
    "print(f\"SGD: {len(costs_sgd)} epochs, final cost: {costs_sgd[-1]:.4f}, time: {time_sgd:.2f}s\")\n",
    "for bs in batch_sizes:\n",
    "    result = mb_results[bs]\n",
    "    print(f\"Mini-batch (bs={bs}): {len(result['costs'])} epochs, final cost: {result['final_cost']:.4f}, time: {result['time']:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Performance Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(weights, X_test, y_test):\n",
    "    \"\"\"Evaluate model performance on test set\"\"\"\n",
    "    predictions = X_test @ weights\n",
    "    mse = mean_squared_error(y_test, predictions)\n",
    "    r2 = r2_score(y_test, predictions)\n",
    "    rmse = np.sqrt(mse)\n",
    "    return {'MSE': mse, 'RMSE': rmse, 'R2': r2, 'predictions': predictions}\n",
    "\n",
    "# Evaluate all models\n",
    "print(\"=== Test Set Performance Evaluation ===\")\n",
    "print(\"\\nModel Performance on Test Set:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# BGD evaluation\n",
    "bgd_eval = evaluate_model(weights_bgd, X_test_bias, y_test)\n",
    "print(f\"Batch GD:     MSE = {bgd_eval['MSE']:.3f}, RMSE = {bgd_eval['RMSE']:.3f}, R² = {bgd_eval['R2']:.4f}\")\n",
    "\n",
    "# SGD evaluation\n",
    "sgd_eval = evaluate_model(weights_sgd, X_test_bias, y_test)\n",
    "print(f\"Stochastic GD: MSE = {sgd_eval['MSE']:.3f}, RMSE = {sgd_eval['RMSE']:.3f}, R² = {sgd_eval['R2']:.4f}\")\n",
    "\n",
    "# Mini-batch evaluations\n",
    "mb_evals = {}\n",
    "for bs in batch_sizes:\n",
    "    mb_eval = evaluate_model(mb_results[bs]['weights'], X_test_bias, y_test)\n",
    "    mb_evals[bs] = mb_eval\n",
    "    print(f\"Mini-batch {bs:2d}: MSE = {mb_eval['MSE']:.3f}, RMSE = {mb_eval['RMSE']:.3f}, R² = {mb_eval['R2']:.4f}\")\n",
    "\n",
    "print(f\"\\nBaseline (predicting mean): MSE = {np.var(y_test):.3f}, R² = 0.0000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Prediction Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions vs actual values\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# BGD predictions\n",
    "plt.subplot(2, 3, 1)\n",
    "plt.scatter(y_test, bgd_eval['predictions'], alpha=0.6, color='blue')\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "plt.xlabel('Actual Price ($1000s)')\n",
    "plt.ylabel('Predicted Price ($1000s)')\n",
    "plt.title(f'Batch GD\\nR² = {bgd_eval[\"R2\"]:.4f}')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# SGD predictions\n",
    "plt.subplot(2, 3, 2)\n",
    "plt.scatter(y_test, sgd_eval['predictions'], alpha=0.6, color='red')\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "plt.xlabel('Actual Price ($1000s)')\n",
    "plt.ylabel('Predicted Price ($1000s)')\n",
    "plt.title(f'Stochastic GD\\nR² = {sgd_eval[\"R2\"]:.4f}')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Mini-batch predictions (show best performing)\n",
    "best_mb_bs = max(batch_sizes, key=lambda bs: mb_evals[bs]['R2'])\n",
    "plt.subplot(2, 3, 3)\n",
    "plt.scatter(y_test, mb_evals[best_mb_bs]['predictions'], alpha=0.6, color='green')\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "plt.xlabel('Actual Price ($1000s)')\n",
    "plt.ylabel('Predicted Price ($1000s)')\n",
    "plt.title(f'Mini-batch GD (bs={best_mb_bs})\\nR² = {mb_evals[best_mb_bs][\"R2\"]:.4f}')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Residual plots\n",
    "plt.subplot(2, 3, 4)\n",
    "residuals_bgd = y_test - bgd_eval['predictions']\n",
    "plt.scatter(bgd_eval['predictions'], residuals_bgd, alpha=0.6, color='blue')\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.xlabel('Predicted Price ($1000s)')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('BGD Residuals')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(2, 3, 5)\n",
    "residuals_sgd = y_test - sgd_eval['predictions']\n",
    "plt.scatter(sgd_eval['predictions'], residuals_sgd, alpha=0.6, color='red')\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.xlabel('Predicted Price ($1000s)')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('SGD Residuals')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(2, 3, 6)\n",
    "residuals_mb = y_test - mb_evals[best_mb_bs]['predictions']\n",
    "plt.scatter(mb_evals[best_mb_bs]['predictions'], residuals_mb, alpha=0.6, color='green')\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.xlabel('Predicted Price ($1000s)')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title(f'Mini-batch (bs={best_mb_bs}) Residuals')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Learning Rate Sensitivity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different learning rates for mini-batch GD\n",
    "learning_rates = [0.001, 0.01, 0.05, 0.1, 0.2]\n",
    "lr_results = {}\n",
    "\n",
    "print(\"=== Learning Rate Sensitivity Analysis ===\")\n",
    "print(\"Testing different learning rates with mini-batch GD (batch_size=32)\\n\")\n",
    "\n",
    "for lr in learning_rates:\n",
    "    print(f\"Testing learning rate {lr}...\")\n",
    "    try:\n",
    "        weights, costs, training_time = mini_batch_gradient_descent(\n",
    "            X_train_bias, y_train, batch_size=32, learning_rate=lr, epochs=200\n",
    "        )\n",
    "        \n",
    "        # Check if training was stable (no NaN or extremely large values)\n",
    "        if np.any(np.isnan(weights)) or np.any(np.abs(weights) > 1000):\n",
    "            print(f\"  Learning rate {lr}: UNSTABLE (diverged)\")\n",
    "            lr_results[lr] = {'stable': False, 'final_cost': float('inf')}\n",
    "        else:\n",
    "            lr_results[lr] = {\n",
    "                'stable': True,\n",
    "                'weights': weights,\n",
    "                'costs': costs,\n",
    "                'time': training_time,\n",
    "                'final_cost': costs[-1]\n",
    "            }\n",
    "            print(f\"  Learning rate {lr}: Final cost = {costs[-1]:.4f}, Time = {training_time:.2f}s\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Learning rate {lr}: ERROR - {str(e)}\")\n",
    "        lr_results[lr] = {'stable': False, 'final_cost': float('inf')}\n",
    "\n",
    "# Visualize learning rate effects\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Plot 1: Convergence curves for different learning rates\n",
    "plt.subplot(1, 2, 1)\n",
    "colors = ['blue', 'green', 'orange', 'red', 'purple']\n",
    "for i, lr in enumerate(learning_rates):\n",
    "    if lr_results[lr]['stable']:\n",
    "        plt.plot(lr_results[lr]['costs'], color=colors[i], linewidth=1.5, \n",
    "                label=f'LR = {lr}', alpha=0.8)\n",
    "    else:\n",
    "        plt.axhline(y=1000, color=colors[i], linestyle='--', \n",
    "                   label=f'LR = {lr} (diverged)', alpha=0.5)\n",
    "\n",
    "plt.title('Learning Rate Sensitivity')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Cost (MSE)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.yscale('log')\n",
    "\n",
    "# Plot 2: Final cost vs learning rate\n",
    "plt.subplot(1, 2, 2)\n",
    "stable_lrs = [lr for lr in learning_rates if lr_results[lr]['stable']]\n",
    "stable_costs = [lr_results[lr]['final_cost'] for lr in stable_lrs]\n",
    "\n",
    "plt.plot(stable_lrs, stable_costs, 'bo-', linewidth=2, markersize=8)\n",
    "plt.title('Final Cost vs Learning Rate')\n",
    "plt.xlabel('Learning Rate')\n",
    "plt.ylabel('Final Cost (MSE)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xscale('log')\n",
    "\n",
    "# Highlight optimal learning rate\n",
    "if stable_costs:\n",
    "    best_lr_idx = np.argmin(stable_costs)\n",
    "    best_lr = stable_lrs[best_lr_idx]\n",
    "    best_cost = stable_costs[best_lr_idx]\n",
    "    plt.plot(best_lr, best_cost, 'ro', markersize=12, alpha=0.7, label=f'Optimal: {best_lr}')\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nOptimal learning rate: {best_lr} (Final cost: {best_cost:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 3: Written Analysis (10%)\n",
    "Answer the critical questions based on your experimental results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis Questions\n",
    "\n",
    "**Answer these questions based on your experimental results:**\n",
    "\n",
    "### Question 1: Which variant converged fastest and why?\n",
    "\n",
    "**Your Answer:** \n",
    "*(Replace this with your analysis based on the results above)*\n",
    "\n",
    "Based on the experimental results:\n",
    "- **Fastest convergence**: [Analyze which method reached low cost values quickest]\n",
    "- **Reasons**: [Explain in terms of update frequency, gradient noise, etc.]\n",
    "- **Trade-offs**: [Discuss what was sacrificed for speed]\n",
    "\n",
    "### Question 2: How did batch size affect convergence stability?\n",
    "\n",
    "**Your Answer:**\n",
    "*(Analyze the mini-batch results with different batch sizes)*\n",
    "\n",
    "- **Small batch sizes (16-32)**: [Describe convergence pattern]\n",
    "- **Large batch sizes (64-128)**: [Describe convergence pattern]\n",
    "- **Stability vs Speed trade-off**: [Explain the relationship]\n",
    "\n",
    "### Question 3: What learning rates worked best for each variant?\n",
    "\n",
    "**Your Answer:**\n",
    "*(Based on your learning rate sensitivity analysis)*\n",
    "\n",
    "- **SGD optimal LR**: [From your observations]\n",
    "- **Mini-batch optimal LR**: [From sensitivity analysis]\n",
    "- **BGD optimal LR**: [From your observations]\n",
    "- **Why different?**: [Explain the relationship between batch size and learning rate]\n",
    "\n",
    "### Question 4: When would you choose each variant in practice?\n",
    "\n",
    "**Your Answer:**\n",
    "*(Practical recommendations based on different scenarios)*\n",
    "\n",
    "**Choose Batch GD when:**\n",
    "- [List scenarios: dataset size, computational resources, etc.]\n",
    "\n",
    "**Choose SGD when:**\n",
    "- [List scenarios: memory constraints, online learning, etc.]\n",
    "\n",
    "**Choose Mini-batch GD when:**\n",
    "- [List scenarios: most common use cases]\n",
    "\n",
    "### Additional Insights\n",
    "\n",
    "**What surprised you in the results?**\n",
    "*(Discuss any unexpected findings)*\n",
    "\n",
    "**Real-world implications:**\n",
    "*(How would this knowledge help in practical deep learning projects?)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Summary and Conclusions\n",
    "\n",
    "## Key Findings Summary\n",
    "\n",
    "Create a final summary table with your results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive results summary\n",
    "import pandas as pd\n",
    "\n",
    "# Compile all results\n",
    "summary_data = {\n",
    "    'Algorithm': ['Batch GD', 'Stochastic GD'] + [f'Mini-batch (bs={bs})' for bs in batch_sizes],\n",
    "    'Final Training Cost': [costs_bgd[-1], costs_sgd[-1]] + [mb_results[bs]['final_cost'] for bs in batch_sizes],\n",
    "    'Test MSE': [bgd_eval['MSE'], sgd_eval['MSE']] + [mb_evals[bs]['MSE'] for bs in batch_sizes],\n",
    "    'Test R²': [bgd_eval['R2'], sgd_eval['R2']] + [mb_evals[bs]['R2'] for bs in batch_sizes],\n",
    "    'Training Time (s)': [time_bgd, time_sgd] + [mb_results[bs]['time'] for bs in batch_sizes],\n",
    "    'Updates per Epoch': [1, len(X_train_bias)] + [len(X_train_bias)//bs for bs in batch_sizes]\n",
    "}\n",
    "\n",
    "results_df = pd.DataFrame(summary_data)\n",
    "results_df = results_df.round(4)\n",
    "\n",
    "print(\"=== COMPREHENSIVE RESULTS SUMMARY ===\")\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Find best performing algorithm\n",
    "best_r2_idx = results_df['Test R²'].idxmax()\n",
    "best_algorithm = results_df.loc[best_r2_idx, 'Algorithm']\n",
    "best_r2 = results_df.loc[best_r2_idx, 'Test R²']\n",
    "\n",
    "print(f\"\\n🏆 Best performing algorithm: {best_algorithm} (R² = {best_r2:.4f})\")\n",
    "\n",
    "# Find fastest algorithm\n",
    "fastest_idx = results_df['Training Time (s)'].idxmin()\n",
    "fastest_algorithm = results_df.loc[fastest_idx, 'Algorithm']\n",
    "fastest_time = results_df.loc[fastest_idx, 'Training Time (s)']\n",
    "\n",
    "print(f\"⚡ Fastest algorithm: {fastest_algorithm} ({fastest_time:.2f} seconds)\")\n",
    "\n",
    "print(\"\\n=== ASSIGNMENT COMPLETION ===\")\n",
    "print(\"✅ Part 1: Implementation Challenge - Complete\")\n",
    "print(\"✅ Part 2: Experimental Analysis - Complete\") \n",
    "print(\"📝 Part 3: Written Analysis - Complete your answers above\")\n",
    "print(\"\\n📊 All visualizations and metrics generated successfully!\")\n",
    "print(\"📋 Ready for submission - ensure all markdown analysis sections are filled out.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Submission Checklist\n",
    "\n",
    "Before submitting, ensure you have:\n",
    "\n",
    "- [ ] **Implemented all three gradient descent variants correctly**\n",
    "- [ ] **Tested multiple batch sizes for mini-batch GD**\n",
    "- [ ] **Generated all required visualizations**\n",
    "- [ ] **Completed the learning rate sensitivity analysis**\n",
    "- [ ] **Answered all analysis questions in Part 3**\n",
    "- [ ] **Included performance comparison table**\n",
    "- [ ] **Added your personal insights and conclusions**\n",
    "- [ ] **Code is well-commented and runs without errors**\n",
    "- [ ] **All plots have proper titles, labels, and legends**\n",
    "\n",
    "## Grading Rubric (Total: 100 points)\n",
    "\n",
    "### Part 1: Implementation (60 points)\n",
    "- Batch GD implementation (15 points)\n",
    "- Stochastic GD implementation (15 points) \n",
    "- Mini-batch GD implementation (20 points)\n",
    "- Code quality and documentation (10 points)\n",
    "\n",
    "### Part 2: Experimental Analysis (30 points)\n",
    "- Convergence visualizations (10 points)\n",
    "- Performance metrics (10 points)\n",
    "- Learning rate analysis (10 points)\n",
    "\n",
    "### Part 3: Written Analysis (10 points)\n",
    "- Quality of analysis (5 points)\n",
    "- Practical insights (5 points)\n",
    "\n",
    "**Good luck with your assignment! 🚀**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}