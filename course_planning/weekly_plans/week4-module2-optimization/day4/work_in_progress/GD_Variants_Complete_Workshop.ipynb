{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Week 4 - Day 4: Gradient Descent Variants Workshop\n",
    "\n",
    "**Course:** Deep Neural Network Architectures (21CSE558T)  \n",
    "**Module:** 2 - Optimization and Regularization  \n",
    "**Topic:** Comparing Batch, Stochastic, and Mini-batch Gradient Descent\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this workshop, you will be able to:\n",
    "\n",
    "1. **Implement** all three gradient descent variants from scratch\n",
    "2. **Visualize** and compare their convergence patterns\n",
    "3. **Analyze** computational trade-offs between methods\n",
    "4. **Select** appropriate GD variant for different scenarios\n",
    "5. **Tune** hyperparameters effectively\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Background\n",
    "\n",
    "**Gradient Descent Equation (from Day 3):**\n",
    "```\n",
    "Œ∏ ‚Üê Œ∏ - Œ±‚àáJ(Œ∏)\n",
    "```\n",
    "\n",
    "**Today's Question:** *How many examples should we use to compute ‚àáJ(Œ∏)?*\n",
    "\n",
    "- **All examples** ‚Üí Batch Gradient Descent\n",
    "- **One example** ‚Üí Stochastic Gradient Descent  \n",
    "- **Small batches** ‚Üí Mini-batch Gradient Descent"
   ],
   "metadata": {
    "id": "header_cell"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## üîß Setup and Imports"
   ],
   "metadata": {
    "id": "setup_header"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imports"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from typing import Tuple, List\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure matplotlib for better plots\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(\"üéØ Ready to explore Gradient Descent Variants!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## üìä Dataset Creation\n",
    "\n",
    "Let's create a simple linear regression problem: **y = 2x + 1 + noise**"
   ],
   "metadata": {
    "id": "dataset_header"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def create_synthetic_dataset(m: int = 1000, noise_std: float = 0.1) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Create synthetic linear regression dataset\n",
    "    \n",
    "    Args:\n",
    "        m: Number of examples\n",
    "        noise_std: Standard deviation of noise\n",
    "        \n",
    "    Returns:\n",
    "        X: Input features (m, 1)\n",
    "        y: Target values (m,) following y = 2x + 1 + noise\n",
    "    \"\"\"\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    X = np.random.randn(m, 1)\n",
    "    y = 2 * X.squeeze() + 1 + noise_std * np.random.randn(m)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# Create our dataset\n",
    "X, y = create_synthetic_dataset(m=1000, noise_std=0.1)\n",
    "\n",
    "print(f\"üìä Dataset Created:\")\n",
    "print(f\"   Relationship: y = 2x + 1 + noise\")\n",
    "print(f\"   Examples: {len(X)}\")\n",
    "print(f\"   X range: [{X.min():.2f}, {X.max():.2f}]\")\n",
    "print(f\"   y range: [{y.min():.2f}, {y.max():.2f}]\")\n",
    "print(f\"   True parameters: w = 2.0, b = 1.0\")"
   ],
   "metadata": {
    "id": "dataset_creation"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Visualize our dataset\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X, y, alpha=0.5, s=20, color='blue')\n",
    "plt.plot(X, 2*X + 1, 'r-', linewidth=2, label='True relationship: y = 2x + 1')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.title('Synthetic Dataset: Linear Regression with Noise')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"üéØ Our goal: Learn the parameters w=2.0 and b=1.0 from this data!\")"
   ],
   "metadata": {
    "id": "dataset_visualization"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## üîç Helper Functions\n",
    "\n",
    "Let's create some utility functions we'll use across all implementations."
   ],
   "metadata": {
    "id": "helpers_header"
   }
  },
  {
   "cell_type": "code",
   "source": "def compute_cost(predictions: np.ndarray, y_true: np.ndarray) -> float:\n    \"\"\"Compute Mean Squared Error cost\"\"\"\n    return float(np.mean((predictions - y_true)**2))\n\ndef initialize_parameters() -> Tuple[float, float]:\n    \"\"\"Initialize weight and bias with small random values\"\"\"\n    w = float(np.random.randn() * 0.01)\n    b = float(np.random.randn() * 0.01)\n    return w, b\n\ndef print_progress(epoch: int, cost: float, w: float, b: float, method: str):\n    \"\"\"Print training progress\"\"\"\n    # Convert to scalar if needed to avoid formatting errors\n    cost = float(cost) if hasattr(cost, 'item') else float(cost)\n    w = float(w) if hasattr(w, 'item') else float(w)  \n    b = float(b) if hasattr(b, 'item') else float(b)\n    print(f\"{method} Epoch {epoch:3d}: Cost = {cost:.8f}, w = {w:.6f}, b = {b:.6f}\")\n\nprint(\"‚úÖ Helper functions defined!\")",
   "metadata": {
    "id": "helper_functions"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# üèóÔ∏è Implementation 1: Batch Gradient Descent\n",
    "\n",
    "**The Perfectionist Approach**\n",
    "\n",
    "- Uses **ALL** training examples to compute each gradient\n",
    "- **Pros:** Stable, smooth convergence\n",
    "- **Cons:** Slow for large datasets, memory intensive\n",
    "\n",
    "**Mathematical Update:**\n",
    "```\n",
    "Œ∏^(t+1) = Œ∏^(t) - Œ± ¬∑ (1/m) Œ£(i=1 to m) ‚àáJ(f_Œ∏(x_i), y_i)\n",
    "```"
   ],
   "metadata": {
    "id": "batch_gd_header"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def batch_gradient_descent(X, y, epochs=100, learning_rate=0.1, verbose=True):\n",
    "    \"\"\"\n",
    "    Batch Gradient Descent Implementation\n",
    "    \n",
    "    Uses the entire dataset to compute gradient at each step.\n",
    "    Provides smooth, stable convergence but is slow for large datasets.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Initialize parameters\n",
    "    w, b = initialize_parameters()\n",
    "    costs = []\n",
    "    m = len(X)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"üîµ BATCH GRADIENT DESCENT\")\n",
    "        print(\"=\"*50)\n",
    "        print(f\"üìä Dataset size: {m} examples\")\n",
    "        print(f\"‚ö° Updates per epoch: 1\")\n",
    "        print(f\"üéØ Initial: w = {w:.6f}, b = {b:.6f}\")\n",
    "        print(f\"üìà Learning rate: {learning_rate}\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Forward pass - compute predictions for ALL examples\n",
    "        predictions = w * X.squeeze() + b\n",
    "        \n",
    "        # Compute cost using entire dataset\n",
    "        cost = compute_cost(predictions, y)\n",
    "        costs.append(cost)\n",
    "        \n",
    "        # Compute gradients using ALL examples\n",
    "        # dJ/dw = (1/m) * Œ£(predictions - y) * X\n",
    "        # dJ/db = (1/m) * Œ£(predictions - y)\n",
    "        dw = np.mean((predictions - y) * X.squeeze())\n",
    "        db = np.mean(predictions - y)\n",
    "        \n",
    "        # Parameter update - SINGLE update per epoch\n",
    "        w -= learning_rate * dw\n",
    "        b -= learning_rate * db\n",
    "        \n",
    "        # Progress logging\n",
    "        if verbose and (epoch % 20 == 0 or epoch < 10):\n",
    "            print_progress(epoch, cost, w, b, \"BGD\")\n",
    "    \n",
    "    time_taken = time.time() - start_time\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nüéØ Final Results:\")\n",
    "        print(f\"   w = {w:.6f}, b = {b:.6f}\")\n",
    "        print(f\"   Final cost = {costs[-1]:.8f}\")\n",
    "        print(f\"   Time taken = {time_taken:.3f} seconds\")\n",
    "        print(f\"   Total parameter updates: {epochs}\")\n",
    "    \n",
    "    return w, b, costs, time_taken"
   ],
   "metadata": {
    "id": "batch_gd_implementation"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Run Batch Gradient Descent\n",
    "print(\"üöÄ Running Batch Gradient Descent...\")\n",
    "w_batch, b_batch, costs_batch, time_batch = batch_gradient_descent(X, y, epochs=100, learning_rate=0.1)"
   ],
   "metadata": {
    "id": "batch_gd_execution"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Visualize Batch GD convergence\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(costs_batch, 'b-', linewidth=2, label='Batch GD')\n",
    "plt.title('Batch GD: Convergence Pattern')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Cost (MSE)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(X, y, alpha=0.3, s=20, color='lightblue', label='Data')\n",
    "plt.plot(X, 2*X + 1, 'g--', linewidth=2, label='True: y = 2x + 1')\n",
    "plt.plot(X, w_batch*X + b_batch, 'b-', linewidth=2, \n",
    "         label=f'Learned: y = {w_batch:.2f}x + {b_batch:.2f}')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.title('Batch GD: Final Fit')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"üìä Batch GD Results:\")\n",
    "print(f\"   True parameters:    w = 2.000, b = 1.000\")\n",
    "print(f\"   Learned parameters: w = {w_batch:.3f}, b = {b_batch:.3f}\")\n",
    "print(f\"   Error: w_error = {abs(w_batch-2):.6f}, b_error = {abs(b_batch-1):.6f}\")"
   ],
   "metadata": {
    "id": "batch_gd_visualization"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ‚ö° Implementation 2: Stochastic Gradient Descent\n",
    "\n",
    "**The Speed Demon Approach**\n",
    "\n",
    "- Updates parameters after **each individual** training example\n",
    "- **Pros:** Fast updates, memory efficient, can escape local minima\n",
    "- **Cons:** Noisy convergence, unstable, requires careful tuning\n",
    "\n",
    "**Mathematical Update:**\n",
    "```\n",
    "Œ∏^(t+1) = Œ∏^(t) - Œ± ¬∑ ‚àáJ(f_Œ∏(x_i), y_i)  [for randomly selected i]\n",
    "```"
   ],
   "metadata": {
    "id": "sgd_header"
   }
  },
  {
   "cell_type": "code",
   "source": "def stochastic_gradient_descent(X, y, epochs=100, learning_rate=0.01, verbose=True):\n    \"\"\"\n    Stochastic Gradient Descent Implementation\n    \n    Updates parameters after each individual training example.\n    Fast updates but noisy convergence pattern.\n    \"\"\"\n    start_time = time.time()\n    \n    # Initialize parameters\n    w, b = initialize_parameters()\n    costs = []\n    m = len(X)\n    \n    if verbose:\n        print(\"\\n\" + \"=\"*50)\n        print(\"üî¥ STOCHASTIC GRADIENT DESCENT\")\n        print(\"=\"*50)\n        print(f\"üìä Dataset size: {m} examples\")\n        print(f\"‚ö° Updates per epoch: {m}\")\n        print(f\"üéØ Initial: w = {w:.6f}, b = {b:.6f}\")\n        print(f\"üìà Learning rate: {learning_rate} (smaller than BGD)\")\n    \n    for epoch in range(epochs):\n        epoch_cost = 0\n        \n        # Shuffle data each epoch for better convergence\n        indices = np.random.permutation(m)\n        \n        # Process each example individually\n        for idx in indices:\n            # Forward pass - SINGLE example\n            x_i = float(X[idx].squeeze())  # Ensure scalar\n            y_i = float(y[idx])            # Ensure scalar\n            prediction = w * x_i + b\n            \n            # Accumulate cost for epoch average\n            cost_i = (prediction - y_i)**2\n            epoch_cost += cost_i\n            \n            # Compute gradients for SINGLE example\n            # dJ/dw = (prediction - y_i) * x_i\n            # dJ/db = (prediction - y_i)\n            dw = (prediction - y_i) * x_i\n            db = (prediction - y_i)\n            \n            # Immediate parameter update\n            w -= learning_rate * dw\n            b -= learning_rate * db\n        \n        # Average cost for the epoch\n        avg_cost = epoch_cost / m\n        costs.append(float(avg_cost))\n        \n        # Progress logging\n        if verbose and (epoch % 20 == 0 or epoch < 10):\n            print_progress(epoch, avg_cost, w, b, \"SGD\")\n    \n    time_taken = time.time() - start_time\n    \n    if verbose:\n        print(f\"\\nüéØ Final Results:\")\n        print(f\"   w = {w:.6f}, b = {b:.6f}\")\n        print(f\"   Final cost = {costs[-1]:.8f}\")\n        print(f\"   Time taken = {time_taken:.3f} seconds\")\n        print(f\"   Total parameter updates: {epochs * m}\")\n    \n    return w, b, costs, time_taken",
   "metadata": {
    "id": "sgd_implementation"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Run Stochastic Gradient Descent\n",
    "print(\"üöÄ Running Stochastic Gradient Descent...\")\n",
    "w_sgd, b_sgd, costs_sgd, time_sgd = stochastic_gradient_descent(X, y, epochs=100, learning_rate=0.01)"
   ],
   "metadata": {
    "id": "sgd_execution"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Visualize SGD convergence\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(costs_sgd, 'r-', alpha=0.7, linewidth=2, label='Stochastic GD')\n",
    "plt.title('Stochastic GD: Noisy Convergence')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Cost (MSE)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(X, y, alpha=0.3, s=20, color='lightcoral', label='Data')\n",
    "plt.plot(X, 2*X + 1, 'g--', linewidth=2, label='True: y = 2x + 1')\n",
    "plt.plot(X, w_sgd*X + b_sgd, 'r-', linewidth=2, \n",
    "         label=f'Learned: y = {w_sgd:.2f}x + {b_sgd:.2f}')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.title('Stochastic GD: Final Fit')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"üìä Stochastic GD Results:\")\n",
    "print(f\"   True parameters:    w = 2.000, b = 1.000\")\n",
    "print(f\"   Learned parameters: w = {w_sgd:.3f}, b = {b_sgd:.3f}\")\n",
    "print(f\"   Error: w_error = {abs(w_sgd-2):.6f}, b_error = {abs(b_sgd-1):.6f}\")"
   ],
   "metadata": {
    "id": "sgd_visualization"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ‚öñÔ∏è Implementation 3: Mini-batch Gradient Descent\n",
    "\n",
    "**The Goldilocks Solution**\n",
    "\n",
    "- Uses **small batches** of examples (typically 16-256)\n",
    "- **Pros:** Balanced convergence, GPU-friendly, practical\n",
    "- **Cons:** Need to choose batch size\n",
    "\n",
    "**Mathematical Update:**\n",
    "```\n",
    "Œ∏^(t+1) = Œ∏^(t) - Œ± ¬∑ (1/|B|) Œ£(i‚ààB) ‚àáJ(f_Œ∏(x_i), y_i)  [B = mini-batch]\n",
    "```"
   ],
   "metadata": {
    "id": "minibatch_gd_header"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def mini_batch_gradient_descent(X, y, batch_size=32, epochs=100, learning_rate=0.05, verbose=True):\n",
    "    \"\"\"\n",
    "    Mini-batch Gradient Descent Implementation\n",
    "    \n",
    "    Updates parameters using small batches of examples.\n",
    "    Balances the stability of BGD with the speed of SGD.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Initialize parameters\n",
    "    w, b = initialize_parameters()\n",
    "    costs = []\n",
    "    m = len(X)\n",
    "    \n",
    "    # Calculate number of batches per epoch\n",
    "    num_batches = int(np.ceil(m / batch_size))\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"üü¢ MINI-BATCH GRADIENT DESCENT\")\n",
    "        print(\"=\"*50)\n",
    "        print(f\"üìä Dataset size: {m} examples\")\n",
    "        print(f\"üì¶ Batch size: {batch_size}\")\n",
    "        print(f\"‚ö° Updates per epoch: {num_batches}\")\n",
    "        print(f\"üéØ Initial: w = {w:.6f}, b = {b:.6f}\")\n",
    "        print(f\"üìà Learning rate: {learning_rate}\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_cost = 0\n",
    "        \n",
    "        # Shuffle data each epoch\n",
    "        indices = np.random.permutation(m)\n",
    "        \n",
    "        # Process data in mini-batches\n",
    "        for i in range(0, m, batch_size):\n",
    "            # Create mini-batch\n",
    "            end_idx = min(i + batch_size, m)\n",
    "            batch_indices = indices[i:end_idx]\n",
    "            \n",
    "            X_batch = X[batch_indices]\n",
    "            y_batch = y[batch_indices]\n",
    "            batch_m = len(X_batch)\n",
    "            \n",
    "            # Forward pass - BATCH of examples\n",
    "            predictions = w * X_batch.squeeze() + b\n",
    "            \n",
    "            # Compute cost for this batch\n",
    "            batch_cost = compute_cost(predictions, y_batch)\n",
    "            epoch_cost += batch_cost * batch_m\n",
    "            \n",
    "            # Compute gradients for BATCH\n",
    "            # dJ/dw = (1/batch_m) * Œ£(predictions - y_batch) * X_batch\n",
    "            # dJ/db = (1/batch_m) * Œ£(predictions - y_batch)\n",
    "            dw = np.mean((predictions - y_batch) * X_batch.squeeze())\n",
    "            db = np.mean(predictions - y_batch)\n",
    "            \n",
    "            # Parameter update after each batch\n",
    "            w -= learning_rate * dw\n",
    "            b -= learning_rate * db\n",
    "        \n",
    "        # Average cost for the epoch\n",
    "        avg_cost = epoch_cost / m\n",
    "        costs.append(avg_cost)\n",
    "        \n",
    "        # Progress logging\n",
    "        if verbose and (epoch % 20 == 0 or epoch < 10):\n",
    "            print_progress(epoch, avg_cost, w, b, \"MBG\")\n",
    "    \n",
    "    time_taken = time.time() - start_time\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nüéØ Final Results:\")\n",
    "        print(f\"   w = {w:.6f}, b = {b:.6f}\")\n",
    "        print(f\"   Final cost = {costs[-1]:.8f}\")\n",
    "        print(f\"   Time taken = {time_taken:.3f} seconds\")\n",
    "        print(f\"   Total parameter updates: {epochs * num_batches}\")\n",
    "    \n",
    "    return w, b, costs, time_taken"
   ],
   "metadata": {
    "id": "minibatch_gd_implementation"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Run Mini-batch Gradient Descent\n",
    "print(\"üöÄ Running Mini-batch Gradient Descent...\")\n",
    "w_mb, b_mb, costs_mb, time_mb = mini_batch_gradient_descent(X, y, batch_size=32, epochs=100, learning_rate=0.05)"
   ],
   "metadata": {
    "id": "minibatch_gd_execution"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Visualize Mini-batch GD convergence\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(costs_mb, 'g-', linewidth=2, label='Mini-batch GD')\n",
    "plt.title('Mini-batch GD: Balanced Convergence')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Cost (MSE)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(X, y, alpha=0.3, s=20, color='lightgreen', label='Data')\n",
    "plt.plot(X, 2*X + 1, 'g--', linewidth=2, label='True: y = 2x + 1')\n",
    "plt.plot(X, w_mb*X + b_mb, 'g-', linewidth=2, \n",
    "         label=f'Learned: y = {w_mb:.2f}x + {b_mb:.2f}')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.title('Mini-batch GD: Final Fit')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"üìä Mini-batch GD Results:\")\n",
    "print(f\"   True parameters:    w = 2.000, b = 1.000\")\n",
    "print(f\"   Learned parameters: w = {w_mb:.3f}, b = {b_mb:.3f}\")\n",
    "print(f\"   Error: w_error = {abs(w_mb-2):.6f}, b_error = {abs(b_mb-1):.6f}\")"
   ],
   "metadata": {
    "id": "minibatch_gd_visualization"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# üìä Comprehensive Comparison\n",
    "\n",
    "Now let's compare all three methods side by side!"
   ],
   "metadata": {
    "id": "comparison_header"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Create comprehensive comparison\n",
    "def compare_all_methods():\n",
    "    \"\"\"Compare all three gradient descent variants\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üèÜ COMPREHENSIVE COMPARISON RESULTS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Results table\n",
    "    methods = [\"Batch GD\", \"Stochastic GD\", \"Mini-batch GD\"]\n",
    "    weights = [w_batch, w_sgd, w_mb]\n",
    "    biases = [b_batch, b_sgd, b_mb]\n",
    "    final_costs = [costs_batch[-1], costs_sgd[-1], costs_mb[-1]]\n",
    "    times = [time_batch, time_sgd, time_mb]\n",
    "    \n",
    "    print(f\"{'Method':<15} {'Weight':<10} {'Bias':<10} {'Final Cost':<12} {'Time (s)':<10}\")\n",
    "    print(\"-\" * 65)\n",
    "    for i in range(3):\n",
    "        print(f\"{methods[i]:<15} {weights[i]:<10.4f} {biases[i]:<10.4f} \"\n",
    "              f\"{final_costs[i]:<12.8f} {times[i]:<10.3f}\")\n",
    "    \n",
    "    print(f\"\\nüéØ True parameters: w = 2.0000, b = 1.0000\")\n",
    "    \n",
    "    # Error analysis\n",
    "    print(f\"\\nüìä Error Analysis (|predicted - true|):\")\n",
    "    w_errors = [abs(w - 2.0) for w in weights]\n",
    "    b_errors = [abs(b - 1.0) for b in biases]\n",
    "    \n",
    "    for i in range(3):\n",
    "        print(f\"   {methods[i]:<15} Weight Error: {w_errors[i]:.6f}, \"\n",
    "              f\"Bias Error: {b_errors[i]:.6f}\")\n",
    "\n",
    "compare_all_methods()"
   ],
   "metadata": {
    "id": "comparison_function"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Create detailed comparison visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('üèÜ Gradient Descent Variants: Complete Comparison', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Individual convergence plots\n",
    "costs_data = [costs_batch, costs_sgd, costs_mb]\n",
    "colors = ['blue', 'red', 'green']\n",
    "methods = ['Batch GD: Smooth & Stable', 'Stochastic GD: Fast & Noisy', 'Mini-batch GD: Balanced']\n",
    "\n",
    "for i, (costs, method, color) in enumerate(zip(costs_data, methods, colors)):\n",
    "    row, col = i // 2, i % 2\n",
    "    if i < 3:\n",
    "        axes[row, col].plot(costs, color=color, linewidth=2, alpha=0.8)\n",
    "        axes[row, col].set_title(method)\n",
    "        axes[row, col].set_xlabel('Epoch')\n",
    "        axes[row, col].set_ylabel('Cost (MSE)')\n",
    "        axes[row, col].grid(True, alpha=0.3)\n",
    "        axes[row, col].set_yscale('log')\n",
    "\n",
    "# Combined comparison plot\n",
    "axes[1, 1].plot(costs_batch, 'b-', linewidth=2, label='Batch GD', alpha=0.8)\n",
    "axes[1, 1].plot(costs_sgd, 'r-', linewidth=2, label='Stochastic GD', alpha=0.7)\n",
    "axes[1, 1].plot(costs_mb, 'g-', linewidth=2, label='Mini-batch GD', alpha=0.8)\n",
    "axes[1, 1].set_title('All Methods Combined')\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_ylabel('Cost (MSE)')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "axes[1, 1].set_yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìà Key Observations:\")\n",
    "print(\"   üîµ Batch GD: Smoothest convergence, most stable\")\n",
    "print(\"   üî¥ Stochastic GD: Noisiest but explores more\")\n",
    "print(\"   üü¢ Mini-batch GD: Best balance for practical use\")"
   ],
   "metadata": {
    "id": "detailed_comparison_viz"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# üß™ Experimentation Section\n",
    "\n",
    "Let's experiment with different settings to understand the trade-offs better!"
   ],
   "metadata": {
    "id": "experimentation_header"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## üî¨ Experiment 1: Batch Size Impact"
   ],
   "metadata": {
    "id": "experiment1_header"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def experiment_batch_sizes(batch_sizes=[1, 8, 16, 32, 64, 128], epochs=50):\n",
    "    \"\"\"Experiment with different batch sizes for mini-batch GD\"\"\"\n",
    "    \n",
    "    print(f\"\\nüî¨ EXPERIMENT 1: Batch Size Impact\")\n",
    "    print(f\"üìä Testing batch sizes: {batch_sizes}\")\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for batch_size in batch_sizes:\n",
    "        print(f\"\\nüß™ Testing batch_size = {batch_size}\")\n",
    "        \n",
    "        if batch_size >= len(X):\n",
    "            # If batch size >= dataset size, this becomes batch GD\n",
    "            w, b, costs, time_taken = batch_gradient_descent(\n",
    "                X, y, epochs=epochs, verbose=False\n",
    "            )\n",
    "            method_name = \"Batch GD\"\n",
    "        else:\n",
    "            w, b, costs, time_taken = mini_batch_gradient_descent(\n",
    "                X, y, batch_size=batch_size, epochs=epochs, verbose=False\n",
    "            )\n",
    "            method_name = f\"Mini-batch (size={batch_size})\"\n",
    "        \n",
    "        results[batch_size] = {\n",
    "            'w': w, 'b': b, 'final_cost': costs[-1], \n",
    "            'time': time_taken, 'costs': costs,\n",
    "            'method': method_name\n",
    "        }\n",
    "        \n",
    "        print(f\"   {method_name}: w={w:.4f}, b={b:.4f}, \"\n",
    "              f\"final_cost={costs[-1]:.6f}, time={time_taken:.3f}s\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run the experiment\n",
    "batch_results = experiment_batch_sizes()"
   ],
   "metadata": {
    "id": "experiment_batch_sizes"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Visualize batch size experiment results\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Plot convergence curves\n",
    "batch_sizes = list(batch_results.keys())\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(batch_sizes)))\n",
    "\n",
    "for batch_size, color in zip(batch_sizes, colors):\n",
    "    costs = batch_results[batch_size]['costs']\n",
    "    ax1.plot(costs, label=f'Batch size: {batch_size}', linewidth=2, color=color)\n",
    "\n",
    "ax1.set_title('üî¨ Convergence vs Batch Size')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Cost (MSE)')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_yscale('log')\n",
    "\n",
    "# Plot performance metrics\n",
    "final_costs = [batch_results[bs]['final_cost'] for bs in batch_sizes]\n",
    "times = [batch_results[bs]['time'] for bs in batch_sizes]\n",
    "\n",
    "ax2_twin = ax2.twinx()\n",
    "\n",
    "bars1 = ax2.bar([f'{bs}' for bs in batch_sizes], final_costs, \n",
    "               alpha=0.7, color='blue', label='Final Cost')\n",
    "bars2 = ax2_twin.bar([f'{bs}' for bs in batch_sizes], times, \n",
    "                   alpha=0.7, color='red', label='Time (s)')\n",
    "\n",
    "ax2.set_xlabel('Batch Size')\n",
    "ax2.set_ylabel('Final Cost', color='blue')\n",
    "ax2_twin.set_ylabel('Time (seconds)', color='red')\n",
    "ax2.set_title('üèÜ Performance vs Batch Size')\n",
    "\n",
    "ax2.legend(loc='upper left')\n",
    "ax2_twin.legend(loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Batch Size Analysis:\")\n",
    "for bs in batch_sizes:\n",
    "    print(f\"   Batch size {bs:3d}: Final cost = {batch_results[bs]['final_cost']:.6f}, \"\n",
    "          f\"Time = {batch_results[bs]['time']:.3f}s\")"
   ],
   "metadata": {
    "id": "batch_size_viz"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## üî¨ Experiment 2: Learning Rate Sensitivity"
   ],
   "metadata": {
    "id": "experiment2_header"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def experiment_learning_rates(learning_rates=[0.001, 0.01, 0.05, 0.1, 0.5, 1.0], epochs=50):\n",
    "    \"\"\"Test different learning rates with mini-batch GD\"\"\"\n",
    "    \n",
    "    print(f\"\\nüî¨ EXPERIMENT 2: Learning Rate Sensitivity\")\n",
    "    print(f\"üìà Testing learning rates: {learning_rates}\")\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for lr in learning_rates:\n",
    "        print(f\"\\nüß™ Testing learning_rate = {lr}\")\n",
    "        \n",
    "        try:\n",
    "            w, b, costs, time_taken = mini_batch_gradient_descent(\n",
    "                X, y, batch_size=32, epochs=epochs, learning_rate=lr, verbose=False\n",
    "            )\n",
    "            \n",
    "            results[lr] = {\n",
    "                'w': w, 'b': b, 'final_cost': costs[-1], \n",
    "                'time': time_taken, 'costs': costs,\n",
    "                'converged': costs[-1] < 10\n",
    "            }\n",
    "            \n",
    "            status = \"‚úÖ Converged\" if costs[-1] < 10 else \"‚ö†Ô∏è High cost\"\n",
    "            print(f\"   Result: w={w:.4f}, b={b:.4f}, final_cost={costs[-1]:.6f} {status}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            results[lr] = {'error': str(e), 'converged': False}\n",
    "            print(f\"   ‚ùå ERROR: Learning rate too large - likely diverged!\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run the experiment\n",
    "lr_results = experiment_learning_rates()"
   ],
   "metadata": {
    "id": "experiment_learning_rates"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Visualize learning rate experiment\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Filter out failed experiments\n",
    "successful_lrs = [lr for lr, result in lr_results.items() if 'costs' in result]\n",
    "colors = plt.cm.plasma(np.linspace(0, 1, len(successful_lrs)))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "for lr, color in zip(successful_lrs, colors):\n",
    "    costs = lr_results[lr]['costs']\n",
    "    plt.plot(costs, label=f'LR = {lr}', linewidth=2, color=color)\n",
    "\n",
    "plt.title('üî¨ Convergence vs Learning Rate')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Cost (MSE)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.yscale('log')\n",
    "\n",
    "# Performance analysis\n",
    "plt.subplot(1, 2, 2)\n",
    "final_costs = [lr_results[lr]['final_cost'] for lr in successful_lrs]\n",
    "plt.bar([str(lr) for lr in successful_lrs], final_costs, \n",
    "        alpha=0.7, color='orange')\n",
    "plt.title('üèÜ Final Cost vs Learning Rate')\n",
    "plt.xlabel('Learning Rate')\n",
    "plt.ylabel('Final Cost')\n",
    "plt.yscale('log')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Learning Rate Analysis:\")\n",
    "for lr in sorted(lr_results.keys()):\n",
    "    result = lr_results[lr]\n",
    "    if 'costs' in result:\n",
    "        status = \"Good\" if result['final_cost'] < 0.1 else \"Too high\"\n",
    "        print(f\"   LR {lr:4.3f}: Final cost = {result['final_cost']:.6f} ({status})\")\n",
    "    else:\n",
    "        print(f\"   LR {lr:4.3f}: ‚ùå Diverged\")"
   ],
   "metadata": {
    "id": "learning_rate_viz"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# üìö Student Exercises\n",
    "\n",
    "Now it's your turn to experiment!"
   ],
   "metadata": {
    "id": "exercises_header"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## üí° Exercise 1: Prediction Challenge\n",
    "\n",
    "Before running the code below, predict the outcomes!"
   ],
   "metadata": {
    "id": "exercise1_header"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Exercise 1: Make predictions before running!\n",
    "print(\"ü§î PREDICTION CHALLENGE\")\n",
    "print(\"Before running the experiments below, predict:\")\n",
    "print(\"\")\n",
    "print(\"1. Which will converge fastest: batch_size=16 or batch_size=128?\")\n",
    "print(\"2. Which will be more stable: learning_rate=0.01 or 0.1?\")\n",
    "print(\"3. Which will use more memory: Batch GD or Mini-batch GD?\")\n",
    "print(\"4. Which will make more parameter updates per epoch: SGD or Mini-batch?\")\n",
    "print(\"\")\n",
    "print(\"Write your predictions, then run the experiments to check!\")\n",
    "\n",
    "# TODO: Students write their predictions here as comments\n",
    "# My predictions:\n",
    "# 1. \n",
    "# 2. \n",
    "# 3. \n",
    "# 4. "
   ],
   "metadata": {
    "id": "exercise1_prediction"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## üí° Exercise 2: Custom Implementation\n",
    "\n",
    "Implement your own variant with modifications!"
   ],
   "metadata": {
    "id": "exercise2_header"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Exercise 2: Custom Implementation\n",
    "def your_custom_gradient_descent(X, y, epochs=100, **kwargs):\n",
    "    \"\"\"\n",
    "    TODO: Implement your own gradient descent variant!\n",
    "    \n",
    "    Ideas to try:\n",
    "    - Adaptive learning rate (decrease over time)\n",
    "    - Different batch size scheduling\n",
    "    - Add momentum (preview of next week!)\n",
    "    - Different initialization strategies\n",
    "    \n",
    "    Args:\n",
    "        X: Input features\n",
    "        y: Target values  \n",
    "        epochs: Number of training epochs\n",
    "        **kwargs: Additional parameters for your variant\n",
    "        \n",
    "    Returns:\n",
    "        w, b, costs, time_taken\n",
    "    \"\"\"\n",
    "    # TODO: Your implementation here!\n",
    "    \n",
    "    # Example starter code:\n",
    "    w, b = initialize_parameters()\n",
    "    costs = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Your innovation here!\n",
    "    \n",
    "    return w, b, costs, time.time() - start_time\n",
    "\n",
    "print(\"üõ†Ô∏è Implement your custom gradient descent variant above!\")\n",
    "print(\"üí° Ideas: adaptive learning rate, momentum, different batch strategies\")"
   ],
   "metadata": {
    "id": "exercise2_custom"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## üí° Exercise 3: Real Dataset Challenge\n",
    "\n",
    "Apply your knowledge to a real dataset!"
   ],
   "metadata": {
    "id": "exercise3_header"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Exercise 3: Real Dataset Challenge\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def create_realistic_dataset():\n",
    "    \"\"\"Create a more realistic regression dataset\"\"\"\n",
    "    X_real, y_real = make_regression(n_samples=5000, n_features=1, \n",
    "                                   noise=10, random_state=42)\n",
    "    \n",
    "    # Standardize the data (important for real datasets!)\n",
    "    scaler_X = StandardScaler()\n",
    "    scaler_y = StandardScaler()\n",
    "    \n",
    "    X_real = scaler_X.fit_transform(X_real)\n",
    "    y_real = scaler_y.fit_transform(y_real.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    return X_real, y_real\n",
    "\n",
    "# Create challenging dataset\n",
    "X_challenge, y_challenge = create_realistic_dataset()\n",
    "\n",
    "print(f\"üéØ CHALLENGE DATASET CREATED:\")\n",
    "print(f\"   Examples: {len(X_challenge)}\")\n",
    "print(f\"   X range: [{X_challenge.min():.2f}, {X_challenge.max():.2f}]\")\n",
    "print(f\"   y range: [{y_challenge.min():.2f}, {y_challenge.max():.2f}]\")\n",
    "print(f\"\")\n",
    "print(f\"üèÜ YOUR CHALLENGE:\")\n",
    "print(f\"   1. Apply all three GD variants to this dataset\")\n",
    "print(f\"   2. Find the best hyperparameters for each\")\n",
    "print(f\"   3. Which method works best and why?\")\n",
    "print(f\"   4. How does standardization affect convergence?\")\n",
    "\n",
    "# TODO: Students implement the challenge here!\n",
    "# Hint: You might need different learning rates than before"
   ],
   "metadata": {
    "id": "exercise3_challenge"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# üéØ Quick Assessment\n",
    "\n",
    "Test your understanding!"
   ],
   "metadata": {
    "id": "assessment_header"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Quick Knowledge Check\n",
    "print(\"üéØ QUICK ASSESSMENT - Answer these questions:\")\n",
    "print(\"\")\n",
    "print(\"1. For a dataset with 10,000 examples, how many parameter updates will SGD make per epoch?\")\n",
    "print(\"   Answer: _______\")\n",
    "print(\"\")\n",
    "print(\"2. If your model's loss is oscillating wildly, what are TWO things you could try?\")\n",
    "print(\"   Answer: _______ and _______\")\n",
    "print(\"\")\n",
    "print(\"3. For production deep learning, which GD variant is most commonly used and why?\")\n",
    "print(\"   Answer: _______\")\n",
    "print(\"\")\n",
    "print(\"4. What's the main advantage of SGD over Batch GD?\")\n",
    "print(\"   Answer: _______\")\n",
    "print(\"\")\n",
    "print(\"5. Why is mini-batch GD called the 'Goldilocks solution'?\")\n",
    "print(\"   Answer: _______\")\n",
    "\n",
    "# Answers (run this cell to check!)\n",
    "def show_answers():\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"üìù ANSWERS:\")\n",
    "    print(\"1. 10,000 updates (one per example)\")\n",
    "    print(\"2. Reduce learning rate AND/OR increase batch size\")\n",
    "    print(\"3. Mini-batch GD - balances speed, stability, and memory\")\n",
    "    print(\"4. Much faster updates, memory efficient, can escape local minima\")\n",
    "    print(\"5. Not too big (batch), not too small (SGD), just right!\")\n",
    "\n",
    "# Uncomment to see answers:\n",
    "# show_answers()"
   ],
   "metadata": {
    "id": "assessment_questions"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# üèÜ Summary and Key Takeaways"
   ],
   "metadata": {
    "id": "summary_header"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Final Summary\n",
    "print(\"üèÜ GRADIENT DESCENT VARIANTS: KEY TAKEAWAYS\")\n",
    "print(\"=\"*60)\n",
    "print(\"\")\n",
    "print(\"üìä COMPARISON TABLE:\")\n",
    "print(f\"{'Method':<15} {'Updates/Epoch':<15} {'Convergence':<12} {'Best For':<20}\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'Batch GD':<15} {'1':<15} {'Smooth':<12} {'Small datasets':<20}\")\n",
    "print(f\"{'Stochastic GD':<15} {'m (all examples)':<15} {'Noisy':<12} {'Online learning':<20}\")\n",
    "print(f\"{'Mini-batch GD':<15} {'m/batch_size':<15} {'Balanced':<12} {'Most problems':<20}\")\n",
    "print(\"\")\n",
    "print(\"üéØ DECISION FRAMEWORK:\")\n",
    "print(\"   üìè Dataset < 1,000 examples     ‚Üí Batch GD\")\n",
    "print(\"   üíæ Memory severely limited      ‚Üí Stochastic GD\")\n",
    "print(\"   üåä Streaming/online data        ‚Üí Stochastic GD\")\n",
    "print(\"   üè≠ Most production scenarios    ‚Üí Mini-batch GD\")\n",
    "print(\"\")\n",
    "print(\"‚ö° HYPERPARAMETER GUIDELINES:\")\n",
    "print(\"   üîµ Batch GD:     learning_rate = 0.1-1.0\")\n",
    "print(\"   üî¥ Stochastic:   learning_rate = 0.001-0.01\")\n",
    "print(\"   üü¢ Mini-batch:   learning_rate = 0.01-0.1, batch_size = 32-128\")\n",
    "print(\"\")\n",
    "print(\"üöÄ NEXT STEPS:\")\n",
    "print(\"   üìö Week 5: Advanced optimizers (Momentum, Adam, RMSprop)\")\n",
    "print(\"   üß† Why basic GD isn't enough for deep networks\")\n",
    "print(\"   ‚ö° Adaptive learning rates and momentum\")\n",
    "print(\"\")\n",
    "print(\"‚úÖ You now understand the foundation of neural network optimization!\")"
   ],
   "metadata": {
    "id": "final_summary"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# üìù Take-Home Assignment\n",
    "\n",
    "**Due: Next class session**\n",
    "\n",
    "## Assignment: GD Variants on Real Data\n",
    "\n",
    "1. **Choose a dataset** from sklearn.datasets or find one online\n",
    "2. **Implement all three GD variants** on your chosen dataset\n",
    "3. **Experiment with hyperparameters** to find optimal settings\n",
    "4. **Create visualizations** comparing convergence patterns\n",
    "5. **Write analysis** explaining which method works best and why\n",
    "6. **Bonus:** Compare with sklearn's SGD implementation\n",
    "\n",
    "**Submission:** Jupyter notebook with code, plots, and analysis\n",
    "\n",
    "**Grading Criteria:**\n",
    "- Implementation correctness (40%)\n",
    "- Experimental design (25%)\n",
    "- Analysis depth (25%)\n",
    "- Presentation quality (10%)\n",
    "\n",
    "---\n",
    "\n",
    "**üéì Congratulations on completing the Gradient Descent Variants Workshop!**\n",
    "\n",
    "You now have the foundational knowledge to optimize neural networks effectively. Next week, we'll explore advanced optimization techniques that make training deep networks possible!"
   ],
   "metadata": {
    "id": "assignment_cell"
   }
  }
 ]
}