
Assessment

- Quick quiz:
    - Gradient: direction of steepest increase; use âˆ’âˆ‡J for descent.
    - Update rule: Î¸ â† Î¸ âˆ’ Î±âˆ‡J(Î¸). Stop when ||âˆ‡J|| < Îµ or Î”J < Î´.
    - MSE vs CE: MSE for regression; CE (with logits) for classification; CE gives stronger gradients when confidently wrong.
    - Learning rate: too small = slow; too large = oscillation/divergence.
    - Learning rate: too small = slow; too large = oscillation/divergence.
-
Exit ticket (2â€“3 minutes):
    - Write the update for a single weight w given gradient g and Î±.
    - Name two landscape features that slow GD and why (plateaus, saddles).
    - If loss oscillates between epochs, what change would you try first? (reduce Î±).
-
Mini exercise (sanity check):
    - Using f(x)=xÂ², x0=âˆ’6: compute next two steps for Î±=0.5 and Î±=1.2; predict converge vs diverge.
    - Explain how the same logic applies to a NNâ€™s last layer with CE loss.

Day 4 Preview

- Goals:
    - Compare Batch, Stochastic, and Mini-batch GD in theory and code.
    - Visualize convergence patterns and analyze compute/memory trade-offs.
    - Tune batch size and learning rate; introduce simple schedules.
    - Tune batch size and learning rate; introduce simple schedules.
-
Plan:
    - Implement three variants on a toy regression set; plot cost vs epoch.
    - Discuss when to use each method; introduce class defaults (mini-batch ~32).
    - Practical heuristics: scale Î± with batch size; shuffle; monitor stability.
    - Tease advanced optimizers (momentum, Adam) and why they help.


  # Variants overview: Batch vs SGD vs Mini-batch
    â–¡ Batch GD details
    â–¡ SGD details
    â–¡ Mini-batch details
    â–¡ Implementation + comparison
    â–¡ Hyperparameter tuning
    â–¡ Assessment + takeaways


## **Variants Overview

- Batch GD: uses full dataset per update.
    - Update: Î¸ â† Î¸ âˆ’ Î± Â· (1/m) Î£_i âˆ‡_Î¸ ğ“›(f_Î¸(x_i), y_i)
    - Pattern: smooth convergence; 1 update/epoch.
- SGD: updates per single example.
    - Update: Î¸ â† Î¸ âˆ’ Î± Â· âˆ‡_Î¸ ğ“›(f_Î¸(x_i), y_i) for each i
    - Pattern: noisy zigzag; many updates/epoch.
- Mini-batch GD: updates per small batch b.
    - Update: Î¸ â† Î¸ âˆ’ Î± Â· (1/b) Î£_{iâˆˆB} âˆ‡_Î¸ ğ“›(f_Î¸(x_i), y_i)
    - Pattern: balanced noise/speed; GPU-friendly.
Mini-batch GD: updates per small batch b.
    - Update: Î¸ â† Î¸ âˆ’ Î± Â· (1/b) Î£_{iâˆˆB} âˆ‡_Î¸ ğ“›(f_Î¸(x_i), y_i)
    - Pattern: balanced noise/speed; GPU-friendly.
-
Trade-offs:
    - Stability: Batch > Mini-batch > SGD
    - Speed/Wall time: Mini-batch â‰¥ SGD > Batch
    - Memory: SGD < Mini-batch < Batch
-
When to use:
    - Small datasets, deterministic runs â†’ Batch GD.
    - Streaming/online or tight memory â†’ SGD.
    - Most practical deep learning â†’ Mini-batch (e.g., 32â€“128).


## **Batch GD details
Batch GD Details

- Definition: Uses the entire training set per update: Î¸ â† Î¸ âˆ’ Î± (1/m) Î£_i âˆ‡_Î¸ ğ“›(f_Î¸(x_i), y_i).
- Convergence: Smooth, deterministic path; for convex/quadratic losses, converges if 0 < Î± < 2/L (L = Lipschitz constant of âˆ‡J).
- Pros: Stable updates; exact gradient; easy to reason/debug; reproducible.
- Cons: One update per epoch â†’ slow on large datasets; high memory footprint; poor hardware utilization vs mini-batching.
-
Pros: Stable updates; exact gradient; easy to reason/debug; reproducible.
-
Cons: One update per epoch â†’ slow on large datasets; high memory footprint; poor hardware utilization vs mini-batching.
-
Pseudocode:
    - initialize Î¸
    - repeat for epochs:
    - compute predictions over all m samples
    - compute mean loss J(Î¸)
    - compute gradient âˆ‡J(Î¸) over all m
    - update Î¸ â† Î¸ âˆ’ Î± âˆ‡J(Î¸)

- Complexity: Each step costs one full forward + backward pass over m; wall-time heavy when m is large.
Complexity: Each step costs one full forward + backward pass over m; wall-time heavy when m is large.
-
When To Use: Small datasets (<10k), deterministic research settings, or to benchmark expected convergence behavior.
-
Numerical Tips:
    - Normalize/standardize inputs; use â€œwith logitsâ€ losses for CE.
    - Regularization: L2 adds âˆ’Î±Î»Î¸ term to update.
    - If loss increases, reduce Î±; consider line search for convex problems.
-
Framework Notes:
    - PyTorch: load whole dataset into a single batch (may be memory-bound); otherwise, simulate by aggregating all gradients before step.
    - Keras: set batch_size = len(X_train) for full-batch (watch RAM/GPU limits).


## **SGD details
- Definition: Update per single example i:
    - Î¸ â† Î¸ âˆ’ Î± âˆ‡Î¸ ğ“›(fÎ¸(x_i), y_i) after each sample (randomly ordered).
- Convergence: Noisy path; for convex problems converges in expectation with diminishing Î± (e.g., Î±_t âˆ 1/t).
- Pros: Very frequent updates; memory-light; supports online/streaming data; noise can escape shallow minima/saddles.
- Cons: High variance gradients; sensitive to Î±; loss curve fluctuates; slower final convergence without variance reduction.
-
Pros: Very frequent updates; memory-light; supports online/streaming data; noise can escape shallow minima/saddles.
-
Cons: High variance gradients; sensitive to Î±; loss curve fluctuates; slower final convergence without variance reduction.
- Working (7s â€¢ Esc to interrupt)
Practical setup
    - Shuffle each epoch (random reshuffling) rather than pure sampling with replacement.
    - Start with smaller Î± than batch GD; consider decay: Î±_t = Î±0/(1+kt) or ReduceLROnPlateau.
    - Normalize inputs; consider gradient clipping if spikes occur.
-
Pseudocode
    - for epoch: shuffle indices; for i in indices:
    - g â† âˆ‡Î¸ ğ“›(fÎ¸(x_i), y_i)
    - Î¸ â† Î¸ âˆ’ Î± g

- Variance control
    - Polyak/Ruppert averaging: keep running average of Î¸ to stabilize final iterate.
    - Mini-batch b=1 is exact SGD; in practice bâˆˆ{4,8} often outperforms strict b=1.
    - Mini-batch b=1 is exact SGD; in practice bâˆˆ{4,8} often outperforms strict b=1.
-
Framework notes
    - PyTorch: use DataLoader with batch_size=1; remember zero_grad() each step.
    - Keras: model.fit(..., batch_size=1). Beware of poor device utilization.
-
When to use
    - Streaming data, severe memory limits, or when intentional noise aids exploration.
    - As a teaching/diagnostic baseline before moving to mini-batch with momentum.


## **Mini-batch details
Mini-batch Details

- Definition: Update per batch B of size b:
Î¸ â† Î¸ âˆ’ Î± Â· (1/b) Î£_{iâˆˆB} âˆ‡Î¸ ğ“›(fÎ¸(x_i), y_i)
- Why itâ€™s default: Balanced noise/smoothness; efficient vectorization; excellent GPU utilization; manageable memory.
- Typical sizes: 16â€“256 (commonly 32, 64, 128). Choose by VRAM/CPU RAM and stability.
- Convergence pattern: Smoother than SGD, faster than full-batch; controlled stochasticity helps escape saddles.
- Pseudocode:
    - for epoch: shuffle; for batches B:
    - g â† (1/b) Î£_{iâˆˆB} âˆ‡Î¸ ğ“›(fÎ¸(x_i), y_i)
    - Î¸ â† Î¸ âˆ’ Î± g
- Practical tips:
    - Shuffle each epoch; set drop_last=True for consistent batch norms.
    - Linear scaling rule: if b â†’ kÂ·b, try Î± â†’ kÂ·Î± with short warmup.
    - Gradient accumulation simulates large b when memory-limited (accumulate g for k steps before step()).
    - BatchNorm needs sufficiently large b; for tiny b, consider GroupNorm/LayerNorm.
- Framework notes:
    - PyTorch DataLoader: batch_size=b, shuffle=True, num_workers>0, pin_memory=True (GPU).
    - Keras: model.fit(..., batch_size=b); monitor steps_per_epoch for dataset iterators.
- When to use: Almost always in deep learning; start with bâ‰ˆ32â€“128, tune by utilization and loss smoothness.