{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 9 DO4 - Introduction to CNNs: Interactive Demo\n",
    "## Deep Neural Network Architectures (21CSE558T)\n",
    "\n",
    "**Date:** October 17, 2025  \n",
    "**Module:** 4 - Convolutional Neural Networks (Introduction)  \n",
    "**Duration:** Interactive hands-on demonstration  \n",
    "**Learning Outcome:** CO-4 - Implement convolutional neural networks\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Notebook Overview\n",
    "\n",
    "This interactive notebook brings to life the concepts from today's lecture:\n",
    "1. **Manual Feature Extraction** (What we learned Week 9)\n",
    "2. **Convolution Operation** (The \"Postal Detective\" analogy in action)\n",
    "3. **Pooling Operation** (The \"Trophy Cabinet\" concept)\n",
    "4. **Complete CNN Architecture** (Putting it all together)\n",
    "5. **Comparing Approaches** (Traditional ML vs CNN)\n",
    "\n",
    "### üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- **See** convolution in action with real images\n",
    "- **Visualize** how filters detect patterns\n",
    "- **Understand** pooling through examples\n",
    "- **Build** your first simple CNN\n",
    "- **Compare** manual features vs automatic learning\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Setup: Install and Import Libraries\n",
    "\n",
    "Let's start by setting up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment if needed)\n",
    "# !pip install tensorflow numpy matplotlib opencv-python-headless scikit-learn\n",
    "\n",
    "# Import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from scipy import signal\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Configure matplotlib for better plots\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: The Problem with Manual Features\n",
    "\n",
    "## Revisiting Week 9 - What We Did Manually\n",
    "\n",
    "Remember last lecture (Oct 15)? We manually extracted features:\n",
    "- **Shape features:** Area, circularity, Hu moments\n",
    "- **Color features:** Histograms, color moments\n",
    "- **Texture features:** Edge density, GLCM\n",
    "\n",
    "Let's see this in action with a simple example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create simple test images\n",
    "def create_simple_shapes():\n",
    "    \"\"\"Create three simple shapes for demonstration.\"\"\"\n",
    "    size = 64\n",
    "    \n",
    "    # Circle\n",
    "    circle = np.zeros((size, size), dtype=np.uint8)\n",
    "    cv2.circle(circle, (size//2, size//2), size//3, 255, -1)\n",
    "    \n",
    "    # Square\n",
    "    square = np.zeros((size, size), dtype=np.uint8)\n",
    "    cv2.rectangle(square, (size//4, size//4), (3*size//4, 3*size//4), 255, -1)\n",
    "    \n",
    "    # Triangle\n",
    "    triangle = np.zeros((size, size), dtype=np.uint8)\n",
    "    pts = np.array([[size//2, size//4], [size//4, 3*size//4], [3*size//4, 3*size//4]])\n",
    "    cv2.fillPoly(triangle, [pts], 255)\n",
    "    \n",
    "    return circle, square, triangle\n",
    "\n",
    "# Create shapes\n",
    "circle, square, triangle = create_simple_shapes()\n",
    "\n",
    "# Display\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "axes[0].imshow(circle, cmap='gray')\n",
    "axes[0].set_title('Circle', fontweight='bold', fontsize=14)\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(square, cmap='gray')\n",
    "axes[1].set_title('Square', fontweight='bold', fontsize=14)\n",
    "axes[1].axis('off')\n",
    "\n",
    "axes[2].imshow(triangle, cmap='gray')\n",
    "axes[2].set_title('Triangle', fontweight='bold', fontsize=14)\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.suptitle('Simple Shapes for Classification', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìê Created 3 simple shapes: Circle, Square, Triangle\")\n",
    "print(f\"   Image size: {circle.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual Feature Extraction (Week 9 Approach)\n",
    "\n",
    "Let's extract features **manually** as we learned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_manual_features(image):\n",
    "    \"\"\"Extract manual features like we did in Week 9.\"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    # Find contour\n",
    "    contours, _ = cv2.findContours(image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    if len(contours) == 0:\n",
    "        return None\n",
    "    contour = max(contours, key=cv2.contourArea)\n",
    "    \n",
    "    # Shape features\n",
    "    area = cv2.contourArea(contour)\n",
    "    perimeter = cv2.arcLength(contour, True)\n",
    "    circularity = 4 * np.pi * area / (perimeter ** 2) if perimeter > 0 else 0\n",
    "    \n",
    "    x, y, w, h = cv2.boundingRect(contour)\n",
    "    aspect_ratio = float(w) / h if h > 0 else 0\n",
    "    \n",
    "    hull = cv2.convexHull(contour)\n",
    "    hull_area = cv2.contourArea(hull)\n",
    "    solidity = area / hull_area if hull_area > 0 else 0\n",
    "    \n",
    "    features['area'] = area\n",
    "    features['perimeter'] = perimeter\n",
    "    features['circularity'] = circularity\n",
    "    features['aspect_ratio'] = aspect_ratio\n",
    "    features['solidity'] = solidity\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Extract features for all shapes\n",
    "shapes = {'Circle': circle, 'Square': square, 'Triangle': triangle}\n",
    "manual_features = {}\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MANUAL FEATURE EXTRACTION (Week 9 Approach)\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "for name, shape in shapes.items():\n",
    "    features = extract_manual_features(shape)\n",
    "    manual_features[name] = features\n",
    "    print(f\"üìä {name}:\")\n",
    "    print(f\"   Area:         {features['area']:.0f}\")\n",
    "    print(f\"   Perimeter:    {features['perimeter']:.2f}\")\n",
    "    print(f\"   Circularity:  {features['circularity']:.3f} (1.0 = perfect circle)\")\n",
    "    print(f\"   Aspect Ratio: {features['aspect_ratio']:.3f} (1.0 = square)\")\n",
    "    print(f\"   Solidity:     {features['solidity']:.3f} (1.0 = convex)\")\n",
    "    print()\n",
    "\n",
    "print(\"‚úÖ We designed these 5 features manually!\")\n",
    "print(\"‚ö†Ô∏è  Problem: What if there are patterns we didn't think of?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: Convolution - The Detective Maria Demo\n",
    "\n",
    "## üîç The Postal Detective in Action\n",
    "\n",
    "Remember Detective Maria with her stamp? Let's see convolution work!\n",
    "\n",
    "**Recall the analogy:**\n",
    "- **Reference stamp** = Filter (3√ó3 matrix)\n",
    "- **Suspicious letter** = Image\n",
    "- **Sliding the stamp** = Convolution operation\n",
    "- **Checking similarity** = Dot product (multiply + sum)\n",
    "- **Result map** = Feature map (where patterns were found)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Create Test Image with Edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple image with vertical edge\n",
    "test_image = np.zeros((7, 7), dtype=np.float32)\n",
    "test_image[:, :3] = 50   # Dark region (left)\n",
    "test_image[:, 4:] = 200  # Bright region (right)\n",
    "# Column 3 is the transition (edge)\n",
    "test_image[:, 3] = 125\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(test_image, cmap='gray', interpolation='nearest')\n",
    "plt.colorbar(label='Pixel Intensity')\n",
    "plt.title('Test Image: Vertical Edge in Middle', fontweight='bold', fontsize=14)\n",
    "plt.xlabel('X (columns)')\n",
    "plt.ylabel('Y (rows)')\n",
    "\n",
    "# Add grid\n",
    "for i in range(8):\n",
    "    plt.axhline(i-0.5, color='red', linewidth=0.5, alpha=0.3)\n",
    "    plt.axvline(i-0.5, color='red', linewidth=0.5, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüì∑ Test Image Created:\")\n",
    "print(\"   Left side (dark):   Pixel value = 50\")\n",
    "print(\"   Middle (edge):      Pixel value = 125\")\n",
    "print(\"   Right side (bright): Pixel value = 200\")\n",
    "print(\"\\nüéØ Goal: Detect the vertical edge!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Create the \"Reference Stamp\" (Filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vertical edge detector filter (Detective Maria's \"reference stamp\")\n",
    "vertical_edge_filter = np.array([[-1, 0, 1],\n",
    "                                  [-1, 0, 1],\n",
    "                                  [-1, 0, 1]], dtype=np.float32)\n",
    "\n",
    "# Horizontal edge detector\n",
    "horizontal_edge_filter = np.array([[ 1,  1,  1],\n",
    "                                    [ 0,  0,  0],\n",
    "                                    [-1, -1, -1]], dtype=np.float32)\n",
    "\n",
    "# Display filters\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "im1 = axes[0].imshow(vertical_edge_filter, cmap='RdBu_r', vmin=-1, vmax=1)\n",
    "axes[0].set_title('Vertical Edge Detector\\n(Reference Stamp)', fontweight='bold')\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        axes[0].text(j, i, f'{vertical_edge_filter[i, j]:.0f}',\n",
    "                    ha='center', va='center', fontsize=12, fontweight='bold')\n",
    "plt.colorbar(im1, ax=axes[0])\n",
    "\n",
    "im2 = axes[1].imshow(horizontal_edge_filter, cmap='RdBu_r', vmin=-1, vmax=1)\n",
    "axes[1].set_title('Horizontal Edge Detector\\n(Another Reference)', fontweight='bold')\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        axes[1].text(j, i, f'{horizontal_edge_filter[i, j]:.0f}',\n",
    "                    ha='center', va='center', fontsize=12, fontweight='bold')\n",
    "plt.colorbar(im2, ax=axes[1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìÆ Detective Maria's Reference Stamps:\")\n",
    "print(\"\\n   Vertical Edge Filter:\")\n",
    "print(\"   [-1  0  1]  ‚Üê Looks for dark-to-bright transition (left to right)\")\n",
    "print(\"   [-1  0  1]\")\n",
    "print(\"   [-1  0  1]\")\n",
    "print(\"\\n   Horizontal Edge Filter:\")\n",
    "print(\"   [ 1  1  1]  ‚Üê Looks for bright-to-dark transition (top to bottom)\")\n",
    "print(\"   [ 0  0  0]\")\n",
    "print(\"   [-1 -1 -1]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Perform Convolution (Detective Maria Slides the Stamp!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_convolution_demo(image, filter_kernel, position=(0, 0)):\n",
    "    \"\"\"Demonstrate convolution at a specific position with detailed output.\"\"\"\n",
    "    i, j = position\n",
    "    \n",
    "    # Extract region\n",
    "    region = image[i:i+3, j:j+3]\n",
    "    \n",
    "    # Element-wise multiplication\n",
    "    multiplied = region * filter_kernel\n",
    "    \n",
    "    # Sum (the similarity score)\n",
    "    result = np.sum(multiplied)\n",
    "    \n",
    "    return region, multiplied, result\n",
    "\n",
    "# Try convolution at different positions\n",
    "positions = [(2, 0), (2, 2), (2, 4)]  # Left, Middle (edge!), Right\n",
    "position_names = ['Left (Dark Region)', 'Middle (EDGE!)', 'Right (Bright Region)']\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CONVOLUTION IN ACTION: Detective Maria Checking Each Position\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "for pos, name in zip(positions, position_names):\n",
    "    region, multiplied, result = manual_convolution_demo(test_image, vertical_edge_filter, pos)\n",
    "    \n",
    "    print(f\"\\nüìç Position {pos} - {name}:\")\n",
    "    print(\"\\n   Image Region:\")\n",
    "    print(region)\n",
    "    print(\"\\n   √ó Filter:\")\n",
    "    print(vertical_edge_filter)\n",
    "    print(\"\\n   = Element-wise Product:\")\n",
    "    print(multiplied)\n",
    "    print(f\"\\n   Sum (Similarity Score): {result:.1f}\")\n",
    "    \n",
    "    if abs(result) > 300:\n",
    "        print(\"   üéØ HIGH SIMILARITY - EDGE DETECTED! (Genuine stamp!)\")\n",
    "    else:\n",
    "        print(\"   ‚ùå Low similarity - No edge here (Forgery!)\")\n",
    "    print(\"   \" + \"-\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Complete Convolution - Slide Across Entire Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform full convolution using scipy\n",
    "vertical_response = signal.correlate2d(test_image, vertical_edge_filter, mode='valid')\n",
    "horizontal_response = signal.correlate2d(test_image, horizontal_edge_filter, mode='valid')\n",
    "\n",
    "# Visualize results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Original image\n",
    "axes[0, 0].imshow(test_image, cmap='gray')\n",
    "axes[0, 0].set_title('Original Image\\n(Dark | Edge | Bright)', fontweight='bold', fontsize=12)\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "# Vertical edge filter\n",
    "axes[0, 1].imshow(vertical_edge_filter, cmap='RdBu_r', vmin=-1, vmax=1)\n",
    "axes[0, 1].set_title('Vertical Edge Filter\\n(Reference Stamp)', fontweight='bold', fontsize=12)\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        axes[0, 1].text(j, i, f'{vertical_edge_filter[i, j]:.0f}',\n",
    "                       ha='center', va='center', fontsize=10, fontweight='bold')\n",
    "axes[0, 1].axis('off')\n",
    "\n",
    "# Vertical edge response (feature map)\n",
    "im1 = axes[1, 0].imshow(vertical_response, cmap='hot', interpolation='nearest')\n",
    "axes[1, 0].set_title('Feature Map: Vertical Edge Response\\n(Where edges were found)', \n",
    "                     fontweight='bold', fontsize=12)\n",
    "plt.colorbar(im1, ax=axes[1, 0], label='Activation Strength')\n",
    "for i in range(vertical_response.shape[0]):\n",
    "    for j in range(vertical_response.shape[1]):\n",
    "        axes[1, 0].text(j, i, f'{vertical_response[i, j]:.0f}',\n",
    "                       ha='center', va='center', fontsize=8,\n",
    "                       color='white' if vertical_response[i, j] > 200 else 'black')\n",
    "\n",
    "# Horizontal edge response\n",
    "im2 = axes[1, 1].imshow(horizontal_response, cmap='hot', interpolation='nearest')\n",
    "axes[1, 1].set_title('Feature Map: Horizontal Edge Response\\n(Should be low - no horizontal edges)', \n",
    "                     fontweight='bold', fontsize=12)\n",
    "plt.colorbar(im2, ax=axes[1, 1], label='Activation Strength')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CONVOLUTION RESULTS\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nüìä Vertical Edge Detector Response:\")\n",
    "print(vertical_response)\n",
    "print(\"\\nüîç Interpretation:\")\n",
    "print(\"   - High values (orange/white in heat map): EDGE DETECTED!\")\n",
    "print(\"   - Low values (dark in heat map): No edge\")\n",
    "print(\"\\n‚úÖ The filter successfully found the vertical edge!\")\n",
    "print(\"   This is EXACTLY what Detective Maria's stamp does!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real Image Example: Edge Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a more interesting test image\n",
    "real_test = np.zeros((100, 100), dtype=np.float32)\n",
    "cv2.rectangle(real_test, (30, 30), (70, 70), 255, -1)\n",
    "\n",
    "# Apply multiple filters\n",
    "vertical_edges = signal.correlate2d(real_test, vertical_edge_filter, mode='same')\n",
    "horizontal_edges = signal.correlate2d(real_test, horizontal_edge_filter, mode='same')\n",
    "\n",
    "# Combine (magnitude)\n",
    "edge_magnitude = np.sqrt(vertical_edges**2 + horizontal_edges**2)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "axes[0, 0].imshow(real_test, cmap='gray')\n",
    "axes[0, 0].set_title('Original: White Square on Black', fontweight='bold')\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "axes[0, 1].imshow(vertical_edges, cmap='RdBu_r')\n",
    "axes[0, 1].set_title('Vertical Edges Detected', fontweight='bold')\n",
    "axes[0, 1].axis('off')\n",
    "\n",
    "axes[1, 0].imshow(horizontal_edges, cmap='RdBu_r')\n",
    "axes[1, 0].set_title('Horizontal Edges Detected', fontweight='bold')\n",
    "axes[1, 0].axis('off')\n",
    "\n",
    "axes[1, 1].imshow(edge_magnitude, cmap='hot')\n",
    "axes[1, 1].set_title('Combined Edge Strength', fontweight='bold')\n",
    "axes[1, 1].axis('off')\n",
    "\n",
    "plt.suptitle('Multiple Filters Detecting Different Patterns', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüé® Multiple Detective Mar√≠as Working Together:\")\n",
    "print(\"   Detective 1 (Vertical filter):   Found left & right edges\")\n",
    "print(\"   Detective 2 (Horizontal filter): Found top & bottom edges\")\n",
    "print(\"   Combined:                        Complete edge map!\")\n",
    "print(\"\\n‚úÖ This is why CNNs use 32, 64, or even 128 filters!\")\n",
    "print(\"   Each filter specializes in detecting different patterns.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 3: Pooling - The Trophy Cabinet Demo\n",
    "\n",
    "## üèÜ Keeping Only the Best\n",
    "\n",
    "Remember the trophy cabinet analogy? You played 100 games but only keep the 10 best trophies.\n",
    "\n",
    "**Max Pooling does the same:**\n",
    "- Takes a region (e.g., 2√ó2)\n",
    "- Keeps only the MAXIMUM value\n",
    "- Reduces size while keeping important information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create example feature map\n",
    "feature_map = np.array([[1, 3, 2, 4],\n",
    "                        [5, 6, 1, 2],\n",
    "                        [7, 2, 8, 3],\n",
    "                        [1, 4, 3, 9]], dtype=np.float32)\n",
    "\n",
    "def max_pool_2x2(input_array):\n",
    "    \"\"\"Apply 2x2 max pooling.\"\"\"\n",
    "    h, w = input_array.shape\n",
    "    output = np.zeros((h//2, w//2), dtype=np.float32)\n",
    "    \n",
    "    for i in range(0, h, 2):\n",
    "        for j in range(0, w, 2):\n",
    "            region = input_array[i:i+2, j:j+2]\n",
    "            output[i//2, j//2] = np.max(region)\n",
    "    \n",
    "    return output\n",
    "\n",
    "pooled = max_pool_2x2(feature_map)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Before pooling\n",
    "im1 = axes[0].imshow(feature_map, cmap='YlOrRd', vmin=0, vmax=9)\n",
    "axes[0].set_title('Before Pooling (4√ó4)\\nAll Values', fontweight='bold', fontsize=14)\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        axes[0].text(j, i, f'{feature_map[i, j]:.0f}',\n",
    "                    ha='center', va='center', fontsize=14, fontweight='bold',\n",
    "                    color='white' if feature_map[i, j] > 5 else 'black')\n",
    "        \n",
    "# Draw pooling regions\n",
    "for i in [1.5]:\n",
    "    axes[0].axhline(i, color='blue', linewidth=3)\n",
    "    axes[0].axvline(i, color='blue', linewidth=3)\n",
    "    \n",
    "plt.colorbar(im1, ax=axes[0])\n",
    "\n",
    "# After pooling\n",
    "im2 = axes[1].imshow(pooled, cmap='YlOrRd', vmin=0, vmax=9)\n",
    "axes[1].set_title('After Max Pooling (2√ó2)\\nOnly Maximum Values Kept', fontweight='bold', fontsize=14)\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        axes[1].text(j, i, f'{pooled[i, j]:.0f}',\n",
    "                    ha='center', va='center', fontsize=18, fontweight='bold',\n",
    "                    color='white')\n",
    "        \n",
    "plt.colorbar(im2, ax=axes[1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MAX POOLING DEMONSTRATION\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nüèÜ Trophy Cabinet Selection Process:\\n\")\n",
    "print(f\"   Top-left 2√ó2 region:    {feature_map[0:2, 0:2].flatten()} ‚Üí Keep MAX = {pooled[0,0]:.0f}\")\n",
    "print(f\"   Top-right 2√ó2 region:   {feature_map[0:2, 2:4].flatten()} ‚Üí Keep MAX = {pooled[0,1]:.0f}\")\n",
    "print(f\"   Bottom-left 2√ó2 region: {feature_map[2:4, 0:2].flatten()} ‚Üí Keep MAX = {pooled[1,0]:.0f}\")\n",
    "print(f\"   Bottom-right 2√ó2 region:{feature_map[2:4, 2:4].flatten()} ‚Üí Keep MAX = {pooled[1,1]:.0f}\")\n",
    "print(\"\\n‚úÖ Results:\")\n",
    "print(f\"   Size reduced: 4√ó4 = 16 values ‚Üí 2√ó2 = 4 values (75% reduction!)\")\n",
    "print(f\"   Information preserved: Kept strongest signals\")\n",
    "print(\"   Benefits: Faster computation, translation invariance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooling on Real Feature Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the edge detection result from before\n",
    "pooled_edges = max_pool_2x2(edge_magnitude[::2, ::2])  # Downsample first to make even dimensions\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "axes[0].imshow(edge_magnitude, cmap='hot')\n",
    "axes[0].set_title(f'Before Pooling\\nSize: {edge_magnitude.shape}', fontweight='bold')\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(pooled_edges, cmap='hot')\n",
    "axes[1].set_title(f'After Pooling\\nSize: {pooled_edges.shape}', fontweight='bold')\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.suptitle('Max Pooling: Reduce Size, Keep Important Information', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìä Pooling Statistics:\")\n",
    "print(f\"   Original size: {edge_magnitude.shape}\")\n",
    "print(f\"   Pooled size: {pooled_edges.shape}\")\n",
    "print(f\"   Size reduction: {100 * (1 - pooled_edges.size / edge_magnitude.size):.1f}%\")\n",
    "print(f\"\\n‚úÖ Edges are still clearly visible after pooling!\")\n",
    "print(f\"   We kept the important information (strong edges) while reducing size.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 4: Complete CNN Architecture\n",
    "\n",
    "## üè≠ The Factory Assembly Line in Action\n",
    "\n",
    "Now let's build a complete CNN and see how it all comes together!\n",
    "\n",
    "**Architecture:**\n",
    "```\n",
    "Input (28√ó28 grayscale image)\n",
    "    ‚Üì\n",
    "Conv Layer 1: 16 filters (3√ó3) + ReLU\n",
    "    ‚Üì\n",
    "Max Pool (2√ó2)\n",
    "    ‚Üì\n",
    "Conv Layer 2: 32 filters (3√ó3) + ReLU\n",
    "    ‚Üì\n",
    "Max Pool (2√ó2)\n",
    "    ‚Üì\n",
    "Flatten\n",
    "    ‚Üì\n",
    "Dense Layer: 64 neurons + ReLU\n",
    "    ‚Üì\n",
    "Output Layer: 3 classes (Circle, Square, Triangle)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a simple CNN\n",
    "def create_simple_cnn(input_shape=(28, 28, 1), num_classes=3):\n",
    "    \"\"\"Create a simple CNN for shape classification.\"\"\"\n",
    "    model = keras.Sequential([\n",
    "        # Layer 1: Learn simple features (edges, gradients)\n",
    "        layers.Conv2D(16, (3, 3), activation='relu', input_shape=input_shape, name='conv1'),\n",
    "        layers.MaxPooling2D((2, 2), name='pool1'),\n",
    "        \n",
    "        # Layer 2: Learn complex features (corners, textures)\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', name='conv2'),\n",
    "        layers.MaxPooling2D((2, 2), name='pool2'),\n",
    "        \n",
    "        # Flatten and classify\n",
    "        layers.Flatten(name='flatten'),\n",
    "        layers.Dense(64, activation='relu', name='dense'),\n",
    "        layers.Dense(num_classes, activation='softmax', name='output')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create model\n",
    "model = create_simple_cnn()\n",
    "\n",
    "# Display architecture\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CNN ARCHITECTURE\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "model.summary()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"UNDERSTANDING THE ARCHITECTURE\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nüè≠ Factory Assembly Line Breakdown:\\n\")\n",
    "print(\"   INPUT (28√ó28√ó1):\")\n",
    "print(\"      Raw image pixels\")\n",
    "print(\"\\n   CONV LAYER 1 (16 filters):\")\n",
    "print(\"      16 different detectives checking for simple patterns\")\n",
    "print(\"      Each detective has 3√ó3 reference stamp\")\n",
    "print(\"      Output: 26√ó26√ó16 (16 feature maps)\")\n",
    "print(\"\\n   MAX POOL 1:\")\n",
    "print(\"      Keep only strongest signals (trophy cabinet)\")\n",
    "print(\"      Output: 13√ó13√ó16 (50% size reduction)\")\n",
    "print(\"\\n   CONV LAYER 2 (32 filters):\")\n",
    "print(\"      32 detectives looking for complex patterns\")\n",
    "print(\"      Combine features from Layer 1\")\n",
    "print(\"      Output: 11√ó11√ó32 (32 feature maps)\")\n",
    "print(\"\\n   MAX POOL 2:\")\n",
    "print(\"      Another compression\")\n",
    "print(\"      Output: 5√ó5√ó32\")\n",
    "print(\"\\n   FLATTEN:\")\n",
    "print(\"      Convert 5√ó5√ó32 = 800 features into single list\")\n",
    "print(\"\\n   DENSE LAYER (64 neurons):\")\n",
    "print(\"      Combine all features intelligently\")\n",
    "print(\"\\n   OUTPUT (3 neurons):\")\n",
    "print(\"      Final decision: Circle? Square? Triangle?\")\n",
    "print(\"\\n‚úÖ Total: Just 38,307 parameters to learn ALL features automatically!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Data Flow Through CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create dummy input to trace through network\ndummy_input = np.random.rand(1, 28, 28, 1).astype(np.float32)\n\n# Alternative approach: Extract outputs by passing through each layer sequentially\nactivations = []\nx = dummy_input\n\n# Pass through first 4 layers (conv1, pool1, conv2, pool2)\nfor layer in model.layers[:4]:\n    x = layer(x)\n    activations.append(x)\n\n# Display shapes at each layer\nprint(\"\\n\" + \"=\"*70)\nprint(\"DATA FLOW THROUGH CNN\")\nprint(\"=\"*70 + \"\\n\")\nprint(f\"   Input shape:        {dummy_input.shape} = {np.prod(dummy_input.shape[1:])} values\")\nprint(f\"   After Conv1:        {activations[0].shape} = {np.prod(activations[0].shape[1:])} values\")\nprint(f\"   After Pool1:        {activations[1].shape} = {np.prod(activations[1].shape[1:])} values\")\nprint(f\"   After Conv2:        {activations[2].shape} = {np.prod(activations[2].shape[1:])} values\")\nprint(f\"   After Pool2:        {activations[3].shape} = {np.prod(activations[3].shape[1:])} values\")\nprint(\"\\nüîç Observations:\")\nprint(\"   - Spatial dimensions decrease (28‚Üí26‚Üí13‚Üí11‚Üí5)\")\nprint(\"   - Number of features increases (1‚Üí16‚Üí32)\")\nprint(\"   - Simple patterns ‚Üí Complex patterns ‚Üí High-level features\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 5: Comparing Approaches\n",
    "\n",
    "## ‚öîÔ∏è Traditional ML vs CNN\n",
    "\n",
    "Let's compare the two approaches side-by-side on a simple task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison table\n",
    "comparison_data = {\n",
    "    'Aspect': ['Feature Design', 'Number of Features', 'Adaptability', \n",
    "               'Data Needed', 'Computational Cost', 'Explainability', \n",
    "               'Performance (Large Data)', 'Performance (Small Data)'],\n",
    "    'Traditional ML (Week 9)': [\n",
    "        'Humans design manually',\n",
    "        '5-25 features',\n",
    "        'Fixed for each problem',\n",
    "        'Works with 100s of samples',\n",
    "        'Low (no GPU needed)',\n",
    "        'High (we know what features mean)',\n",
    "        'Good (~85-90%)',\n",
    "        'Excellent'\n",
    "    ],\n",
    "    'CNN (Module 4)': [\n",
    "        'CNN learns automatically',\n",
    "        '10,000+ learned features',\n",
    "        'Adapts to each problem',\n",
    "        'Needs 1000s-10,000s samples',\n",
    "        'High (GPU recommended)',\n",
    "        'Low (black box)',\n",
    "        'Excellent (95-99%)',\n",
    "        'Poor (overfits)'\n",
    "    ]\n",
    "}\n",
    "\n",
    "import pandas as pd\n",
    "df_comparison = pd.DataFrame(comparison_data)\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"TRADITIONAL ML vs CNN: COMPREHENSIVE COMPARISON\")\n",
    "print(\"=\"*100 + \"\\n\")\n",
    "print(df_comparison.to_string(index=False))\n",
    "\n",
    "print(\"\\n\\nüéØ When to Use Which Approach?\\n\")\n",
    "print(\"   Use TRADITIONAL ML (Week 9) when:\")\n",
    "print(\"   ‚úÖ Small dataset (< 1,000 images)\")\n",
    "print(\"   ‚úÖ Need explainability (medical, legal applications)\")\n",
    "print(\"   ‚úÖ Limited computational resources (no GPU)\")\n",
    "print(\"   ‚úÖ Domain expertise available\")\n",
    "print(\"\\n   Use CNN (Module 4) when:\")\n",
    "print(\"   ‚úÖ Large dataset (> 10,000 images)\")\n",
    "print(\"   ‚úÖ Complex visual patterns\")\n",
    "print(\"   ‚úÖ Maximum accuracy required\")\n",
    "print(\"   ‚úÖ GPU available\")\n",
    "print(\"   ‚úÖ Patterns hard to describe manually\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual Comparison: Feature Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature comparison\n",
    "approaches = ['Manual\\nFeatures\\n(Week 9)', 'CNN Layer 1\\n(16 filters)', \n",
    "              'CNN Layer 2\\n(32 filters)', 'CNN Total\\n(All Layers)']\n",
    "feature_counts = [5, 16*3*3, 32*3*3, 38307]  # Approximate\n",
    "colors = ['#ff6b6b', '#4ecdc4', '#45b7d1', '#96ceb4']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "bars = ax.bar(approaches, feature_counts, color=colors, edgecolor='black', linewidth=2)\n",
    "\n",
    "ax.set_ylabel('Number of Learnable Parameters', fontweight='bold', fontsize=12)\n",
    "ax.set_title('Feature Complexity: Manual vs CNN', fontweight='bold', fontsize=14)\n",
    "ax.set_yscale('log')\n",
    "ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "\n",
    "# Add value labels\n",
    "for bar, count in zip(bars, feature_counts):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "           f'{count:,}',\n",
    "           ha='center', va='bottom', fontweight='bold', fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Feature Complexity Analysis:\")\n",
    "print(f\"   Manual features:        {feature_counts[0]} parameters (we designed)\")\n",
    "print(f\"   CNN total:              {feature_counts[3]:,} parameters (CNN learns)\")\n",
    "print(f\"   Increase:               {feature_counts[3] // feature_counts[0]:,}√ó more features!\")\n",
    "print(\"\\n‚úÖ CNNs can capture FAR more complex patterns than we can design manually!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üéì Summary and Key Takeaways\n",
    "\n",
    "## What We Learned Today\n",
    "\n",
    "### 1. The Problem with Manual Features\n",
    "- ‚ùå Limited by human imagination\n",
    "- ‚ùå Time-consuming to design\n",
    "- ‚ùå Not scalable to complex problems\n",
    "- ‚úÖ Good for small datasets\n",
    "\n",
    "### 2. Convolution - Pattern Detection\n",
    "- üîç Like Detective Maria sliding a stamp\n",
    "- üìÆ Filter = Reference pattern (3√ó3 numbers)\n",
    "- üîÑ Slide across image, compute similarity\n",
    "- üó∫Ô∏è Output = Feature map (where patterns found)\n",
    "- üéØ Multiple filters = Multiple pattern detectors\n",
    "\n",
    "### 3. Pooling - Intelligent Compression\n",
    "- üèÜ Keep only strongest signals (trophy cabinet)\n",
    "- üìâ Reduce size by 50-75%\n",
    "- ‚úÖ Preserve important information\n",
    "- ‚ö° Faster computation\n",
    "\n",
    "### 4. Complete CNN Architecture\n",
    "- üè≠ Assembly line: Input ‚Üí Conv ‚Üí Pool ‚Üí Conv ‚Üí Pool ‚Üí Dense ‚Üí Output\n",
    "- üìä Hierarchy: Simple features ‚Üí Complex features ‚Üí Objects\n",
    "- ü§ñ Learns ALL features automatically from data\n",
    "- üéØ End-to-end learning (pixels ‚Üí predictions)\n",
    "\n",
    "### 5. When to Use What\n",
    "- üìö Small data (< 1K): Manual features\n",
    "- üóÉÔ∏è Large data (> 10K): CNN\n",
    "- üí° Need explainability: Manual features\n",
    "- üéØ Maximum accuracy: CNN\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Next Steps\n",
    "\n",
    "### Week 10 (Coming Soon):\n",
    "1. **Deep dive into CNN mathematics**\n",
    "2. **Backpropagation in CNNs** (how filters learn)\n",
    "3. **Tutorial T10:** Build and train your first CNN on CIFAR-10!\n",
    "4. **Famous architectures:** LeNet, AlexNet preview\n",
    "\n",
    "### Homework for You:\n",
    "1. **Run this notebook** and experiment with different filters\n",
    "2. **Try different pooling sizes** (2√ó2 vs 3√ó3)\n",
    "3. **Modify the CNN architecture** (add more layers, change filter counts)\n",
    "4. **Compare manual features** you extracted in Week 9 with CNN learned features\n",
    "\n",
    "### Practice Exercise:\n",
    "1. Create your own 7√ó7 test image with different patterns\n",
    "2. Design a filter to detect that specific pattern\n",
    "3. Run convolution and verify it detects correctly\n",
    "4. Share your findings in the next class!\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Additional Resources\n",
    "\n",
    "**Interactive Demos:**\n",
    "- CNN Explainer: https://poloclub.github.io/cnn-explainer/\n",
    "- Setosa.io Convolution: https://setosa.io/ev/image-kernels/\n",
    "\n",
    "**Videos:**\n",
    "- 3Blue1Brown: \"But what IS a convolution?\"\n",
    "- Stanford CS231n Lecture 5\n",
    "\n",
    "**Reading:**\n",
    "- Chollet, \"Deep Learning with Python\" - Chapter 5\n",
    "- Goodfellow et al., \"Deep Learning\" - Chapter 9\n",
    "\n",
    "---\n",
    "\n",
    "**Notebook Prepared by:** Professor Ramesh Babu  \n",
    "**Course:** 21CSE558T - Deep Neural Network Architectures  \n",
    "**Department:** School of Computing, SRM University  \n",
    "**Version:** 1.0 (October 2025)\n",
    "\n",
    "---\n",
    "\n",
    "## üé§ Final Thought\n",
    "\n",
    "> *\"Manual feature extraction is like teaching someone to fish by describing every movement. CNNs are like letting them watch 10,000 fishermen and figure it out themselves!\"*\n",
    "\n",
    "**Welcome to the world of automatic feature learning! üéâ**\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}