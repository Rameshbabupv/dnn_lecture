{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 21CSE558T - Deep Neural Network Architectures\n",
    "## Module 5: Object Detection\n",
    "### Notebook 01: Object Detection Introduction - Understanding the Problem\n",
    "\n",
    "---\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Understand the difference between classification, localization, and detection\n",
    "- Recognize real-world applications of object detection\n",
    "- Identify challenges in object detection tasks\n",
    "- Learn the input/output format for detection systems\n",
    "\n",
    "**Estimated Time:** 10 minutes\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Setup and Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print(\"Environment setup complete!\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Matplotlib version: {plt.matplotlib.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Object Detection?\n",
    "\n",
    "Object detection is a computer vision task that combines **classification** and **localization**. Let's understand the progression:\n",
    "\n",
    "### The Three Computer Vision Tasks:\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  1. IMAGE CLASSIFICATION                                    â”‚\n",
    "â”‚     Question: \"What is this?\"                               â”‚\n",
    "â”‚     Input:  Image                                           â”‚\n",
    "â”‚     Output: Single label (e.g., \"Cat\")                      â”‚\n",
    "â”‚                                                             â”‚\n",
    "â”‚     [Image] â†’ Neural Network â†’ \"Cat\"                        â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  2. OBJECT LOCALIZATION                                     â”‚\n",
    "â”‚     Question: \"Where is THE object?\"                        â”‚\n",
    "â”‚     Input:  Image (with ONE primary object)                â”‚\n",
    "â”‚     Output: Label + Bounding Box (x, y, w, h)               â”‚\n",
    "â”‚                                                             â”‚\n",
    "â”‚     [Image] â†’ Neural Network â†’ \"Cat\" + [100, 50, 200, 150]  â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  3. OBJECT DETECTION                                        â”‚\n",
    "â”‚     Question: \"Where are ALL the objects?\"                  â”‚\n",
    "â”‚     Input:  Image (with MULTIPLE objects)                  â”‚\n",
    "â”‚     Output: List of [Label + Bounding Box + Confidence]     â”‚\n",
    "â”‚                                                             â”‚\n",
    "â”‚     [Image] â†’ Neural Network â†’ [                            â”‚\n",
    "â”‚         (\"Cat\", [100, 50, 200, 150], 0.95),                â”‚\n",
    "â”‚         (\"Dog\", [300, 100, 180, 200], 0.87),               â”‚\n",
    "â”‚         (\"Cat\", [450, 80, 150, 140], 0.92)                 â”‚\n",
    "â”‚     ]                                                       â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "**Key Differences:**\n",
    "- **Classification**: Answers \"what\" (no location)\n",
    "- **Localization**: Answers \"what\" + \"where\" (single object)\n",
    "- **Detection**: Answers \"what\" + \"where\" for ALL objects (multiple objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Classification Example\n",
    "# Create a simple synthetic image with a cat-like shape\n",
    "\n",
    "def create_cat_image():\n",
    "    \"\"\"Create a simple cat representation\"\"\"\n",
    "    img = np.ones((300, 400, 3), dtype=np.uint8) * 240  # Light background\n",
    "    \n",
    "    # Draw a simple cat silhouette (using rectangles and circles)\n",
    "    # Body\n",
    "    img[150:250, 150:300] = [255, 140, 0]  # Orange body\n",
    "    # Head\n",
    "    img[100:180, 180:270] = [255, 140, 0]  # Orange head\n",
    "    # Ears\n",
    "    img[80:120, 170:200] = [255, 140, 0]   # Left ear\n",
    "    img[80:120, 250:280] = [255, 140, 0]   # Right ear\n",
    "    # Eyes\n",
    "    img[120:135, 200:215] = [0, 0, 0]      # Left eye\n",
    "    img[120:135, 235:250] = [0, 0, 0]      # Right eye\n",
    "    \n",
    "    return img\n",
    "\n",
    "# Create and display\n",
    "cat_img = create_cat_image()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(cat_img)\n",
    "plt.axis('off')\n",
    "plt.title('IMAGE CLASSIFICATION EXAMPLE', fontsize=16, fontweight='bold')\n",
    "plt.text(200, 280, 'Output: \"Cat\"', \n",
    "         ha='center', va='center', \n",
    "         fontsize=20, fontweight='bold',\n",
    "         bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.8))\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Classification Output:\")\n",
    "print(\"  Label: Cat\")\n",
    "print(\"  (No location information)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Localization Example\n",
    "# Same image but with a bounding box around THE object\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "ax.imshow(cat_img)\n",
    "ax.axis('off')\n",
    "ax.set_title('OBJECT LOCALIZATION EXAMPLE', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Draw bounding box around the cat\n",
    "# Format: (x, y, width, height)\n",
    "bbox_x, bbox_y, bbox_w, bbox_h = 140, 75, 150, 180\n",
    "rect = patches.Rectangle((bbox_x, bbox_y), bbox_w, bbox_h, \n",
    "                         linewidth=3, edgecolor='red', facecolor='none')\n",
    "ax.add_patch(rect)\n",
    "\n",
    "# Add label\n",
    "ax.text(bbox_x + 5, bbox_y - 5, 'Cat', \n",
    "        fontsize=14, fontweight='bold', color='red',\n",
    "        bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Localization Output:\")\n",
    "print(f\"  Label: Cat\")\n",
    "print(f\"  Bounding Box: (x={bbox_x}, y={bbox_y}, w={bbox_w}, h={bbox_h})\")\n",
    "print(\"  (Single object with location)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Detection Example - Multiple Objects\n",
    "# Create an image with multiple objects\n",
    "\n",
    "def create_multi_object_image():\n",
    "    \"\"\"Create image with multiple cats and dogs\"\"\"\n",
    "    img = np.ones((400, 600, 3), dtype=np.uint8) * 240\n",
    "    \n",
    "    # Cat 1 (orange)\n",
    "    img[80:160, 50:150] = [255, 140, 0]\n",
    "    img[50:90, 60:100] = [255, 140, 0]  # head\n",
    "    img[60:70, 70:85] = [0, 0, 0]  # eye\n",
    "    \n",
    "    # Dog 1 (brown)\n",
    "    img[200:300, 100:220] = [139, 90, 43]\n",
    "    img[180:230, 130:190] = [139, 90, 43]  # head\n",
    "    img[190:200, 145:165] = [0, 0, 0]  # eye\n",
    "    \n",
    "    # Cat 2 (gray)\n",
    "    img[120:200, 350:450] = [128, 128, 128]\n",
    "    img[90:130, 370:430] = [128, 128, 128]  # head\n",
    "    img[100:110, 385:400] = [0, 0, 0]  # eye\n",
    "    \n",
    "    # Dog 2 (light brown)\n",
    "    img[250:340, 400:520] = [160, 120, 80]\n",
    "    img[230:270, 430:490] = [160, 120, 80]  # head\n",
    "    img[240:250, 450:465] = [0, 0, 0]  # eye\n",
    "    \n",
    "    return img\n",
    "\n",
    "multi_img = create_multi_object_image()\n",
    "\n",
    "# Detection results\n",
    "detections = [\n",
    "    {'label': 'Cat', 'bbox': (45, 45, 110, 120), 'confidence': 0.95, 'color': 'red'},\n",
    "    {'label': 'Dog', 'bbox': (95, 175, 130, 130), 'confidence': 0.87, 'color': 'blue'},\n",
    "    {'label': 'Cat', 'bbox': (345, 85, 110, 120), 'confidence': 0.92, 'color': 'green'},\n",
    "    {'label': 'Dog', 'bbox': (395, 225, 130, 120), 'confidence': 0.89, 'color': 'purple'}\n",
    "]\n",
    "\n",
    "# Display with all bounding boxes\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
    "ax.imshow(multi_img)\n",
    "ax.axis('off')\n",
    "ax.set_title('OBJECT DETECTION EXAMPLE - Multiple Objects', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Draw all bounding boxes\n",
    "for det in detections:\n",
    "    x, y, w, h = det['bbox']\n",
    "    rect = patches.Rectangle((x, y), w, h, \n",
    "                             linewidth=3, edgecolor=det['color'], facecolor='none')\n",
    "    ax.add_patch(rect)\n",
    "    \n",
    "    # Add label with confidence\n",
    "    label_text = f\"{det['label']} ({det['confidence']:.2f})\"\n",
    "    ax.text(x + 5, y - 5, label_text, \n",
    "            fontsize=12, fontweight='bold', color=det['color'],\n",
    "            bbox=dict(boxstyle='round', facecolor='white', alpha=0.9))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Object Detection Output:\")\n",
    "print(\"  List of detections:\")\n",
    "for i, det in enumerate(detections, 1):\n",
    "    print(f\"    {i}. Label: {det['label']}, BBox: {det['bbox']}, Confidence: {det['confidence']:.2f}\")\n",
    "print(\"\\n  (Multiple objects, each with label + location + confidence)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real-World Applications of Object Detection\n",
    "\n",
    "Object detection is a foundational technology powering many modern AI systems:\n",
    "\n",
    "### 1. **Autonomous Driving**\n",
    "- Detect pedestrians, vehicles, traffic signs, lane markings\n",
    "- Critical for safety: must work in real-time (30+ FPS)\n",
    "- Example: Tesla Autopilot, Waymo self-driving cars\n",
    "\n",
    "### 2. **Medical Imaging**\n",
    "- Detect tumors, lesions, abnormalities in X-rays, CT scans, MRIs\n",
    "- Assist radiologists in early disease detection\n",
    "- Example: Cancer detection, organ segmentation\n",
    "\n",
    "### 3. **Surveillance and Security**\n",
    "- Person detection, face recognition, suspicious behavior detection\n",
    "- Track multiple people across camera networks\n",
    "- Example: Airport security, retail loss prevention\n",
    "\n",
    "### 4. **Retail and E-Commerce**\n",
    "- Product recognition on shelves (inventory management)\n",
    "- Visual search (\"find products similar to this image\")\n",
    "- Amazon Go stores: detect items picked up by customers\n",
    "\n",
    "### 5. **Wildlife Monitoring**\n",
    "- Detect and count animals in camera trap images\n",
    "- Track endangered species populations\n",
    "- Example: Conservation efforts, poaching prevention\n",
    "\n",
    "### 6. **Agriculture**\n",
    "- Detect crop diseases, pests, weeds\n",
    "- Estimate crop yield by counting fruits/vegetables\n",
    "- Example: Precision agriculture, automated harvesting robots\n",
    "\n",
    "### 7. **Manufacturing Quality Control**\n",
    "- Detect defects in products on assembly lines\n",
    "- Real-time inspection at high speeds\n",
    "- Example: Electronics manufacturing, automotive parts inspection\n",
    "\n",
    "### 8. **Augmented Reality (AR)**\n",
    "- Detect objects to overlay virtual information\n",
    "- Example: Snapchat filters, IKEA furniture placement app\n",
    "\n",
    "---\n",
    "\n",
    "**Common Requirements:**\n",
    "- **Accuracy**: Must minimize false positives and false negatives\n",
    "- **Speed**: Many applications need real-time performance (30+ FPS)\n",
    "- **Robustness**: Must work under various lighting, angles, occlusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: The Challenge - Difficult Cases\n",
    "# Demonstrate challenging scenarios for object detection\n",
    "\n",
    "def create_challenge_images():\n",
    "    \"\"\"Create images demonstrating detection challenges\"\"\"\n",
    "    \n",
    "    # 1. Occlusion - objects hiding each other\n",
    "    occlusion = np.ones((200, 200, 3), dtype=np.uint8) * 240\n",
    "    occlusion[80:180, 50:120] = [255, 140, 0]  # Front cat (orange)\n",
    "    occlusion[100:180, 100:170] = [128, 128, 128]  # Back cat (gray, partially hidden)\n",
    "    \n",
    "    # 2. Small objects\n",
    "    small_obj = np.ones((200, 200, 3), dtype=np.uint8) * 240\n",
    "    small_obj[90:110, 90:110] = [255, 0, 0]  # Tiny object\n",
    "    small_obj[140:160, 140:160] = [0, 255, 0]  # Another tiny object\n",
    "    \n",
    "    # 3. Crowded scene\n",
    "    crowded = np.ones((200, 200, 3), dtype=np.uint8) * 240\n",
    "    # Multiple overlapping objects\n",
    "    crowded[50:100, 30:80] = [255, 0, 0]\n",
    "    crowded[60:110, 70:120] = [0, 255, 0]\n",
    "    crowded[70:120, 110:160] = [0, 0, 255]\n",
    "    crowded[110:160, 40:90] = [255, 255, 0]\n",
    "    crowded[120:170, 90:140] = [255, 0, 255]\n",
    "    \n",
    "    # 4. Different scales\n",
    "    scales = np.ones((200, 200, 3), dtype=np.uint8) * 240\n",
    "    scales[40:120, 30:110] = [255, 140, 0]  # Large object\n",
    "    scales[140:170, 140:170] = [128, 128, 128]  # Small object\n",
    "    \n",
    "    # 5. Low lighting (darker image)\n",
    "    low_light = np.ones((200, 200, 3), dtype=np.uint8) * 60  # Dark background\n",
    "    low_light[70:150, 60:140] = [100, 80, 60]  # Barely visible object\n",
    "    \n",
    "    # 6. Rotation/Different angles\n",
    "    rotation = np.ones((200, 200, 3), dtype=np.uint8) * 240\n",
    "    # Simulate rotated object (diagonal rectangle)\n",
    "    for i in range(50, 150):\n",
    "        for j in range(max(0, i-20), min(200, i+20)):\n",
    "            rotation[j, i] = [255, 140, 0]\n",
    "    \n",
    "    return occlusion, small_obj, crowded, scales, low_light, rotation\n",
    "\n",
    "# Create challenge images\n",
    "challenges = create_challenge_images()\n",
    "titles = [\n",
    "    '1. Occlusion\\n(Objects hiding each other)',\n",
    "    '2. Small Objects\\n(Hard to detect)',\n",
    "    '3. Crowded Scene\\n(Many overlapping objects)',\n",
    "    '4. Different Scales\\n(Near vs Far)',\n",
    "    '5. Low Lighting\\n(Poor visibility)',\n",
    "    '6. Rotation/Pose\\n(Different angles)'\n",
    "]\n",
    "\n",
    "# Display all challenges\n",
    "fig, axes = plt.subplots(2, 3, figsize=(14, 9))\n",
    "fig.suptitle('OBJECT DETECTION CHALLENGES', fontsize=18, fontweight='bold')\n",
    "\n",
    "for idx, (ax, img, title) in enumerate(zip(axes.flat, challenges, titles)):\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(title, fontsize=11, fontweight='bold')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Challenges in Object Detection:\")\n",
    "print(\"  1. Occlusion: Objects partially hidden by other objects\")\n",
    "print(\"  2. Scale Variation: Objects at different distances (sizes)\")\n",
    "print(\"  3. Crowded Scenes: Many objects close together\")\n",
    "print(\"  4. Small Objects: Difficult to detect with limited pixels\")\n",
    "print(\"  5. Lighting Conditions: Shadows, darkness, glare\")\n",
    "print(\"  6. Viewpoint/Pose: Objects at different angles/orientations\")\n",
    "print(\"\\nA robust detector must handle ALL these challenges!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input vs Output Format\n",
    "\n",
    "Understanding the data format is crucial for building object detection systems.\n",
    "\n",
    "### INPUT Format\n",
    "```python\n",
    "# Input: An image\n",
    "image = np.array(shape=(Height, Width, 3))  # RGB image\n",
    "# Example: (640, 480, 3) for a 640x480 pixel image\n",
    "```\n",
    "\n",
    "### OUTPUT Format\n",
    "```python\n",
    "# Output: List of detections\n",
    "detections = [\n",
    "    {\n",
    "        'class_name': 'cat',           # Object class (string)\n",
    "        'class_id': 0,                 # Class ID (integer)\n",
    "        'bbox': [x, y, width, height], # Bounding box coordinates\n",
    "        'confidence': 0.95             # Confidence score (0 to 1)\n",
    "    },\n",
    "    {\n",
    "        'class_name': 'dog',\n",
    "        'class_id': 1,\n",
    "        'bbox': [x2, y2, width2, height2],\n",
    "        'confidence': 0.87\n",
    "    },\n",
    "    # ... more detections\n",
    "]\n",
    "```\n",
    "\n",
    "### Bounding Box Coordinate Systems\n",
    "\n",
    "There are multiple ways to represent bounding boxes:\n",
    "\n",
    "1. **[x, y, width, height]** (XYWH format)\n",
    "   - (x, y): Top-left corner coordinates\n",
    "   - width, height: Box dimensions\n",
    "   - Example: [100, 50, 200, 150]\n",
    "\n",
    "2. **[x_min, y_min, x_max, y_max]** (XYXY format)\n",
    "   - (x_min, y_min): Top-left corner\n",
    "   - (x_max, y_max): Bottom-right corner\n",
    "   - Example: [100, 50, 300, 200]\n",
    "\n",
    "3. **[x_center, y_center, width, height]** (YOLO format)\n",
    "   - (x_center, y_center): Center of the box\n",
    "   - width, height: Box dimensions\n",
    "   - Often normalized to [0, 1] range\n",
    "\n",
    "### Example Output\n",
    "```python\n",
    "# Real detection output example\n",
    "output = [\n",
    "    {'class_name': 'person', 'bbox': [120, 80, 150, 320], 'confidence': 0.98},\n",
    "    {'class_name': 'car', 'bbox': [350, 200, 280, 180], 'confidence': 0.92},\n",
    "    {'class_name': 'bicycle', 'bbox': [50, 150, 100, 140], 'confidence': 0.85}\n",
    "]\n",
    "```\n",
    "\n",
    "### Confidence Score\n",
    "- Range: 0.0 to 1.0 (0% to 100%)\n",
    "- Threshold: Typically filter detections with confidence < 0.5\n",
    "- Higher confidence = more certain detection\n",
    "- Example: confidence > 0.9 â†’ very confident, confidence < 0.6 â†’ uncertain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preview of Solutions\n",
    "\n",
    "You might be wondering: **\"How do we actually build an object detector?\"**\n",
    "\n",
    "Great question! This week (Week 13) we'll learn the **foundations** and building blocks. In Weeks 14-15, we'll implement complete detection systems.\n",
    "\n",
    "---\n",
    "\n",
    "### Week 13: **Foundations** (This Week)\n",
    "Learn the core concepts needed for ANY object detection system:\n",
    "\n",
    "1. **Bounding Boxes** - How to represent and manipulate boxes\n",
    "2. **IoU (Intersection over Union)** - Measuring box overlap (critical metric)\n",
    "3. **Non-Maximum Suppression (NMS)** - Removing duplicate detections\n",
    "4. **Anchor Boxes** - Pre-defined box templates\n",
    "5. **Evaluation Metrics** - mAP, precision, recall\n",
    "\n",
    "**Think of it as:** Learning the alphabet before writing essays\n",
    "\n",
    "---\n",
    "\n",
    "### Week 14: **YOLO (You Only Look Once)**\n",
    "**Approach:** Single-shot detector (fast and real-time)\n",
    "\n",
    "**How it works:**\n",
    "- Divide image into grid (e.g., 13Ã—13)\n",
    "- Each grid cell predicts bounding boxes + class probabilities\n",
    "- Single pass through neural network â†’ very fast (45+ FPS)\n",
    "\n",
    "**Strengths:**\n",
    "- âš¡ Real-time speed (great for video, autonomous driving)\n",
    "- Simple architecture, end-to-end training\n",
    "\n",
    "**Weaknesses:**\n",
    "- Struggles with small objects\n",
    "- Lower accuracy than two-stage methods\n",
    "\n",
    "**Versions:** YOLO v1, v2, v3, v4, v5, v8 (constantly improving)\n",
    "\n",
    "---\n",
    "\n",
    "### Week 15: **R-CNN Family (Region-based CNN)**\n",
    "**Approach:** Two-stage detector (accurate but slower)\n",
    "\n",
    "**How it works:**\n",
    "- **Stage 1:** Generate region proposals (where objects might be)\n",
    "- **Stage 2:** Classify each region + refine bounding box\n",
    "\n",
    "**Evolution:**\n",
    "1. **R-CNN** (2014): Slow but groundbreaking\n",
    "2. **Fast R-CNN** (2015): Faster training and inference\n",
    "3. **Faster R-CNN** (2016): Real-time region proposals (RPN)\n",
    "4. **Mask R-CNN** (2017): Adds instance segmentation (pixel-level masks)\n",
    "\n",
    "**Strengths:**\n",
    "- ğŸ¯ High accuracy (better than YOLO on small objects)\n",
    "- Precise bounding boxes\n",
    "\n",
    "**Weaknesses:**\n",
    "- Slower than YOLO (not suitable for real-time in some cases)\n",
    "- More complex architecture\n",
    "\n",
    "---\n",
    "\n",
    "### Comparison Table\n",
    "\n",
    "| Aspect | YOLO (Week 14) | R-CNN Family (Week 15) |\n",
    "|--------|---------------|------------------------|\n",
    "| **Speed** | Very Fast (45+ FPS) | Moderate (5-15 FPS) |\n",
    "| **Accuracy** | Good | Excellent |\n",
    "| **Stages** | Single-stage | Two-stage |\n",
    "| **Small Objects** | Struggles | Handles well |\n",
    "| **Use Case** | Real-time apps | High-accuracy needs |\n",
    "| **Complexity** | Simpler | More complex |\n",
    "\n",
    "---\n",
    "\n",
    "### Modern Trends (Beyond this course)\n",
    "- **EfficientDet**: Balances speed and accuracy\n",
    "- **Vision Transformers (ViT)**: Attention-based detectors (DETR)\n",
    "- **Mobile Detectors**: MobileNet-SSD, Tiny YOLO (for phones/edge devices)\n",
    "\n",
    "---\n",
    "\n",
    "**This Week's Goal:** Master the foundational concepts so you can understand and implement BOTH YOLO and R-CNN confidently!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Test your understanding with these questions:\n",
    "\n",
    "---\n",
    "\n",
    "### TODO Exercise 1: Task Classification\n",
    "\n",
    "**Scenario:** You have an image containing 3 cats and 2 dogs. You want to identify ALL animals and their locations.\n",
    "\n",
    "**Question:** Is this a:\n",
    "- A) Classification task?\n",
    "- B) Localization task?\n",
    "- C) Detection task?\n",
    "\n",
    "**Your Answer:** ___ (Write A, B, or C)\n",
    "\n",
    "**Explanation:** (Write 1-2 sentences explaining why)\n",
    "\n",
    "```\n",
    "YOUR ANSWER HERE:\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### TODO Exercise 2: Real-World Applications\n",
    "\n",
    "**Question:** List 3 real-world applications where object detection is **critical** (not just helpful, but essential).\n",
    "\n",
    "For each application, explain in 1 sentence WHY object detection is critical.\n",
    "\n",
    "**Your Answers:**\n",
    "```\n",
    "1. Application: _____________\n",
    "   Why critical: _____________\n",
    "\n",
    "2. Application: _____________\n",
    "   Why critical: _____________\n",
    "\n",
    "3. Application: _____________\n",
    "   Why critical: _____________\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### TODO Exercise 3: Challenges Analysis\n",
    "\n",
    "**Question:** What makes object detection harder than simple image classification?\n",
    "\n",
    "List at least 3 specific reasons with brief explanations.\n",
    "\n",
    "**Your Answers:**\n",
    "```\n",
    "1. _____________\n",
    "\n",
    "2. _____________\n",
    "\n",
    "3. _____________\n",
    "\n",
    "(Optional) 4. _____________\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### TODO Exercise 4: Output Format\n",
    "\n",
    "**Question:** Given this detection output:\n",
    "```python\n",
    "detection = {\n",
    "    'class_name': 'person',\n",
    "    'bbox': [100, 50, 200, 300],\n",
    "    'confidence': 0.87\n",
    "}\n",
    "```\n",
    "\n",
    "**Questions:**\n",
    "- a) What are the coordinates of the top-left corner of the bounding box?\n",
    "- b) What is the width and height of the bounding box?\n",
    "- c) How confident is the model about this detection (in percentage)?\n",
    "- d) If we use a confidence threshold of 0.9, will this detection be kept or filtered out?\n",
    "\n",
    "**Your Answers:**\n",
    "```\n",
    "a) Top-left corner: _____________\n",
    "\n",
    "b) Width: _____, Height: _____\n",
    "\n",
    "c) Confidence: _____%\n",
    "\n",
    "d) Kept or filtered? _____ (explain why)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Check your answers with your instructor or peers before moving to the next notebook!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Congratulations! You've completed the introduction to object detection.\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Three Computer Vision Tasks:**\n",
    "   - Classification: \"What is this?\" â†’ Single label\n",
    "   - Localization: \"Where is THE object?\" â†’ Label + 1 bounding box\n",
    "   - Detection: \"Where are ALL objects?\" â†’ Multiple [label + box + confidence]\n",
    "\n",
    "2. **Real-World Impact:**\n",
    "   - Object detection powers autonomous vehicles, medical diagnosis, security systems, retail automation, and many more applications\n",
    "   - Requires both accuracy and speed for practical deployment\n",
    "\n",
    "3. **Key Challenges:**\n",
    "   - Occlusion (objects hiding each other)\n",
    "   - Scale variation (small and large objects)\n",
    "   - Crowded scenes with overlapping objects\n",
    "   - Different lighting and viewing angles\n",
    "\n",
    "4. **Input/Output Format:**\n",
    "   - Input: Image (H Ã— W Ã— 3)\n",
    "   - Output: List of [class, bounding box, confidence score]\n",
    "   - Multiple coordinate systems: XYWH, XYXY, center-based\n",
    "\n",
    "5. **Learning Path:**\n",
    "   - Week 13: Foundations (IoU, NMS, anchors, metrics) â† You are here\n",
    "   - Week 14: YOLO (fast, single-stage detector)\n",
    "   - Week 15: R-CNN (accurate, two-stage detector)\n",
    "\n",
    "---\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "**Next Notebook:** `02_iou_calculation.ipynb`\n",
    "\n",
    "You'll learn about **IoU (Intersection over Union)**, the most important metric for:\n",
    "- Measuring how well a predicted box matches the ground truth\n",
    "- Deciding which detections to keep (Non-Maximum Suppression)\n",
    "- Evaluating detector performance (mAP calculation)\n",
    "\n",
    "**Preview:** We'll implement IoU from scratch and visualize box overlaps!\n",
    "\n",
    "---\n",
    "\n",
    "### Before You Go\n",
    "\n",
    "âœ… Make sure you understand the difference between classification, localization, and detection  \n",
    "âœ… Complete all TODO exercises above  \n",
    "âœ… Review the bounding box coordinate formats  \n",
    "âœ… Think about a real-world application you'd like to build  \n",
    "\n",
    "---\n",
    "\n",
    "**Questions?** Ask your instructor or discuss with peers in the next class!\n",
    "\n",
    "**Ready?** Move on to Notebook 02: IoU Calculation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
