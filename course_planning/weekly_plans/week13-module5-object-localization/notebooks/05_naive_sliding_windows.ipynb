{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 05: Naive Sliding Windows\n",
    "\n",
    "**Course:** Deep Neural Network Architectures (21CSE558T)  \n",
    "**Module 5:** Object Detection and Localization  \n",
    "**Week 13:** Object Localization Fundamentals  \n",
    "**Duration:** ~10 minutes\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this notebook, you will be able to:\n",
    "- Understand the classical sliding window approach to object detection\n",
    "- Implement a basic sliding window detector\n",
    "- Analyze computational limitations of this approach\n",
    "- Appreciate why modern methods (YOLO, R-CNN) are necessary\n",
    "- Understand the motivation for deep learning-based detection\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Before deep learning revolutionized computer vision (circa 2012), object detection relied on:\n",
    "\n",
    "### Classical Approach (2000-2012):\n",
    "1. **Sliding Window**: Move a fixed-size window across the image\n",
    "2. **Feature Extraction**: HOG (Histogram of Oriented Gradients), SIFT, etc.\n",
    "3. **Classification**: SVM, Random Forest on hand-crafted features\n",
    "4. **Multi-Scale**: Repeat at different image scales\n",
    "\n",
    "### Problems:\n",
    "- **Slow**: 1000s of windows per image\n",
    "- **Redundant computation**: Same features computed many times\n",
    "- **Fixed aspect ratio**: One window size per scale\n",
    "- **Not end-to-end**: Separate feature extraction and classification\n",
    "\n",
    "### Why Study This?\n",
    "- Historical context: Appreciate modern methods\n",
    "- Understand the evolution of object detection\n",
    "- Some concepts (multi-scale, window-based) still relevant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.patches import Rectangle\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import time\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure matplotlib\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"OpenCV version: {cv2.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The 2010 Approach: Before Deep Learning\n",
    "\n",
    "### How Object Detection Worked (Pre-2012)\n",
    "\n",
    "#### 1. Hand-Crafted Features\n",
    "Instead of learning features from data, researchers manually designed feature extractors:\n",
    "\n",
    "- **HOG (Histogram of Oriented Gradients)**:\n",
    "  - Count edge orientations in local regions\n",
    "  - Good for detecting pedestrians, vehicles\n",
    "  - Introduced in Dalal & Triggs (2005)\n",
    "\n",
    "- **SIFT (Scale-Invariant Feature Transform)**:\n",
    "  - Detect and describe local keypoints\n",
    "  - Robust to scale and rotation\n",
    "  - Lowe (2004)\n",
    "\n",
    "- **Haar Cascades**:\n",
    "  - Rectangle features for face detection\n",
    "  - Viola-Jones (2001)\n",
    "\n",
    "#### 2. Sliding Window\n",
    "- Move a fixed-size window across the entire image\n",
    "- Extract features from each window\n",
    "- Classify: \"Does this window contain an object?\"\n",
    "\n",
    "#### 3. Multi-Scale Pyramid\n",
    "- Create multiple resized versions of the image\n",
    "- Apply sliding window at each scale\n",
    "- Detect objects of different sizes\n",
    "\n",
    "#### 4. Classification\n",
    "- Train SVM (Support Vector Machine) or similar\n",
    "- Binary: object vs background\n",
    "- Or multi-class: car vs person vs dog vs background\n",
    "\n",
    "### Landmark Systems:\n",
    "- **Viola-Jones Face Detector (2001)**: First real-time detector\n",
    "- **Dalal-Triggs Pedestrian Detector (2005)**: HOG + SVM\n",
    "- **Deformable Part Models (2008)**: Won PASCAL VOC challenges\n",
    "\n",
    "### The Deep Learning Revolution (2012)\n",
    "- **AlexNet (2012)**: CNN beats hand-crafted features on ImageNet\n",
    "- **R-CNN (2014)**: First deep learning object detector\n",
    "- **YOLO (2016)**: Real-time detection with single forward pass\n",
    "- **Current**: YOLOv8, Faster R-CNN dominate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Simple Classifier\n",
    "\n",
    "For demonstration, we'll use a mock classifier instead of training a real one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MockClassifier:\n",
    "    \"\"\"\n",
    "    Mock classifier to simulate object detection.\n",
    "    In reality, this would be a trained SVM or neural network.\n",
    "    \"\"\"\n",
    "    def __init__(self, target_regions=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            target_regions: List of (x, y, width, height) where objects are located\n",
    "        \"\"\"\n",
    "        self.target_regions = target_regions or []\n",
    "    \n",
    "    def predict(self, window_x, window_y, window_size, image):\n",
    "        \"\"\"\n",
    "        Simulate classification of a window.\n",
    "        \n",
    "        Returns:\n",
    "            confidence: 0-1 score (higher if window overlaps target region)\n",
    "        \"\"\"\n",
    "        max_overlap = 0.0\n",
    "        \n",
    "        for target_x, target_y, target_w, target_h in self.target_regions:\n",
    "            # Calculate overlap with target region\n",
    "            x_overlap = max(0, min(window_x + window_size, target_x + target_w) - \n",
    "                          max(window_x, target_x))\n",
    "            y_overlap = max(0, min(window_y + window_size, target_y + target_h) - \n",
    "                          max(window_y, target_y))\n",
    "            \n",
    "            overlap_area = x_overlap * y_overlap\n",
    "            window_area = window_size * window_size\n",
    "            \n",
    "            if window_area > 0:\n",
    "                overlap_ratio = overlap_area / window_area\n",
    "                max_overlap = max(max_overlap, overlap_ratio)\n",
    "        \n",
    "        # Simulate confidence score with some noise\n",
    "        confidence = max_overlap * 0.9 + np.random.rand() * 0.1\n",
    "        return min(1.0, confidence)\n",
    "\n",
    "# Create a test image with objects\n",
    "image = np.ones((480, 640, 3), dtype=np.uint8) * 240  # Gray background\n",
    "\n",
    "# Add some \"objects\" (colored rectangles)\n",
    "objects = [\n",
    "    (100, 80, 120, 140),   # Red rectangle (x, y, w, h)\n",
    "    (400, 200, 150, 160),  # Blue rectangle\n",
    "]\n",
    "\n",
    "# Draw objects\n",
    "cv2.rectangle(image, (100, 80), (220, 220), (220, 50, 50), -1)  # Red\n",
    "cv2.circle(image, (475, 280), 75, (50, 50, 220), -1)           # Blue\n",
    "\n",
    "# Create classifier that knows where objects are\n",
    "classifier = MockClassifier(target_regions=objects)\n",
    "\n",
    "# Display image\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "plt.title(\"Test Image with Objects\", fontsize=14, fontweight='bold')\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Image size: {image.shape[1]} × {image.shape[0]} pixels\")\n",
    "print(f\"Number of objects: {len(objects)}\")\n",
    "print(\"\\nClassifier created (mock for demonstration)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sliding Window Function\n",
    "\n",
    "The core of the classical detection approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window(image, window_size, stride):\n",
    "    \"\"\"\n",
    "    Generate sliding windows across an image.\n",
    "    \n",
    "    Args:\n",
    "        image: Input image (H, W, C)\n",
    "        window_size: Size of the square window\n",
    "        stride: Step size for sliding (pixels)\n",
    "    \n",
    "    Yields:\n",
    "        (x, y, window): Position and extracted window\n",
    "    \"\"\"\n",
    "    height, width = image.shape[:2]\n",
    "    \n",
    "    for y in range(0, height - window_size + 1, stride):\n",
    "        for x in range(0, width - window_size + 1, stride):\n",
    "            # Extract window\n",
    "            window = image[y:y+window_size, x:x+window_size]\n",
    "            yield (x, y, window)\n",
    "\n",
    "def count_windows(image_width, image_height, window_size, stride):\n",
    "    \"\"\"\n",
    "    Count total number of windows for given parameters.\n",
    "    \"\"\"\n",
    "    num_x = (image_width - window_size) // stride + 1\n",
    "    num_y = (image_height - window_size) // stride + 1\n",
    "    return num_x * num_y\n",
    "\n",
    "# Test with different window sizes and strides\n",
    "window_size = 100\n",
    "stride = 50\n",
    "\n",
    "height, width = image.shape[:2]\n",
    "total_windows = count_windows(width, height, window_size, stride)\n",
    "\n",
    "print(\"Sliding Window Configuration:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Image size:    {width} × {height} pixels\")\n",
    "print(f\"Window size:   {window_size} × {window_size} pixels\")\n",
    "print(f\"Stride:        {stride} pixels\")\n",
    "print(f\"\\nTotal windows: {total_windows}\")\n",
    "\n",
    "# Show impact of different strides\n",
    "print(\"\\nEffect of Stride on Number of Windows:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Stride':<10} {'Windows':<15} {'Coverage':<15}\")\n",
    "print(\"=\" * 60)\n",
    "for s in [10, 20, 30, 50, 100]:\n",
    "    windows = count_windows(width, height, window_size, s)\n",
    "    coverage = \"Dense\" if s <= window_size/3 else \"Moderate\" if s <= window_size/2 else \"Sparse\"\n",
    "    print(f\"{s:<10} {windows:<15} {coverage:<15}\")\n",
    "\n",
    "print(\"\\nTrade-off:\")\n",
    "print(\"  - Smaller stride → More windows → Better coverage, slower\")\n",
    "print(\"  - Larger stride → Fewer windows → Faster, might miss objects\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Windows\n",
    "\n",
    "Let's see where all the windows are positioned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_sliding_windows(image, window_size, stride, max_windows=50):\n",
    "    \"\"\"\n",
    "    Visualize sliding window positions on the image.\n",
    "    \n",
    "    Args:\n",
    "        image: Input image\n",
    "        window_size: Window size\n",
    "        stride: Stride size\n",
    "        max_windows: Maximum windows to display (for clarity)\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    ax.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    \n",
    "    count = 0\n",
    "    total = 0\n",
    "    \n",
    "    for x, y, window in sliding_window(image, window_size, stride):\n",
    "        total += 1\n",
    "        if count < max_windows:\n",
    "            # Draw window\n",
    "            rect = Rectangle((x, y), window_size, window_size, \n",
    "                           linewidth=1, edgecolor='lime', facecolor='none', alpha=0.6)\n",
    "            ax.add_patch(rect)\n",
    "            count += 1\n",
    "    \n",
    "    ax.set_title(f\"Sliding Windows (showing {count} of {total} windows)\\n\" + \n",
    "                f\"Window: {window_size}×{window_size}, Stride: {stride}\",\n",
    "                fontsize=14, fontweight='bold')\n",
    "    ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return total\n",
    "\n",
    "# Visualize with moderate stride\n",
    "total = visualize_sliding_windows(image, window_size=100, stride=80, max_windows=50)\n",
    "print(f\"\\nTotal windows at this scale: {total}\")\n",
    "print(\"Note: Only first 50 windows shown for clarity\")\n",
    "\n",
    "# Compare different window sizes\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "configs = [(64, 60), (100, 80), (150, 120)]\n",
    "\n",
    "for ax, (ws, st) in zip(axes, configs):\n",
    "    ax.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    \n",
    "    count = 0\n",
    "    total = 0\n",
    "    for x, y, window in sliding_window(image, ws, st):\n",
    "        total += 1\n",
    "        if count < 30:  # Show fewer for clarity\n",
    "            rect = Rectangle((x, y), ws, ws, \n",
    "                           linewidth=1.5, edgecolor='lime', facecolor='none', alpha=0.6)\n",
    "            ax.add_patch(rect)\n",
    "            count += 1\n",
    "    \n",
    "    ax.set_title(f\"Window: {ws}×{ws}\\nStride: {st}\\nTotal: {total} windows\",\n",
    "                fontweight='bold')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle(\"Different Window Sizes\", fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nObservation: Larger windows cover objects better but need more scales\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computational Cost Analysis\n",
    "\n",
    "Let's calculate the computational cost of this approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_computational_cost(image_width, image_height, window_sizes, strides, num_scales=1):\n",
    "    \"\"\"\n",
    "    Analyze computational cost of sliding window detection.\n",
    "    \n",
    "    Args:\n",
    "        image_width, image_height: Image dimensions\n",
    "        window_sizes: List of window sizes\n",
    "        strides: List of strides (same length as window_sizes)\n",
    "        num_scales: Number of image scales (pyramid levels)\n",
    "    \n",
    "    Returns:\n",
    "        Total number of windows to process\n",
    "    \"\"\"\n",
    "    total_windows = 0\n",
    "    \n",
    "    print(\"Computational Cost Analysis:\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"{'Scale':<8} {'Size':<15} {'Window':<12} {'Stride':<10} {'Windows':<10}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    for scale_idx in range(num_scales):\n",
    "        scale_factor = 0.8 ** scale_idx  # Each scale is 80% of previous\n",
    "        scaled_width = int(image_width * scale_factor)\n",
    "        scaled_height = int(image_height * scale_factor)\n",
    "        \n",
    "        for window_size, stride in zip(window_sizes, strides):\n",
    "            if scaled_width < window_size or scaled_height < window_size:\n",
    "                continue\n",
    "            \n",
    "            windows = count_windows(scaled_width, scaled_height, window_size, stride)\n",
    "            total_windows += windows\n",
    "            \n",
    "            print(f\"{scale_idx+1:<8} {scaled_width}×{scaled_height:<9} \"\n",
    "                  f\"{window_size}×{window_size:<6} {stride:<10} {windows:<10}\")\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(f\"\\nTotal windows to process: {total_windows:,}\")\n",
    "    \n",
    "    # Estimate time (assuming 10ms per window for feature extraction + classification)\n",
    "    time_per_window_ms = 10\n",
    "    total_time_ms = total_windows * time_per_window_ms\n",
    "    total_time_sec = total_time_ms / 1000\n",
    "    \n",
    "    print(f\"\\nEstimated processing time:\")\n",
    "    print(f\"  Assuming {time_per_window_ms}ms per window\")\n",
    "    print(f\"  Total: {total_time_sec:.2f} seconds ({total_time_ms:,} ms)\")\n",
    "    print(f\"  FPS: {1/total_time_sec:.2f}\")\n",
    "    \n",
    "    return total_windows\n",
    "\n",
    "# Analyze for typical configuration\n",
    "window_sizes = [64, 100, 150]\n",
    "strides = [32, 50, 75]\n",
    "\n",
    "total = analyze_computational_cost(\n",
    "    image_width=640,\n",
    "    image_height=480,\n",
    "    window_sizes=window_sizes,\n",
    "    strides=strides,\n",
    "    num_scales=5  # 5 different image scales\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"\\nConclusion:\")\n",
    "print(\"  - Processing 1000s of windows per image is SLOW\")\n",
    "print(\"  - Not suitable for real-time applications\")\n",
    "print(\"  - Most windows are background (wasted computation)\")\n",
    "print(\"  - This motivated the development of faster methods\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Scale Pyramid\n",
    "\n",
    "To detect objects of different sizes, we need to process the image at multiple scales:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_image_pyramid(image, num_scales=5, scale_factor=0.8):\n",
    "    \"\"\"\n",
    "    Create an image pyramid for multi-scale detection.\n",
    "    \n",
    "    Args:\n",
    "        image: Original image\n",
    "        num_scales: Number of pyramid levels\n",
    "        scale_factor: Resize factor between levels (< 1.0)\n",
    "    \n",
    "    Returns:\n",
    "        List of scaled images\n",
    "    \"\"\"\n",
    "    pyramid = [image]\n",
    "    \n",
    "    for i in range(1, num_scales):\n",
    "        scale = scale_factor ** i\n",
    "        width = int(image.shape[1] * scale)\n",
    "        height = int(image.shape[0] * scale)\n",
    "        \n",
    "        if width < 50 or height < 50:  # Minimum size\n",
    "            break\n",
    "        \n",
    "        scaled = cv2.resize(image, (width, height))\n",
    "        pyramid.append(scaled)\n",
    "    \n",
    "    return pyramid\n",
    "\n",
    "# Create pyramid\n",
    "pyramid = create_image_pyramid(image, num_scales=5, scale_factor=0.75)\n",
    "\n",
    "# Visualize pyramid\n",
    "fig, axes = plt.subplots(1, len(pyramid), figsize=(18, 4))\n",
    "\n",
    "for idx, (ax, scaled_img) in enumerate(zip(axes, pyramid)):\n",
    "    ax.imshow(cv2.cvtColor(scaled_img, cv2.COLOR_BGR2RGB))\n",
    "    h, w = scaled_img.shape[:2]\n",
    "    scale = w / image.shape[1]\n",
    "    ax.set_title(f\"Scale {idx}\\n{w}×{h}\\n({scale:.2f}×)\", fontweight='bold')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle(\"Image Pyramid for Multi-Scale Detection\", fontsize=16, fontweight='bold', y=1.05)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nPyramid Statistics:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Level':<8} {'Size':<15} {'Scale Factor':<15} {'Pixels':<15}\")\n",
    "print(\"=\" * 60)\n",
    "for idx, scaled_img in enumerate(pyramid):\n",
    "    h, w = scaled_img.shape[:2]\n",
    "    scale = w / image.shape[1]\n",
    "    pixels = h * w\n",
    "    print(f\"{idx:<8} {w}×{h:<9} {scale:<15.3f} {pixels:,}\")\n",
    "\n",
    "total_pixels = sum(img.shape[0] * img.shape[1] for img in pyramid)\n",
    "original_pixels = image.shape[0] * image.shape[1]\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total pixels to process: {total_pixels:,} ({total_pixels/original_pixels:.1f}× original)\")\n",
    "\n",
    "print(\"\\nWhy Multiple Scales?\")\n",
    "print(\"  - Fixed window size (e.g., 100×100 pixels)\")\n",
    "print(\"  - Small objects: Need large image scale\")\n",
    "print(\"  - Large objects: Need small image scale\")\n",
    "print(\"  - Solution: Process image at multiple scales\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Detection\n",
    "\n",
    "Let's simulate the full detection pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sliding_window_detection(image, classifier, window_size=100, stride=50, \n",
    "                                 threshold=0.5, num_scales=3):\n",
    "    \"\"\"\n",
    "    Run complete sliding window detection pipeline.\n",
    "    \n",
    "    Args:\n",
    "        image: Input image\n",
    "        classifier: Classifier object with predict() method\n",
    "        window_size: Window size\n",
    "        stride: Stride size\n",
    "        threshold: Confidence threshold\n",
    "        num_scales: Number of pyramid scales\n",
    "    \n",
    "    Returns:\n",
    "        List of detections (x, y, size, confidence)\n",
    "    \"\"\"\n",
    "    detections = []\n",
    "    windows_processed = 0\n",
    "    \n",
    "    # Create image pyramid\n",
    "    pyramid = create_image_pyramid(image, num_scales=num_scales)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for scale_idx, scaled_img in enumerate(pyramid):\n",
    "        scale_factor = scaled_img.shape[1] / image.shape[1]\n",
    "        \n",
    "        # Skip if image too small for window\n",
    "        if scaled_img.shape[0] < window_size or scaled_img.shape[1] < window_size:\n",
    "            continue\n",
    "        \n",
    "        # Slide window\n",
    "        for x, y, window in sliding_window(scaled_img, window_size, stride):\n",
    "            windows_processed += 1\n",
    "            \n",
    "            # Classify window\n",
    "            confidence = classifier.predict(x, y, window_size, scaled_img)\n",
    "            \n",
    "            if confidence >= threshold:\n",
    "                # Convert coordinates back to original image scale\n",
    "                orig_x = int(x / scale_factor)\n",
    "                orig_y = int(y / scale_factor)\n",
    "                orig_size = int(window_size / scale_factor)\n",
    "                \n",
    "                detections.append({\n",
    "                    'x': orig_x,\n",
    "                    'y': orig_y,\n",
    "                    'size': orig_size,\n",
    "                    'confidence': confidence,\n",
    "                    'scale': scale_idx\n",
    "                })\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"Detection Complete:\")\n",
    "    print(f\"  Windows processed: {windows_processed:,}\")\n",
    "    print(f\"  Detections found: {len(detections)}\")\n",
    "    print(f\"  Time elapsed: {elapsed_time:.3f} seconds\")\n",
    "    print(f\"  FPS: {1/elapsed_time:.2f}\")\n",
    "    \n",
    "    return detections\n",
    "\n",
    "# Run detection\n",
    "print(\"Running Sliding Window Detection...\")\n",
    "print(\"=\" * 60)\n",
    "detections = run_sliding_window_detection(\n",
    "    image, classifier, \n",
    "    window_size=100, \n",
    "    stride=30,  # Small stride for better coverage\n",
    "    threshold=0.6,\n",
    "    num_scales=3\n",
    ")\n",
    "\n",
    "# Visualize detections\n",
    "result_img = image.copy()\n",
    "\n",
    "for det in detections:\n",
    "    x, y, size, conf = det['x'], det['y'], det['size'], det['confidence']\n",
    "    \n",
    "    # Color based on confidence\n",
    "    color = (0, int(255 * conf), int(255 * (1 - conf)))\n",
    "    \n",
    "    cv2.rectangle(result_img, (x, y), (x + size, y + size), color, 2)\n",
    "    cv2.putText(result_img, f\"{conf:.2f}\", (x, y - 5),\n",
    "               cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(cv2.cvtColor(result_img, cv2.COLOR_BGR2RGB))\n",
    "plt.title(f\"Detections: {len(detections)} boxes found\\n(Green = high confidence, Red = low confidence)\",\n",
    "         fontsize=14, fontweight='bold')\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop Detections:\")\n",
    "print(\"=\" * 60)\n",
    "sorted_detections = sorted(detections, key=lambda x: x['confidence'], reverse=True)\n",
    "for i, det in enumerate(sorted_detections[:5], 1):\n",
    "    print(f\"{i}. Position: ({det['x']}, {det['y']})  \"\n",
    "          f\"Size: {det['size']}×{det['size']}  \"\n",
    "          f\"Confidence: {det['confidence']:.3f}  \"\n",
    "          f\"Scale: {det['scale']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems Summary\n",
    "\n",
    "### Why Sliding Windows Don't Scale:\n",
    "\n",
    "#### 1. Computational Inefficiency\n",
    "```\n",
    "Typical configuration:\n",
    "  - Image: 640×480\n",
    "  - Window sizes: 3 (64, 100, 150)\n",
    "  - Scales: 5\n",
    "  - Total windows: ~2000-3000\n",
    "  - Processing time: 10-20 seconds per image\n",
    "  - FPS: 0.05-0.1 (NOT real-time!)\n",
    "```\n",
    "\n",
    "#### 2. Redundant Computation\n",
    "- **Problem**: Extract features from overlapping windows independently\n",
    "- **Waste**: Same pixels processed many times\n",
    "- **Example**: 50% overlap → 2× redundant computation\n",
    "\n",
    "#### 3. Fixed Aspect Ratios\n",
    "- **Problem**: Square windows (100×100)\n",
    "- **Reality**: Objects have various aspect ratios\n",
    "  - Pedestrian: 1:3 (tall and narrow)\n",
    "  - Car: 3:1 (wide and short)\n",
    "- **Solution needed**: Multiple aspect ratios → even more windows!\n",
    "\n",
    "#### 4. Not End-to-End\n",
    "- **Classical**: Hand-crafted features → Separate classifier\n",
    "- **Deep Learning**: Learn features and classification together\n",
    "- **Result**: Better accuracy with learned features\n",
    "\n",
    "#### 5. Poor Localization\n",
    "- **Problem**: Discrete grid of windows\n",
    "- **Limitation**: Bounding box positions quantized to stride\n",
    "- **Example**: Stride=50 → Can't detect object at position 25\n",
    "\n",
    "### Performance Comparison\n",
    "\n",
    "| Method | Speed (FPS) | mAP | Year |\n",
    "|--------|-------------|-----|------|\n",
    "| Sliding Window + HOG | 0.1 | 0.30 | 2005 |\n",
    "| Deformable Parts | 0.07 | 0.33 | 2008 |\n",
    "| R-CNN | 0.05 | 0.54 | 2014 |\n",
    "| Fast R-CNN | 0.5 | 0.70 | 2015 |\n",
    "| Faster R-CNN | 7 | 0.73 | 2015 |\n",
    "| YOLO v1 | 45 | 0.63 | 2016 |\n",
    "| YOLO v8 | **60+** | **0.53** | 2023 |\n",
    "\n",
    "### Key Insight:\n",
    "Modern methods (YOLO, R-CNN) are **100-1000× faster** while achieving **2-3× better accuracy**!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Solution: Modern Deep Learning Methods\n",
    "\n",
    "### How Modern Detectors Solve These Problems:\n",
    "\n",
    "#### 1. Region Proposal Networks (Faster R-CNN)\n",
    "**Idea**: Instead of exhaustive search, predict where objects might be\n",
    "- **Input**: Entire image\n",
    "- **Output**: ~300 candidate regions (not 3000 windows!)\n",
    "- **How**: Convolutional network predicts \"objectness\" at each location\n",
    "- **Result**: 10× fewer regions to classify\n",
    "\n",
    "#### 2. Single Shot Detection (YOLO, SSD)\n",
    "**Idea**: Predict bounding boxes + classes in one forward pass\n",
    "- **Input**: Image → Single CNN → Output: All detections\n",
    "- **No sliding window**: Grid-based prediction\n",
    "- **No pyramid**: Multi-scale feature maps\n",
    "- **Result**: Real-time (30-60 FPS)\n",
    "\n",
    "#### 3. Feature Pyramid Networks (FPN)\n",
    "**Idea**: Build multi-scale features inside the network\n",
    "- **Classical**: Resize image multiple times → Process separately\n",
    "- **Modern**: Single image → Multi-scale feature maps in CNN\n",
    "- **Benefit**: Share computation across scales\n",
    "\n",
    "#### 4. End-to-End Learning\n",
    "**Idea**: Learn features, proposals, and classification together\n",
    "- **Classical**: HOG (fixed) → SVM (trained)\n",
    "- **Modern**: All weights learned from data\n",
    "- **Result**: Features optimized for detection task\n",
    "\n",
    "### Architecture Comparison:\n",
    "\n",
    "```\n",
    "Classical Sliding Window:\n",
    "  Image → Resize (5×) → Slide Window (2000×) → HOG → SVM → Detections\n",
    "  Time: 10-20 seconds\n",
    "\n",
    "YOLO:\n",
    "  Image → CNN → Predictions (grid) → NMS → Detections\n",
    "  Time: 0.02 seconds (50 FPS)\n",
    "\n",
    "Faster R-CNN:\n",
    "  Image → CNN → Region Proposals (300) → Classify → Detections\n",
    "  Time: 0.1 seconds (10 FPS)\n",
    "```\n",
    "\n",
    "### Key Innovations:\n",
    "\n",
    "1. **YOLO (You Only Look Once)**:\n",
    "   - Divide image into grid (e.g., 7×7)\n",
    "   - Each cell predicts bounding boxes + class probabilities\n",
    "   - Single CNN forward pass → All detections\n",
    "\n",
    "2. **R-CNN Family**:\n",
    "   - R-CNN: Selective Search + CNN features\n",
    "   - Fast R-CNN: ROI pooling for speed\n",
    "   - Faster R-CNN: Replace Selective Search with RPN\n",
    "\n",
    "3. **Feature Reuse**:\n",
    "   - Compute features once for entire image\n",
    "   - Extract region features from shared feature map\n",
    "   - No redundant computation!\n",
    "\n",
    "### What We'll Learn Next:\n",
    "\n",
    "**Week 14: YOLO**\n",
    "- Grid-based prediction\n",
    "- Anchor boxes\n",
    "- Loss function design\n",
    "- Real-time detection\n",
    "\n",
    "**Week 15: R-CNN Family**\n",
    "- Region Proposal Networks\n",
    "- ROI pooling\n",
    "- Two-stage detection\n",
    "- Higher accuracy applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Calculate Windows Needed\n",
    "\n",
    "Test your understanding of computational costs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"EXERCISE: Sliding Window Calculations\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n1. Calculate number of windows for:\")\n",
    "print(\"   - Image: 800×600\")\n",
    "print(\"   - Window: 100×100\")\n",
    "print(\"   - Stride: 25\")\n",
    "print(\"   - Single scale\")\n",
    "\n",
    "# Solution:\n",
    "# windows = count_windows(800, 600, 100, 25)\n",
    "# print(f\"\\n   Answer: {windows} windows\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"\\n2. How many windows for multi-scale?\")\n",
    "print(\"   - Same configuration as above\")\n",
    "print(\"   - 4 scales: 1.0×, 0.8×, 0.6×, 0.4×\")\n",
    "print(\"   - Assume stride scales proportionally\")\n",
    "\n",
    "# Solution:\n",
    "# scales = [1.0, 0.8, 0.6, 0.4]\n",
    "# total = 0\n",
    "# for scale in scales:\n",
    "#     w = int(800 * scale)\n",
    "#     h = int(600 * scale)\n",
    "#     s = int(25 * scale)\n",
    "#     total += count_windows(w, h, 100, s)\n",
    "# print(f\"\\n   Answer: {total} windows\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"\\n3. Processing time estimate:\")\n",
    "print(\"   - Use your answer from question 2\")\n",
    "print(\"   - Assume 5ms per window (feature extraction + classification)\")\n",
    "print(\"   - Calculate: Total time in seconds, FPS\")\n",
    "\n",
    "# Solution:\n",
    "# time_ms = total * 5\n",
    "# time_sec = time_ms / 1000\n",
    "# fps = 1 / time_sec\n",
    "# print(f\"\\n   Answer: {time_sec:.2f} seconds, {fps:.3f} FPS\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"\\n4. Compare with YOLO:\")\n",
    "print(\"   - YOLO processes same image in 20ms\")\n",
    "print(\"   - How much faster is YOLO?\")\n",
    "\n",
    "# Solution:\n",
    "# speedup = time_ms / 20\n",
    "# print(f\"\\n   Answer: YOLO is {speedup:.1f}× faster\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"\\nUncomment the solution code to see answers!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary + Motivation for Week 14\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "1. **Classical Sliding Window Approach (2000-2012)**:\n",
    "   - Exhaustive search: Slide window across image\n",
    "   - Hand-crafted features: HOG, SIFT, Haar\n",
    "   - Multi-scale pyramid: Process image at different sizes\n",
    "   - Separate classification: SVM on extracted features\n",
    "\n",
    "2. **Computational Cost**:\n",
    "   - 2000-5000 windows per image\n",
    "   - 10-20 seconds processing time\n",
    "   - 0.05-0.1 FPS (NOT real-time)\n",
    "   - Most computation wasted on background\n",
    "\n",
    "3. **Fundamental Problems**:\n",
    "   - Too slow for real-time applications\n",
    "   - Redundant computation (overlapping windows)\n",
    "   - Fixed aspect ratios (square windows)\n",
    "   - Not end-to-end (separate feature extraction)\n",
    "   - Poor localization (discrete grid)\n",
    "\n",
    "4. **The Deep Learning Revolution**:\n",
    "   - R-CNN (2014): Region proposals + CNN features\n",
    "   - YOLO (2016): Single-shot detection, real-time\n",
    "   - Modern: 100-1000× faster, 2-3× more accurate\n",
    "\n",
    "### Why This Matters\n",
    "\n",
    "Understanding the classical approach helps you appreciate modern methods:\n",
    "\n",
    "- **YOLO's Innovation**: Grid prediction instead of sliding windows\n",
    "- **Faster R-CNN**: Learned region proposals instead of exhaustive search\n",
    "- **FPN**: Multi-scale features without image pyramids\n",
    "- **End-to-End**: Learn features optimized for detection\n",
    "\n",
    "### Historical Timeline\n",
    "\n",
    "```\n",
    "2001: Viola-Jones face detector (first real-time)\n",
    "2005: HOG pedestrian detector\n",
    "2008: Deformable Part Models (won PASCAL VOC)\n",
    "2012: AlexNet (deep learning revolution)\n",
    "2014: R-CNN (first deep learning detector)\n",
    "2015: Fast R-CNN, Faster R-CNN\n",
    "2016: YOLO, SSD (real-time detection)\n",
    "2017: RetinaNet, Feature Pyramid Networks\n",
    "2020: EfficientDet, DETR\n",
    "2023: YOLOv8, RT-DETR\n",
    "```\n",
    "\n",
    "### Next Steps: Week 14-15\n",
    "\n",
    "**Week 14: YOLO Architecture**\n",
    "- How YOLO achieves real-time detection\n",
    "- Grid-based prediction mechanism\n",
    "- Anchor boxes and multi-scale detection\n",
    "- Loss function design\n",
    "- Hands-on: Implement YOLO detector\n",
    "\n",
    "**Week 15: R-CNN Family**\n",
    "- Region Proposal Networks (RPN)\n",
    "- Two-stage detection pipeline\n",
    "- ROI pooling and alignment\n",
    "- When to use R-CNN vs YOLO\n",
    "- Hands-on: Fine-tune Faster R-CNN\n",
    "\n",
    "### Key Takeaway\n",
    "\n",
    "**The Problem**: Sliding windows are too slow and wasteful\n",
    "\n",
    "**The Solution**: \n",
    "- **YOLO**: Predict all boxes in single pass (speed)\n",
    "- **R-CNN**: Smart region proposals (accuracy)\n",
    "- **Both**: End-to-end learning, feature reuse, multi-scale\n",
    "\n",
    "**Result**: Real-time object detection with high accuracy!\n",
    "\n",
    "---\n",
    "\n",
    "**Completion Time:** ~10 minutes  \n",
    "**Next Notebook:** Week 14 - YOLO Architecture and Implementation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
