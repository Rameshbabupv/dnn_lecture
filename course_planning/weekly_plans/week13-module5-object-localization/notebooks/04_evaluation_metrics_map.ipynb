{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 04: Evaluation Metrics (mAP)\n",
    "\n",
    "**Course:** Deep Neural Network Architectures (21CSE558T)  \n",
    "**Module 5:** Object Detection and Localization  \n",
    "**Week 13:** Object Localization Fundamentals  \n",
    "**Duration:** ~15 minutes\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this notebook, you will be able to:\n",
    "- Calculate Precision and Recall for object detection\n",
    "- Understand and compute Average Precision (AP)\n",
    "- Calculate mean Average Precision (mAP)\n",
    "- Interpret mAP values for model comparison\n",
    "- Understand IoU-based detection evaluation\n",
    "\n",
    "## Introduction\n",
    "Unlike classification, object detection requires evaluating both:\n",
    "1. **Localization accuracy**: How well the box aligns with ground truth (IoU)\n",
    "2. **Classification accuracy**: Correct class prediction\n",
    "\n",
    "**mAP (mean Average Precision)** is the standard metric for object detection, used by:\n",
    "- PASCAL VOC Challenge\n",
    "- MS COCO Competition\n",
    "- All modern detection papers (YOLO, Faster R-CNN, SSD, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure matplotlib\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Makes a Good Detection?\n",
    "\n",
    "### IoU (Intersection over Union) Threshold\n",
    "\n",
    "A detection is considered **correct** (True Positive) if:\n",
    "1. **Class matches** ground truth\n",
    "2. **IoU ≥ threshold** (typically 0.5 for PASCAL VOC, 0.5-0.95 for COCO)\n",
    "\n",
    "```\n",
    "IoU = Area of Overlap / Area of Union\n",
    "```\n",
    "\n",
    "### Classification of Detections:\n",
    "\n",
    "| Term | Definition | Example |\n",
    "|------|------------|--------|\n",
    "| **True Positive (TP)** | Correct detection with IoU ≥ threshold | Detected car with IoU=0.75 |\n",
    "| **False Positive (FP)** | Wrong detection (wrong class or IoU < threshold) | Background detected as car, or IoU=0.3 |\n",
    "| **False Negative (FN)** | Missed ground truth object | Car in image but not detected |\n",
    "\n",
    "**Note:** True Negative (TN) is not defined in object detection (infinite background regions)\n",
    "\n",
    "### Confidence Threshold\n",
    "- Detectors output confidence scores (0-1)\n",
    "- By varying threshold, we get different precision-recall trade-offs\n",
    "- Higher threshold → fewer detections → higher precision, lower recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Detections\n",
    "\n",
    "Let's create synthetic predictions and ground truth for demonstration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ground truth objects (what's actually in the image)\n",
    "ground_truth = [\n",
    "    {'class': 'car', 'bbox': [100, 50, 250, 200]},\n",
    "    {'class': 'car', 'bbox': [300, 100, 450, 250]},\n",
    "    {'class': 'person', 'bbox': [50, 150, 120, 350]},\n",
    "    {'class': 'dog', 'bbox': [400, 300, 500, 400]},\n",
    "    {'class': 'person', 'bbox': [200, 250, 280, 400]}\n",
    "]\n",
    "\n",
    "# Model predictions (class, confidence, bbox)\n",
    "predictions = [\n",
    "    {'class': 'car', 'confidence': 0.95, 'bbox': [105, 55, 245, 195]},      # Good car detection\n",
    "    {'class': 'car', 'confidence': 0.88, 'bbox': [295, 105, 455, 245]},     # Good car detection\n",
    "    {'class': 'person', 'confidence': 0.92, 'bbox': [48, 155, 118, 345]},   # Good person detection\n",
    "    {'class': 'dog', 'confidence': 0.78, 'bbox': [405, 295, 495, 405]},     # Good dog detection\n",
    "    {'class': 'person', 'confidence': 0.85, 'bbox': [198, 255, 282, 395]},  # Good person detection\n",
    "    {'class': 'car', 'confidence': 0.65, 'bbox': [500, 50, 600, 150]},      # False positive (no GT)\n",
    "    {'class': 'person', 'confidence': 0.45, 'bbox': [150, 100, 200, 180]},  # False positive (low IoU)\n",
    "    {'class': 'dog', 'confidence': 0.35, 'bbox': [250, 350, 320, 420]},     # False positive (wrong class/loc)\n",
    "]\n",
    "\n",
    "print(\"Ground Truth Objects:\")\n",
    "print(\"=\" * 60)\n",
    "for i, gt in enumerate(ground_truth, 1):\n",
    "    print(f\"{i}. {gt['class']:<8} at {gt['bbox']}\")\n",
    "\n",
    "print(\"\\nModel Predictions:\")\n",
    "print(\"=\" * 60)\n",
    "for i, pred in enumerate(predictions, 1):\n",
    "    print(f\"{i}. {pred['class']:<8} Confidence: {pred['confidence']:.2f} at {pred['bbox']}\")\n",
    "\n",
    "print(f\"\\nTotal GT objects: {len(ground_truth)}\")\n",
    "print(f\"Total predictions: {len(predictions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification with IoU\n",
    "\n",
    "First, we need the IoU function from Notebook 02:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_iou(box1, box2):\n",
    "    \"\"\"\n",
    "    Calculate Intersection over Union (IoU) for two boxes.\n",
    "    \n",
    "    Args:\n",
    "        box1, box2: [x_min, y_min, x_max, y_max]\n",
    "    \n",
    "    Returns:\n",
    "        IoU value (0 to 1)\n",
    "    \"\"\"\n",
    "    # Calculate intersection\n",
    "    x_min_inter = max(box1[0], box2[0])\n",
    "    y_min_inter = max(box1[1], box2[1])\n",
    "    x_max_inter = min(box1[2], box2[2])\n",
    "    y_max_inter = min(box1[3], box2[3])\n",
    "    \n",
    "    # Check if boxes intersect\n",
    "    if x_max_inter < x_min_inter or y_max_inter < y_min_inter:\n",
    "        return 0.0\n",
    "    \n",
    "    intersection = (x_max_inter - x_min_inter) * (y_max_inter - y_min_inter)\n",
    "    \n",
    "    # Calculate union\n",
    "    area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "    area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "    union = area1 + area2 - intersection\n",
    "    \n",
    "    return intersection / union if union > 0 else 0.0\n",
    "\n",
    "def match_predictions_to_gt(predictions, ground_truth, iou_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Match predictions to ground truth and classify as TP or FP.\n",
    "    \n",
    "    Args:\n",
    "        predictions: List of prediction dicts\n",
    "        ground_truth: List of ground truth dicts\n",
    "        iou_threshold: IoU threshold for TP\n",
    "    \n",
    "    Returns:\n",
    "        List of classification results\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    matched_gt = set()  # Track which GT objects have been matched\n",
    "    \n",
    "    # Sort predictions by confidence (highest first)\n",
    "    sorted_preds = sorted(predictions, key=lambda x: x['confidence'], reverse=True)\n",
    "    \n",
    "    for pred in sorted_preds:\n",
    "        best_iou = 0\n",
    "        best_gt_idx = -1\n",
    "        \n",
    "        # Find best matching ground truth\n",
    "        for gt_idx, gt in enumerate(ground_truth):\n",
    "            # Skip if already matched or different class\n",
    "            if gt_idx in matched_gt or gt['class'] != pred['class']:\n",
    "                continue\n",
    "            \n",
    "            iou = calculate_iou(pred['bbox'], gt['bbox'])\n",
    "            if iou > best_iou:\n",
    "                best_iou = iou\n",
    "                best_gt_idx = gt_idx\n",
    "        \n",
    "        # Classify as TP or FP\n",
    "        if best_iou >= iou_threshold:\n",
    "            classification = 'TP'\n",
    "            matched_gt.add(best_gt_idx)\n",
    "        else:\n",
    "            classification = 'FP'\n",
    "        \n",
    "        results.append({\n",
    "            'class': pred['class'],\n",
    "            'confidence': pred['confidence'],\n",
    "            'classification': classification,\n",
    "            'iou': best_iou\n",
    "        })\n",
    "    \n",
    "    # Count false negatives (unmatched GT objects)\n",
    "    fn_count = len(ground_truth) - len(matched_gt)\n",
    "    \n",
    "    return results, fn_count\n",
    "\n",
    "# Match predictions to ground truth\n",
    "results, fn_count = match_predictions_to_gt(predictions, ground_truth, iou_threshold=0.5)\n",
    "\n",
    "# Display results\n",
    "print(\"Detection Classification (IoU threshold = 0.5):\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Class':<10} {'Confidence':<12} {'Classification':<15} {'IoU':<10}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for result in results:\n",
    "    symbol = \"✓\" if result['classification'] == 'TP' else \"✗\"\n",
    "    print(f\"{symbol} {result['class']:<8} {result['confidence']:<12.2f} \"\n",
    "          f\"{result['classification']:<15} {result['iou']:<10.3f}\")\n",
    "\n",
    "tp_count = sum(1 for r in results if r['classification'] == 'TP')\n",
    "fp_count = sum(1 for r in results if r['classification'] == 'FP')\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"True Positives (TP): {tp_count}\")\n",
    "print(f\"False Positives (FP): {fp_count}\")\n",
    "print(f\"False Negatives (FN): {fn_count}\")\n",
    "print(f\"Total GT objects: {len(ground_truth)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision and Recall\n",
    "\n",
    "### Definitions:\n",
    "\n",
    "**Precision**: Of all detections made, how many were correct?\n",
    "```\n",
    "Precision = TP / (TP + FP)\n",
    "```\n",
    "- High precision → Few false alarms\n",
    "- Example: 0.80 means 80% of detections are correct\n",
    "\n",
    "**Recall**: Of all ground truth objects, how many were detected?\n",
    "```\n",
    "Recall = TP / (TP + FN)\n",
    "```\n",
    "- High recall → Few missed objects\n",
    "- Example: 0.90 means 90% of objects were found\n",
    "\n",
    "### Trade-off:\n",
    "- Lower confidence threshold → More detections → Higher recall, lower precision\n",
    "- Higher confidence threshold → Fewer detections → Lower recall, higher precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_precision_recall(results, fn_count):\n",
    "    \"\"\"\n",
    "    Calculate precision and recall from classification results.\n",
    "    \n",
    "    Args:\n",
    "        results: List of classification results\n",
    "        fn_count: Number of false negatives\n",
    "    \n",
    "    Returns:\n",
    "        precision, recall\n",
    "    \"\"\"\n",
    "    tp = sum(1 for r in results if r['classification'] == 'TP')\n",
    "    fp = sum(1 for r in results if r['classification'] == 'FP')\n",
    "    \n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn_count) if (tp + fn_count) > 0 else 0\n",
    "    \n",
    "    return precision, recall\n",
    "\n",
    "precision, recall = calculate_precision_recall(results, fn_count)\n",
    "\n",
    "print(\"Overall Metrics (All Detections):\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Precision: {precision:.3f} ({precision*100:.1f}%)\")\n",
    "print(f\"Recall:    {recall:.3f} ({recall*100:.1f}%)\")\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(f\"  - {precision*100:.1f}% of our detections are correct\")\n",
    "print(f\"  - We found {recall*100:.1f}% of all objects in the image\")\n",
    "\n",
    "# F1 Score (harmonic mean of precision and recall)\n",
    "f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "print(f\"\\nF1 Score: {f1:.3f} (balanced metric)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Varying Confidence Threshold\n",
    "\n",
    "Let's see how precision and recall change with different confidence thresholds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_pr_at_threshold(predictions, ground_truth, conf_threshold, iou_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Calculate precision and recall at a specific confidence threshold.\n",
    "    \n",
    "    Args:\n",
    "        predictions: List of predictions\n",
    "        ground_truth: List of ground truth\n",
    "        conf_threshold: Confidence threshold\n",
    "        iou_threshold: IoU threshold for TP\n",
    "    \n",
    "    Returns:\n",
    "        precision, recall, tp, fp, fn\n",
    "    \"\"\"\n",
    "    # Filter predictions by confidence threshold\n",
    "    filtered_preds = [p for p in predictions if p['confidence'] >= conf_threshold]\n",
    "    \n",
    "    if len(filtered_preds) == 0:\n",
    "        return 0.0, 0.0, 0, 0, len(ground_truth)\n",
    "    \n",
    "    # Match to ground truth\n",
    "    results, fn = match_predictions_to_gt(filtered_preds, ground_truth, iou_threshold)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    tp = sum(1 for r in results if r['classification'] == 'TP')\n",
    "    fp = sum(1 for r in results if r['classification'] == 'FP')\n",
    "    \n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    \n",
    "    return precision, recall, tp, fp, fn\n",
    "\n",
    "# Test different thresholds\n",
    "thresholds = [0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "pr_results = []\n",
    "\n",
    "print(\"Precision-Recall at Different Confidence Thresholds:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Threshold':<12} {'Precision':<12} {'Recall':<12} {'TP':<6} {'FP':<6} {'FN':<6}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for threshold in thresholds:\n",
    "    precision, recall, tp, fp, fn = calculate_pr_at_threshold(\n",
    "        predictions, ground_truth, threshold\n",
    "    )\n",
    "    pr_results.append((threshold, precision, recall))\n",
    "    print(f\"{threshold:<12.2f} {precision:<12.3f} {recall:<12.3f} {tp:<6} {fp:<6} {fn:<6}\")\n",
    "\n",
    "print(\"\\nObservations:\")\n",
    "print(\"  - Lower threshold → More detections → Higher recall, lower precision\")\n",
    "print(\"  - Higher threshold → Fewer detections → Lower recall, higher precision\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision-Recall Curve\n",
    "\n",
    "Plotting precision vs recall shows the trade-off visually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate PR curve with finer granularity\n",
    "conf_thresholds = np.linspace(0.0, 1.0, 50)\n",
    "precisions = []\n",
    "recalls = []\n",
    "\n",
    "for threshold in conf_thresholds:\n",
    "    precision, recall, _, _, _ = calculate_pr_at_threshold(\n",
    "        predictions, ground_truth, threshold\n",
    "    )\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "\n",
    "# Plot PR curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(recalls, precisions, 'b-', linewidth=2, label='PR Curve')\n",
    "plt.scatter(recalls, precisions, c=conf_thresholds, cmap='viridis', \n",
    "           s=30, alpha=0.6, edgecolors='black', linewidth=0.5)\n",
    "\n",
    "# Add colorbar for confidence thresholds\n",
    "cbar = plt.colorbar(label='Confidence Threshold')\n",
    "\n",
    "plt.xlabel('Recall', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Precision', fontsize=12, fontweight='bold')\n",
    "plt.title('Precision-Recall Curve', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlim([0, 1.05])\n",
    "plt.ylim([0, 1.05])\n",
    "\n",
    "# Add annotations\n",
    "plt.text(0.5, 0.95, 'High Precision\\nLow Recall', \n",
    "         ha='center', va='top', fontsize=10, \n",
    "         bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "plt.text(0.95, 0.5, 'High Recall\\nLow Precision', \n",
    "         ha='right', va='center', fontsize=10,\n",
    "         bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nPR Curve Interpretation:\")\n",
    "print(\"  - Ideal curve: Top-right corner (high precision AND high recall)\")\n",
    "print(\"  - Points move right-to-left as confidence threshold increases\")\n",
    "print(\"  - Area under curve = Average Precision (AP)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average Precision (AP) Calculation\n",
    "\n",
    "**Average Precision (AP)** = Area under the Precision-Recall curve\n",
    "\n",
    "### Two Methods:\n",
    "\n",
    "1. **11-point interpolation** (PASCAL VOC 2007):\n",
    "   - Sample at 11 recall levels: 0.0, 0.1, 0.2, ..., 1.0\n",
    "   - Take max precision at each level\n",
    "\n",
    "2. **All-point interpolation** (PASCAL VOC 2010+, COCO):\n",
    "   - Use all unique recall values\n",
    "   - More accurate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ap_11point(precisions, recalls):\n",
    "    \"\"\"\n",
    "    Calculate AP using 11-point interpolation (PASCAL VOC 2007 style).\n",
    "    \n",
    "    Args:\n",
    "        precisions: List of precision values\n",
    "        recalls: List of recall values\n",
    "    \n",
    "    Returns:\n",
    "        Average Precision\n",
    "    \"\"\"\n",
    "    ap = 0.0\n",
    "    recall_levels = np.linspace(0, 1, 11)  # 0.0, 0.1, 0.2, ..., 1.0\n",
    "    \n",
    "    for r_level in recall_levels:\n",
    "        # Find precisions where recall >= r_level\n",
    "        precisions_above = [p for p, r in zip(precisions, recalls) if r >= r_level]\n",
    "        \n",
    "        if len(precisions_above) > 0:\n",
    "            ap += max(precisions_above)\n",
    "    \n",
    "    return ap / 11.0\n",
    "\n",
    "def calculate_ap_allpoint(precisions, recalls):\n",
    "    \"\"\"\n",
    "    Calculate AP using all-point interpolation (PASCAL VOC 2010+ style).\n",
    "    \n",
    "    Args:\n",
    "        precisions: List of precision values (sorted by decreasing recall)\n",
    "        recalls: List of recall values (sorted)\n",
    "    \n",
    "    Returns:\n",
    "        Average Precision\n",
    "    \"\"\"\n",
    "    # Sort by recall\n",
    "    sorted_indices = np.argsort(recalls)\n",
    "    recalls = np.array(recalls)[sorted_indices]\n",
    "    precisions = np.array(precisions)[sorted_indices]\n",
    "    \n",
    "    # Add sentinel values\n",
    "    recalls = np.concatenate(([0.], recalls, [1.]))\n",
    "    precisions = np.concatenate(([0.], precisions, [0.]))\n",
    "    \n",
    "    # Compute the precision envelope (monotonic decreasing)\n",
    "    for i in range(len(precisions) - 2, -1, -1):\n",
    "        precisions[i] = max(precisions[i], precisions[i + 1])\n",
    "    \n",
    "    # Calculate area under curve\n",
    "    indices = np.where(recalls[1:] != recalls[:-1])[0] + 1\n",
    "    ap = np.sum((recalls[indices] - recalls[indices - 1]) * precisions[indices])\n",
    "    \n",
    "    return ap\n",
    "\n",
    "# Calculate both AP methods\n",
    "ap_11pt = calculate_ap_11point(precisions, recalls)\n",
    "ap_all = calculate_ap_allpoint(precisions, recalls)\n",
    "\n",
    "print(\"Average Precision (AP):\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"11-point interpolation:  {ap_11pt:.4f} ({ap_11pt*100:.2f}%)\")\n",
    "print(f\"All-point interpolation: {ap_all:.4f} ({ap_all*100:.2f}%)\")\n",
    "print(\"\\nNote: All-point method is more accurate and widely used today.\")\n",
    "\n",
    "# Visualize interpolation\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# 11-point interpolation\n",
    "ax = axes[0]\n",
    "ax.plot(recalls, precisions, 'b-', alpha=0.3, label='Original')\n",
    "recall_levels = np.linspace(0, 1, 11)\n",
    "interp_precisions = []\n",
    "for r_level in recall_levels:\n",
    "    precisions_above = [p for p, r in zip(precisions, recalls) if r >= r_level]\n",
    "    interp_precisions.append(max(precisions_above) if len(precisions_above) > 0 else 0)\n",
    "ax.step(recall_levels, interp_precisions, 'r-', where='post', linewidth=2, label='11-point')\n",
    "ax.scatter(recall_levels, interp_precisions, c='red', s=50, zorder=5)\n",
    "ax.set_xlabel('Recall', fontweight='bold')\n",
    "ax.set_ylabel('Precision', fontweight='bold')\n",
    "ax.set_title(f'11-Point Interpolation\\nAP = {ap_11pt:.4f}', fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend()\n",
    "ax.set_xlim([0, 1])\n",
    "ax.set_ylim([0, 1.05])\n",
    "\n",
    "# All-point interpolation\n",
    "ax = axes[1]\n",
    "ax.plot(recalls, precisions, 'b-', alpha=0.3, label='Original')\n",
    "# Show monotonic envelope\n",
    "sorted_indices = np.argsort(recalls)\n",
    "sorted_recalls = np.array(recalls)[sorted_indices]\n",
    "sorted_precisions = np.array(precisions)[sorted_indices]\n",
    "envelope = sorted_precisions.copy()\n",
    "for i in range(len(envelope) - 2, -1, -1):\n",
    "    envelope[i] = max(envelope[i], envelope[i + 1])\n",
    "ax.step(sorted_recalls, envelope, 'g-', where='post', linewidth=2, label='All-point')\n",
    "ax.set_xlabel('Recall', fontweight='bold')\n",
    "ax.set_ylabel('Precision', fontweight='bold')\n",
    "ax.set_title(f'All-Point Interpolation\\nAP = {ap_all:.4f}', fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend()\n",
    "ax.set_xlim([0, 1])\n",
    "ax.set_ylim([0, 1.05])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Classes\n",
    "\n",
    "Real object detection involves multiple classes. Calculate AP for each class separately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ap_per_class(predictions, ground_truth, class_name, iou_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Calculate AP for a specific class.\n",
    "    \n",
    "    Args:\n",
    "        predictions: All predictions\n",
    "        ground_truth: All ground truth\n",
    "        class_name: Class to evaluate\n",
    "        iou_threshold: IoU threshold\n",
    "    \n",
    "    Returns:\n",
    "        Average Precision for the class\n",
    "    \"\"\"\n",
    "    # Filter by class\n",
    "    class_preds = [p for p in predictions if p['class'] == class_name]\n",
    "    class_gt = [g for g in ground_truth if g['class'] == class_name]\n",
    "    \n",
    "    if len(class_gt) == 0:\n",
    "        return 0.0  # No ground truth for this class\n",
    "    \n",
    "    # Calculate PR curve\n",
    "    conf_thresholds = sorted(set([p['confidence'] for p in class_preds] + [0.0]), reverse=True)\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    \n",
    "    for threshold in conf_thresholds:\n",
    "        precision, recall, _, _, _ = calculate_pr_at_threshold(\n",
    "            class_preds, class_gt, threshold, iou_threshold\n",
    "        )\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "    \n",
    "    # Calculate AP\n",
    "    if len(precisions) > 0:\n",
    "        return calculate_ap_allpoint(precisions, recalls)\n",
    "    return 0.0\n",
    "\n",
    "# Calculate AP for each class\n",
    "classes = ['car', 'person', 'dog']\n",
    "class_aps = {}\n",
    "\n",
    "print(\"Average Precision per Class:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for class_name in classes:\n",
    "    ap = calculate_ap_per_class(predictions, ground_truth, class_name)\n",
    "    class_aps[class_name] = ap\n",
    "    \n",
    "    # Count objects\n",
    "    gt_count = sum(1 for g in ground_truth if g['class'] == class_name)\n",
    "    pred_count = sum(1 for p in predictions if p['class'] == class_name)\n",
    "    \n",
    "    print(f\"{class_name:<10} AP: {ap:.4f} ({ap*100:.2f}%)  \"\n",
    "          f\"[GT: {gt_count}, Predictions: {pred_count}]\")\n",
    "\n",
    "# Visualize per-class AP\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(class_aps.keys(), class_aps.values(), \n",
    "               color=['steelblue', 'coral', 'mediumseagreen'],\n",
    "               edgecolor='black', linewidth=1.5)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{height:.3f}',\n",
    "            ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.xlabel('Class', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Average Precision (AP)', fontsize=12, fontweight='bold')\n",
    "plt.title('Average Precision per Class (IoU = 0.5)', fontsize=14, fontweight='bold')\n",
    "plt.ylim([0, 1.1])\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Average Precision (mAP)\n",
    "\n",
    "**mAP** = Average of AP across all classes\n",
    "\n",
    "```\n",
    "mAP = (AP_class1 + AP_class2 + ... + AP_classN) / N\n",
    "```\n",
    "\n",
    "This is the **single number metric** used to compare object detection models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_map(predictions, ground_truth, classes, iou_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Calculate mean Average Precision (mAP).\n",
    "    \n",
    "    Args:\n",
    "        predictions: All predictions\n",
    "        ground_truth: All ground truth\n",
    "        classes: List of class names\n",
    "        iou_threshold: IoU threshold\n",
    "    \n",
    "    Returns:\n",
    "        mAP value and per-class APs\n",
    "    \"\"\"\n",
    "    aps = {}\n",
    "    \n",
    "    for class_name in classes:\n",
    "        ap = calculate_ap_per_class(predictions, ground_truth, class_name, iou_threshold)\n",
    "        aps[class_name] = ap\n",
    "    \n",
    "    map_value = np.mean(list(aps.values()))\n",
    "    return map_value, aps\n",
    "\n",
    "# Calculate mAP\n",
    "map_50, aps = calculate_map(predictions, ground_truth, classes, iou_threshold=0.5)\n",
    "\n",
    "print(\"Mean Average Precision (mAP):\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nmAP@0.5 = {map_50:.4f} ({map_50*100:.2f}%)\")\n",
    "print(\"\\nBreakdown by class:\")\n",
    "for class_name, ap in aps.items():\n",
    "    print(f\"  {class_name:<10} AP: {ap:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"\\nNotation:\")\n",
    "print(\"  mAP@0.5  = mAP at IoU threshold 0.5 (PASCAL VOC)\")\n",
    "print(\"  mAP@0.75 = mAP at IoU threshold 0.75 (stricter)\")\n",
    "print(\"  mAP@0.5:0.95 = Average mAP from IoU 0.5 to 0.95 (COCO standard)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mAP@0.5 vs mAP@0.5:0.95 (COCO Standard)\n",
    "\n",
    "The **COCO dataset** uses a more rigorous metric:\n",
    "\n",
    "- **mAP@0.5:0.95**: Average mAP calculated at IoU thresholds [0.5, 0.55, 0.6, ..., 0.95]\n",
    "- Total: 10 different thresholds\n",
    "- Rewards models with better localization accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mAP at different IoU thresholds\n",
    "iou_thresholds = np.arange(0.5, 1.0, 0.05)  # 0.5, 0.55, 0.6, ..., 0.95\n",
    "map_values = []\n",
    "\n",
    "print(\"mAP at Different IoU Thresholds:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'IoU Threshold':<15} {'mAP':<10}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for iou_threshold in iou_thresholds:\n",
    "    map_value, _ = calculate_map(predictions, ground_truth, classes, iou_threshold)\n",
    "    map_values.append(map_value)\n",
    "    print(f\"{iou_threshold:<15.2f} {map_value:<10.4f}\")\n",
    "\n",
    "# Calculate COCO-style mAP\n",
    "coco_map = np.mean(map_values)\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"\\nmAP@0.5:0.95 (COCO): {coco_map:.4f} ({coco_map*100:.2f}%)\")\n",
    "print(f\"mAP@0.5 (PASCAL):    {map_50:.4f} ({map_50*100:.2f}%)\")\n",
    "\n",
    "print(\"\\nObservation:\")\n",
    "print(\"  COCO mAP is typically lower because it requires better localization\")\n",
    "\n",
    "# Visualize mAP vs IoU threshold\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(iou_thresholds, map_values, 'o-', linewidth=2, markersize=8, color='steelblue')\n",
    "plt.axhline(y=coco_map, color='red', linestyle='--', linewidth=2, \n",
    "           label=f'COCO mAP = {coco_map:.4f}')\n",
    "plt.xlabel('IoU Threshold', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('mAP', fontsize=12, fontweight='bold')\n",
    "plt.title('mAP vs IoU Threshold', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend(fontsize=11)\n",
    "plt.xlim([0.45, 1.0])\n",
    "plt.ylim([0, max(map_values) * 1.1])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nAs IoU threshold increases:\")\n",
    "print(\"  - Fewer detections count as TP\")\n",
    "print(\"  - mAP decreases\")\n",
    "print(\"  - Model must localize objects more precisely\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation\n",
    "\n",
    "### What does mAP mean?\n",
    "\n",
    "| mAP Value | Interpretation | Example Use Case |\n",
    "|-----------|----------------|------------------|\n",
    "| 0.90+ | Excellent | Research benchmarks |\n",
    "| 0.70-0.90 | Good | Production systems |\n",
    "| 0.50-0.70 | Moderate | Proof of concept |\n",
    "| 0.30-0.50 | Poor | Needs improvement |\n",
    "| <0.30 | Very Poor | Baseline/broken |\n",
    "\n",
    "### Comparing Models\n",
    "\n",
    "**Example:**\n",
    "- **YOLO v8 mAP@0.5:0.95 = 0.53** on COCO\n",
    "- **Faster R-CNN mAP@0.5:0.95 = 0.42** on COCO\n",
    "\n",
    "→ YOLO v8 is better at detecting and localizing objects\n",
    "\n",
    "### Important Notes:\n",
    "\n",
    "1. **Dataset matters**: \n",
    "   - mAP on COCO ≠ mAP on custom dataset\n",
    "   - Compare models on **same** dataset\n",
    "\n",
    "2. **Speed-accuracy trade-off**:\n",
    "   - Faster R-CNN: Higher mAP, slower (5-10 FPS)\n",
    "   - YOLO: Lower mAP, faster (30-60 FPS)\n",
    "\n",
    "3. **Class imbalance**:\n",
    "   - mAP averages across classes\n",
    "   - Check per-class AP for important classes\n",
    "\n",
    "4. **IoU threshold**:\n",
    "   - Always specify: mAP@0.5 or mAP@0.5:0.95\n",
    "   - COCO standard: mAP@0.5:0.95"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Calculate mAP for Given Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"EXERCISE: Calculate mAP\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# New dataset\n",
    "exercise_gt = [\n",
    "    {'class': 'cat', 'bbox': [50, 50, 150, 150]},\n",
    "    {'class': 'cat', 'bbox': [200, 100, 300, 200]},\n",
    "    {'class': 'dog', 'bbox': [100, 200, 200, 300]},\n",
    "    {'class': 'bird', 'bbox': [300, 50, 350, 100]},\n",
    "]\n",
    "\n",
    "exercise_preds = [\n",
    "    {'class': 'cat', 'confidence': 0.95, 'bbox': [52, 48, 148, 152]},\n",
    "    {'class': 'cat', 'confidence': 0.88, 'bbox': [205, 95, 295, 205]},\n",
    "    {'class': 'dog', 'confidence': 0.92, 'bbox': [98, 205, 202, 295]},\n",
    "    {'class': 'bird', 'confidence': 0.65, 'bbox': [305, 55, 345, 95]},\n",
    "    {'class': 'cat', 'confidence': 0.45, 'bbox': [150, 150, 200, 200]},  # FP\n",
    "]\n",
    "\n",
    "print(\"\\nTasks:\")\n",
    "print(\"1. Calculate AP for each class (cat, dog, bird)\")\n",
    "print(\"2. Calculate mAP@0.5\")\n",
    "print(\"3. Which class has the highest AP?\")\n",
    "print(\"4. How many false positives are there?\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Uncomment the code below to see the solution:\\n\")\n",
    "\n",
    "# Solution (uncomment to run)\n",
    "# exercise_classes = ['cat', 'dog', 'bird']\n",
    "# exercise_map, exercise_aps = calculate_map(exercise_preds, exercise_gt, exercise_classes)\n",
    "# \n",
    "# print(\"\\nSolution:\")\n",
    "# print(\"=\" * 60)\n",
    "# for class_name, ap in exercise_aps.items():\n",
    "#     print(f\"{class_name:<10} AP: {ap:.4f}\")\n",
    "# print(f\"\\nmAP@0.5: {exercise_map:.4f}\")\n",
    "# \n",
    "# best_class = max(exercise_aps, key=exercise_aps.get)\n",
    "# print(f\"\\nBest class: {best_class} (AP = {exercise_aps[best_class]:.4f})\")\n",
    "# \n",
    "# # Count FPs\n",
    "# results, _ = match_predictions_to_gt(exercise_preds, exercise_gt)\n",
    "# fp_count = sum(1 for r in results if r['classification'] == 'FP')\n",
    "# print(f\"False positives: {fp_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **Detection Classification:**\n",
    "   - **TP (True Positive)**: Correct detection with IoU ≥ threshold\n",
    "   - **FP (False Positive)**: Wrong detection or IoU < threshold\n",
    "   - **FN (False Negative)**: Missed ground truth object\n",
    "\n",
    "2. **Metrics:**\n",
    "   - **Precision** = TP / (TP + FP) — Accuracy of detections\n",
    "   - **Recall** = TP / (TP + FN) — Completeness of detections\n",
    "   - **AP (Average Precision)** = Area under PR curve\n",
    "   - **mAP (mean Average Precision)** = Average of AP across all classes\n",
    "\n",
    "3. **Evaluation Standards:**\n",
    "   - **PASCAL VOC**: mAP@0.5 (IoU threshold = 0.5)\n",
    "   - **MS COCO**: mAP@0.5:0.95 (average across 10 IoU thresholds)\n",
    "   - COCO is more strict and widely used in research\n",
    "\n",
    "4. **Interpretation:**\n",
    "   - Higher mAP = Better detector\n",
    "   - Compare models on same dataset\n",
    "   - Always specify IoU threshold\n",
    "   - Check per-class AP for imbalanced datasets\n",
    "\n",
    "5. **Trade-offs:**\n",
    "   - Confidence threshold: Precision vs Recall\n",
    "   - Model choice: Speed vs Accuracy\n",
    "   - IoU threshold: Localization strictness\n",
    "\n",
    "### Typical mAP Values (COCO Dataset)\n",
    "\n",
    "| Model | mAP@0.5:0.95 | Speed (FPS) |\n",
    "|-------|--------------|-------------|\n",
    "| YOLOv8-nano | 0.37 | 80+ |\n",
    "| YOLOv8-small | 0.45 | 60 |\n",
    "| YOLOv8-medium | 0.50 | 40 |\n",
    "| YOLOv8-large | 0.53 | 30 |\n",
    "| Faster R-CNN | 0.42 | 5-10 |\n",
    "| RetinaNet | 0.40 | 15 |\n",
    "\n",
    "### Next Steps\n",
    "- **Notebook 05**: Classical sliding window detection (pre-deep learning)\n",
    "- **Week 14**: Modern architectures (YOLO, R-CNN family)\n",
    "- **Week 15**: Advanced topics (NMS, anchor boxes)\n",
    "\n",
    "### Further Reading\n",
    "- PASCAL VOC Challenge: http://host.robots.ox.ac.uk/pascal/VOC/\n",
    "- COCO Evaluation Metrics: https://cocodataset.org/#detection-eval\n",
    "- Papers: \"The PASCAL Visual Object Classes Challenge\" (2010)\n",
    "\n",
    "---\n",
    "\n",
    "**Completion Time:** ~15 minutes  \n",
    "**Mastery:** Practice calculating mAP on real datasets (COCO, VOC)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
