# Week 3 Deliverables Overview
**Deep Neural Network Architectures (21CSE558T)**

## Week 3: Aug 25-29, 2025
**Module 1 Completion - Advanced Neural Network Fundamentals**

---

**© 2025 Prof. Ramesh Babu | SRM University | Data Science and Business Systems (DSBS)**  
*Course Materials for 21CSE558T - Deep Neural Network Architectures*

---

## Primary Learning Objectives

1. **Master Activation Functions** - Understand mathematical properties and practical applications
2. **Comprehend Layer Architecture** - Learn neural network layer composition and initialization
3. **Grasp Mathematical Foundations** - Understand forward pass formulations and backpropagation framework
4. **Implement Tensor Operations** - Build practical skills in tensor manipulation and basic neural networks

---

## Core Deliverables

### 1. Activation Functions Mastery
- **Sigmoid, Tanh, ReLU Family** - Mathematical properties, gradients, use cases
- **Advanced Activations** - Leaky ReLU, ELU, Swish, GELU
- **Practical Selection Criteria** - When and why to choose specific functions

### 2. Neural Network Layer Architecture
- **Dense/Fully Connected Layers** - Structure and parameter initialization
- **Layer Composition** - Building multi-layer architectures
- **Forward Pass Mathematics** - Matrix operations and computations

### 3. Mathematical Model Foundation
- **Matrix Operations** - Core linear algebra in neural networks  
- **Backpropagation Framework** - Mathematical derivation and implementation
- **Loss Function Derivatives** - Gradient computation fundamentals

### 4. Tutorial Implementation: T3
**"Building Programs to Perform Basic Operations in Tensors"**
- Advanced tensor manipulation techniques
- Custom activation function implementation
- Basic neural network construction from mathematical principles

---

## Session Distribution

### Day 3 (2-Hour Lecture): Theory Foundation
**Focus:** Deep dive into activation functions and mathematical models
- Activation function analysis and comparison
- Mathematical foundations of neural networks
- Forward pass and backpropagation mathematics

### Day 4 (1-Hour Tutorial): Practical Implementation  
**Focus:** Hands-on tensor operations and neural network building
- Tutorial T3 implementation
- Custom activation functions in code
- Basic network construction from scratch

---

## Assessment Preparation

**Unit Test 1 Preparation** (Scheduled: Sep 19)
- Mathematical understanding of activation functions
- Neural network layer composition concepts
- Basic tensor operations and manipulations
- Foundation for Module 2 optimization topics

---

## Learning Outcome Achievement

**By Week 3 End:** Students will understand the mathematical underpinnings of neural networks and be prepared to tackle optimization challenges in Module 2.

**CO-1 Progress:** Foundation building for creating simple deep neural networks and explaining their functions.

---

## Prerequisites for Success

- **Week 1-2 Knowledge:** Perceptron, MLP, XOR problem, TensorFlow basics
- **Mathematical Foundation:** Linear algebra comfort, basic calculus
- **Programming Skills:** Python basics, tensor manipulation readiness

---

---

## CO-PO Alignment & Assessment

### **Course Outcomes (CO) Coverage**

**Primary Focus:**
- **CO-1**: "Create a simple deep neural network and explain its functions" 
  - **Week 3 Contribution:** Mathematical understanding of neural network components and activation functions
  - **Assessment Level:** Foundation building (40% of CO-1 achievement)

- **CO-2**: "Build neural networks with multiple layers with appropriate activations"
  - **Week 3 Contribution:** Learning activation function selection criteria and layer composition
  - **Assessment Level:** Introduction level (25% of CO-2 achievement)

### **Programme Outcomes (PO) Alignment**

**From CO-1 mapping:**
- **PO-1 (Engineering Knowledge) - Level 3:** Deep mathematical understanding of activation functions and neural network fundamentals
- **PO-3 (Design & Development) - Level 1:** Basic design decisions for activation function selection

**From CO-2 mapping:**
- **PO-1 (Engineering Knowledge) - Level 1:** Mathematical foundations for multi-layer networks  
- **PO-2 (Problem Analysis) - Level 2:** Understanding when to use specific activation functions

---

## Required Reading & Preparation

### **Mandatory Pre-Reading**

**Primary Source:**
- **Chollet, "Deep Learning with Python" (2018)**
  - **Chapter 2.3**: "The gears of neural networks" (activation functions, layers)
  - **Chapter 3.1**: "Anatomy of a neural network" (detailed activation functions)

**Mathematical Foundation:**
- **Goodfellow et al., "Deep Learning" (2017)**  
  - **Chapter 6.3**: "Hidden units" (activation functions mathematical analysis)
  - **Chapter 6.4**: "Architecture design" (layer composition principles)

### **Recommended Supplementary Reading**

- **Aggarwal, "Neural Networks and Deep Learning" (2018)**
  - **Chapter 1.3**: "Activation functions" (comprehensive overview)
  - **Chapter 2.2**: "Training with backpropagation" (mathematical foundations)

- **Manaswi, "Deep Learning with Applications Using Python" (2018)**
  - **Chapter 2**: "Neural Networks" (implementation examples in TensorFlow)

### **Essential Mathematical Review**
- **Linear Algebra:** Matrix multiplication, broadcasting, derivatives
- **Calculus:** Chain rule, partial derivatives, gradient computation
- **Python/NumPy:** Array operations, mathematical functions

---

## Success Metrics

✅ **Theoretical Understanding:** Can explain activation function properties and selection criteria  
✅ **Mathematical Competency:** Can derive forward pass computations manually  
✅ **Practical Skills:** Can implement custom activation functions and basic networks  
✅ **Integration Readiness:** Prepared for Module 2 optimization concepts  
✅ **CO-PO Achievement:** Measurable progress toward CO-1 (40%) and CO-2 (25%)