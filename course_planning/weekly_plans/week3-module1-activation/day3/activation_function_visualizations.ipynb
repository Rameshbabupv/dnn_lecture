{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# ğŸ­ Activation Function Visualizations - Week 3 Day 3\n",
    "\n",
    "**Deep Neural Network Architectures (21CSE558T)**\n",
    "\n",
    "Â© 2025 Prof. Ramesh Babu | SRM University | DSBS\n",
    "\n",
    "This notebook generates comprehensive visualizations for understanding:\n",
    "- Activation functions and their derivatives\n",
    "- Gradient flow problems\n",
    "- Neural network architecture\n",
    "- Weight initialization strategies\n",
    "- Backpropagation mechanics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## ğŸ“¦ Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imports"
   },
   "outputs": [],
   "source": "import numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib.patches import Rectangle, FancyBboxPatch\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set up beautiful plotting\nplt.style.use('default')\nsns.set_palette(\"husl\")\nplt.rcParams['figure.figsize'] = (12, 8)\nplt.rcParams['font.size'] = 12\nplt.rcParams['axes.linewidth'] = 1.5\nplt.rcParams['grid.alpha'] = 0.3\n\nprint(\"âœ… Libraries imported successfully!\")\nprint(f\"ğŸ“Š NumPy version: {np.__version__}\")\nprint(f\"ğŸ“ˆ Matplotlib version: {matplotlib.__version__}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "helper-functions"
   },
   "source": [
    "## ğŸ”§ Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "activation-functions"
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"Sigmoid activation function\"\"\"\n",
    "    return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    \"\"\"Sigmoid derivative\"\"\"\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    \"\"\"Tanh derivative\"\"\"\n",
    "    t = np.tanh(x)\n",
    "    return 1 - t**2\n",
    "\n",
    "def relu(x):\n",
    "    \"\"\"ReLU activation function\"\"\"\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    \"\"\"ReLU derivative\"\"\"\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "def leaky_relu(x, alpha=0.01):\n",
    "    \"\"\"Leaky ReLU activation function\"\"\"\n",
    "    return np.where(x > 0, x, alpha * x)\n",
    "\n",
    "def leaky_relu_derivative(x, alpha=0.01):\n",
    "    \"\"\"Leaky ReLU derivative\"\"\"\n",
    "    return np.where(x > 0, 1, alpha)\n",
    "\n",
    "print(\"ğŸ”§ Activation functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "activation-comparison"
   },
   "source": [
    "## ğŸ­ 1. Activation Functions Comparison\n",
    "\n",
    "Let's visualize the classical activation functions and understand their properties:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create-activation-comparison"
   },
   "outputs": [],
   "source": [
    "def create_activation_comparison():\n",
    "    \"\"\"Create comprehensive activation function comparison\"\"\"\n",
    "    x = np.linspace(-5, 5, 1000)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle('ğŸ­ Classical Activation Functions & Their Derivatives', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Sigmoid\n",
    "    axes[0, 0].plot(x, sigmoid(x), 'purple', linewidth=3, label='Sigmoid')\n",
    "    axes[0, 0].plot(x, sigmoid_derivative(x), 'purple', linestyle='--', linewidth=2, label='Sigmoid Derivative')\n",
    "    axes[0, 0].set_title('Sigmoid: Ïƒ(x) = 1/(1 + eâ»Ë£)', fontweight='bold')\n",
    "    axes[0, 0].set_ylabel('f(x)')\n",
    "    axes[0, 0].grid(True)\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "    axes[0, 0].axvline(x=0, color='k', linestyle='-', alpha=0.3)\n",
    "    \n",
    "    # Add annotations for sigmoid\n",
    "    axes[0, 0].annotate('Saturation\\n(Vanishing Gradients)', xy=(-4, 0.02), xytext=(-3, 0.3),\n",
    "                       arrowprops=dict(arrowstyle='->', color='red'), fontsize=10, ha='center',\n",
    "                       bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.7))\n",
    "    \n",
    "    # Tanh\n",
    "    axes[0, 1].plot(x, np.tanh(x), 'teal', linewidth=3, label='Tanh')\n",
    "    axes[0, 1].plot(x, tanh_derivative(x), 'teal', linestyle='--', linewidth=2, label='Tanh Derivative')\n",
    "    axes[0, 1].set_title('Tanh: (eË£ - eâ»Ë£)/(eË£ + eâ»Ë£)', fontweight='bold')\n",
    "    axes[0, 1].grid(True)\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "    axes[0, 1].axvline(x=0, color='k', linestyle='-', alpha=0.3)\n",
    "    \n",
    "    # Add annotations for tanh\n",
    "    axes[0, 1].annotate('Zero-centered\\n(Better than Sigmoid)', xy=(0, 0), xytext=(2, -0.7),\n",
    "                       arrowprops=dict(arrowstyle='->', color='green'), fontsize=10, ha='center',\n",
    "                       bbox=dict(boxstyle='round,pad=0.3', facecolor='lightgreen', alpha=0.7))\n",
    "    \n",
    "    # ReLU\n",
    "    axes[1, 0].plot(x, relu(x), 'red', linewidth=3, label='ReLU')\n",
    "    axes[1, 0].plot(x, relu_derivative(x), 'red', linestyle='--', linewidth=2, label='ReLU Derivative')\n",
    "    axes[1, 0].set_title('ReLU: f(x) = max(0, x)', fontweight='bold')\n",
    "    axes[1, 0].set_xlabel('x')\n",
    "    axes[1, 0].set_ylabel('f(x)')\n",
    "    axes[1, 0].grid(True)\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "    axes[1, 0].axvline(x=0, color='k', linestyle='-', alpha=0.3)\n",
    "    \n",
    "    # Add annotations for ReLU\n",
    "    axes[1, 0].annotate('Dead Neurons\\n(x < 0)', xy=(-2, 0), xytext=(-3, 2),\n",
    "                       arrowprops=dict(arrowstyle='->', color='red'), fontsize=10, ha='center',\n",
    "                       bbox=dict(boxstyle='round,pad=0.3', facecolor='orange', alpha=0.7))\n",
    "    \n",
    "    # Leaky ReLU\n",
    "    axes[1, 1].plot(x, leaky_relu(x), 'orange', linewidth=3, label='Leaky ReLU')\n",
    "    axes[1, 1].plot(x, leaky_relu_derivative(x), 'orange', linestyle='--', linewidth=2, label='Leaky ReLU Derivative')\n",
    "    axes[1, 1].set_title('Leaky ReLU: f(x) = max(0.01x, x)', fontweight='bold')\n",
    "    axes[1, 1].set_xlabel('x')\n",
    "    axes[1, 1].grid(True)\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "    axes[1, 1].axvline(x=0, color='k', linestyle='-', alpha=0.3)\n",
    "    \n",
    "    # Add annotations for Leaky ReLU\n",
    "    axes[1, 1].annotate('Small gradient\\n(Prevents dying)', xy=(-2, -0.02), xytext=(-3, 1.5),\n",
    "                       arrowprops=dict(arrowstyle='->', color='green'), fontsize=10, ha='center',\n",
    "                       bbox=dict(boxstyle='round,pad=0.3', facecolor='lightblue', alpha=0.7))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Run the visualization\n",
    "create_activation_comparison()\n",
    "print(\"âœ… Activation functions comparison completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gradient-flow"
   },
   "source": [
    "## ğŸŒŠ 2. Gradient Flow Analysis\n",
    "\n",
    "Understanding the vanishing gradient problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create-gradient-flow"
   },
   "outputs": [],
   "source": [
    "def create_gradient_flow_demo():\n",
    "    \"\"\"Demonstrate vanishing gradient problem\"\"\"\n",
    "    x = np.linspace(-5, 5, 1000)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    fig.suptitle('ğŸŒŠ Gradient Flow Analysis: Why ReLU Won the Battle', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Sigmoid gradients\n",
    "    axes[0].plot(x, sigmoid_derivative(x), 'purple', linewidth=3, label='Sigmoid Gradient')\n",
    "    axes[0].fill_between(x, 0, sigmoid_derivative(x), alpha=0.3, color='purple')\n",
    "    axes[0].set_title('Sigmoid Gradients\\n(Vanishing Problem)', fontweight='bold')\n",
    "    axes[0].set_ylabel('Gradient Magnitude')\n",
    "    axes[0].grid(True)\n",
    "    axes[0].axhline(y=0.25, color='red', linestyle='--', alpha=0.7, label='Max Gradient = 0.25')\n",
    "    axes[0].legend()\n",
    "    \n",
    "    # Tanh gradients\n",
    "    axes[1].plot(x, tanh_derivative(x), 'teal', linewidth=3, label='Tanh Gradient')\n",
    "    axes[1].fill_between(x, 0, tanh_derivative(x), alpha=0.3, color='teal')\n",
    "    axes[1].set_title('Tanh Gradients\\n(Better, but still saturates)', fontweight='bold')\n",
    "    axes[1].set_xlabel('x')\n",
    "    axes[1].grid(True)\n",
    "    axes[1].axhline(y=1.0, color='red', linestyle='--', alpha=0.7, label='Max Gradient = 1.0')\n",
    "    axes[1].legend()\n",
    "    \n",
    "    # ReLU gradients\n",
    "    relu_grad = relu_derivative(x)\n",
    "    axes[2].plot(x, relu_grad, 'red', linewidth=3, label='ReLU Gradient')\n",
    "    axes[2].fill_between(x, 0, relu_grad, alpha=0.3, color='red')\n",
    "    axes[2].set_title('ReLU Gradients\\n(No Saturation!)', fontweight='bold')\n",
    "    axes[2].grid(True)\n",
    "    axes[2].set_ylim(-0.1, 1.2)\n",
    "    axes[2].axhline(y=1.0, color='green', linestyle='--', alpha=0.7, label='Constant Gradient = 1.0')\n",
    "    axes[2].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Run the visualization\n",
    "create_gradient_flow_demo()\n",
    "print(\"âœ… Gradient flow analysis completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "neural-architecture"
   },
   "source": [
    "## ğŸ§  3. Neural Network Architecture\n",
    "\n",
    "Visualizing layer structure and matrix operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create-neural-architecture"
   },
   "outputs": [],
   "source": [
    "def create_neural_layer_architecture():\n",
    "    \"\"\"Visualize neural network layer architecture with matrix operations\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "    fig.suptitle('ğŸ§  Neural Network Layer Architecture & Matrix Operations', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Layer architecture diagram\n",
    "    ax1 = axes[0]\n",
    "    \n",
    "    # Input layer\n",
    "    input_y = [1, 2, 3]\n",
    "    for i, y in enumerate(input_y):\n",
    "        circle = plt.Circle((1, y), 0.3, color='lightblue', ec='black', linewidth=2)\n",
    "        ax1.add_patch(circle)\n",
    "        ax1.text(1, y, f'x{i+1}', ha='center', va='center', fontweight='bold')\n",
    "    \n",
    "    ax1.text(1, 0.2, 'Input Layer\\n(3 neurons)', ha='center', fontweight='bold')\n",
    "    \n",
    "    # Hidden layer\n",
    "    hidden_y = [0.5, 1.5, 2.5, 3.5]\n",
    "    for i, y in enumerate(hidden_y):\n",
    "        circle = plt.Circle((3, y), 0.3, color='lightgreen', ec='black', linewidth=2)\n",
    "        ax1.add_patch(circle)\n",
    "        ax1.text(3, y, f'h{i+1}', ha='center', va='center', fontweight='bold')\n",
    "        # Add activation function symbol\n",
    "        ax1.text(3.5, y, 'Ïƒ', ha='center', va='center', fontsize=16, color='red', fontweight='bold')\n",
    "    \n",
    "    ax1.text(3, -0.3, 'Hidden Layer\\n(4 neurons)', ha='center', fontweight='bold')\n",
    "    \n",
    "    # Output layer\n",
    "    output_y = [1.5, 2.5]\n",
    "    for i, y in enumerate(output_y):\n",
    "        circle = plt.Circle((5, y), 0.3, color='lightcoral', ec='black', linewidth=2)\n",
    "        ax1.add_patch(circle)\n",
    "        ax1.text(5, y, f'y{i+1}', ha='center', va='center', fontweight='bold')\n",
    "    \n",
    "    ax1.text(5, 0.7, 'Output Layer\\n(2 neurons)', ha='center', fontweight='bold')\n",
    "    \n",
    "    # Connections\n",
    "    for i in input_y:\n",
    "        for j in hidden_y:\n",
    "            ax1.plot([1.3, 2.7], [i, j], 'k-', alpha=0.3, linewidth=1)\n",
    "    \n",
    "    for i in hidden_y:\n",
    "        for j in output_y:\n",
    "            ax1.plot([3.3, 4.7], [i, j], 'k-', alpha=0.3, linewidth=1)\n",
    "    \n",
    "    # Weight matrices labels\n",
    "    ax1.text(2, 4, 'Wâ‚ (3Ã—4)\\nbâ‚ (4Ã—1)', ha='center', va='center', fontsize=12,\n",
    "             bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.7))\n",
    "    ax1.text(4, 4, 'Wâ‚‚ (4Ã—2)\\nbâ‚‚ (2Ã—1)', ha='center', va='center', fontsize=12,\n",
    "             bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.7))\n",
    "    \n",
    "    ax1.set_xlim(0, 6)\n",
    "    ax1.set_ylim(-0.5, 4.5)\n",
    "    ax1.set_aspect('equal')\n",
    "    ax1.axis('off')\n",
    "    ax1.set_title('Network Architecture', fontweight='bold')\n",
    "    \n",
    "    # Matrix operations visualization\n",
    "    ax2 = axes[1]\n",
    "    \n",
    "    # Create matrix visualization\n",
    "    ax2.text(0.5, 0.9, 'Forward Pass Mathematics:', fontsize=14, fontweight='bold', transform=ax2.transAxes)\n",
    "    \n",
    "    equations = [\n",
    "        'Layer 1: zâ‚ = xÂ·Wâ‚ + bâ‚',\n",
    "        '         aâ‚ = Ïƒ(zâ‚)',\n",
    "        '',\n",
    "        'Layer 2: zâ‚‚ = aâ‚Â·Wâ‚‚ + bâ‚‚', \n",
    "        '         aâ‚‚ = Ïƒ(zâ‚‚)',\n",
    "        '',\n",
    "        'Matrix Dimensions:',\n",
    "        'â€¢ Input: (batch_size, 3)',\n",
    "        'â€¢ Wâ‚: (3, 4), bâ‚: (4,)',\n",
    "        'â€¢ Hidden: (batch_size, 4)',\n",
    "        'â€¢ Wâ‚‚: (4, 2), bâ‚‚: (2,)',\n",
    "        'â€¢ Output: (batch_size, 2)'\n",
    "    ]\n",
    "    \n",
    "    for i, eq in enumerate(equations):\n",
    "        ax2.text(0.05, 0.8 - i*0.06, eq, fontsize=12, transform=ax2.transAxes,\n",
    "                fontweight='bold' if 'â€¢' in eq or 'Layer' in eq else 'normal',\n",
    "                color='red' if 'Ïƒ' in eq else 'black')\n",
    "    \n",
    "    # Add activation function examples\n",
    "    ax2.text(0.05, 0.25, 'Common Activation Choices:', fontsize=12, fontweight='bold', transform=ax2.transAxes)\n",
    "    ax2.text(0.05, 0.20, 'â€¢ Hidden layers: ReLU, Tanh, Leaky ReLU', fontsize=11, transform=ax2.transAxes)\n",
    "    ax2.text(0.05, 0.15, 'â€¢ Binary output: Sigmoid', fontsize=11, transform=ax2.transAxes)\n",
    "    ax2.text(0.05, 0.10, 'â€¢ Multi-class: Softmax', fontsize=11, transform=ax2.transAxes)\n",
    "    ax2.text(0.05, 0.05, 'â€¢ Regression: Linear (no activation)', fontsize=11, transform=ax2.transAxes)\n",
    "    \n",
    "    ax2.axis('off')\n",
    "    ax2.set_title('Mathematical Operations', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Run the visualization\n",
    "create_neural_layer_architecture()\n",
    "print(\"âœ… Neural layer architecture completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "initialization"
   },
   "source": [
    "## âš–ï¸ 4. Weight Initialization Strategies\n",
    "\n",
    "Comparing different weight initialization methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create-initialization"
   },
   "outputs": [],
   "source": [
    "def create_initialization_demo():\n",
    "    \"\"\"Demonstrate weight initialization strategies\"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    fig.suptitle('âš–ï¸ Weight Initialization Strategies & Their Impact', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    n_inputs, n_outputs = 100, 50\n",
    "    n_samples = 1000\n",
    "    \n",
    "    # Different initialization strategies\n",
    "    strategies = [\n",
    "        ('Random Normal (Ïƒ=1)', np.random.normal(0, 1, (n_inputs, n_outputs))),\n",
    "        ('Random Normal (Ïƒ=0.01)', np.random.normal(0, 0.01, (n_inputs, n_outputs))),\n",
    "        ('Xavier/Glorot', np.random.normal(0, np.sqrt(2.0/(n_inputs + n_outputs)), (n_inputs, n_outputs))),\n",
    "        ('He Initialization', np.random.normal(0, np.sqrt(2.0/n_inputs), (n_inputs, n_outputs))),\n",
    "        ('Random Uniform [-1,1]', np.random.uniform(-1, 1, (n_inputs, n_outputs))),\n",
    "        ('Zeros (Bad!)', np.zeros((n_inputs, n_outputs)))\n",
    "    ]\n",
    "    \n",
    "    # Simulate forward pass with different initializations\n",
    "    input_data = np.random.normal(0, 1, (n_samples, n_inputs))\n",
    "    \n",
    "    for idx, (name, weights) in enumerate(strategies):\n",
    "        row = idx // 3\n",
    "        col = idx % 3\n",
    "        ax = axes[row, col]\n",
    "        \n",
    "        # Forward pass\n",
    "        output = np.dot(input_data, weights)\n",
    "        \n",
    "        # Apply ReLU activation\n",
    "        activated_output = np.maximum(0, output)\n",
    "        \n",
    "        # Plot distribution of activations\n",
    "        ax.hist(activated_output.flatten(), bins=50, alpha=0.7, density=True, color=plt.cm.Set3(idx))\n",
    "        ax.set_title(f'{name}\\n(Mean: {activated_output.mean():.3f}, Std: {activated_output.std():.3f})', \n",
    "                    fontweight='bold')\n",
    "        ax.set_xlabel('Activation Values')\n",
    "        ax.set_ylabel('Density')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add annotations for good/bad cases\n",
    "        if 'Xavier' in name or 'He' in name:\n",
    "            ax.text(0.02, 0.95, 'âœ… Good!', transform=ax.transAxes, fontsize=12, \n",
    "                   bbox=dict(boxstyle='round,pad=0.3', facecolor='lightgreen', alpha=0.8))\n",
    "        elif 'Zeros' in name or 'Ïƒ=1' in name:\n",
    "            ax.text(0.02, 0.95, 'âŒ Bad!', transform=ax.transAxes, fontsize=12,\n",
    "                   bbox=dict(boxstyle='round,pad=0.3', facecolor='lightcoral', alpha=0.8))\n",
    "        else:\n",
    "            ax.text(0.02, 0.95, 'âš ï¸ Okay', transform=ax.transAxes, fontsize=12,\n",
    "                   bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Run the visualization\n",
    "create_initialization_demo()\n",
    "print(\"âœ… Initialization strategies demonstration completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "backpropagation"
   },
   "source": [
    "## ğŸ”„ 5. Backpropagation Visualization\n",
    "\n",
    "Understanding the learning mechanism:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create-backpropagation"
   },
   "outputs": [],
   "source": [
    "def create_backpropagation_visual():\n",
    "    \"\"\"Create backpropagation visualization\"\"\"\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(16, 12))\n",
    "    fig.suptitle('ğŸ”„ Backpropagation: The Learning Engine', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Forward pass visualization\n",
    "    ax1 = axes[0]\n",
    "    \n",
    "    # Network layers\n",
    "    layers = ['Input', 'Hidden 1', 'Hidden 2', 'Output']\n",
    "    layer_x = [0, 2, 4, 6]\n",
    "    \n",
    "    for i, (layer, x) in enumerate(zip(layers, layer_x)):\n",
    "        # Draw layer\n",
    "        rect = Rectangle((x-0.3, 1), 0.6, 2, facecolor=plt.cm.Set3(i), alpha=0.7, ec='black')\n",
    "        ax1.add_patch(rect)\n",
    "        ax1.text(x, 2, layer, ha='center', va='center', fontweight='bold')\n",
    "        \n",
    "        # Add arrows for forward pass\n",
    "        if i < len(layers) - 1:\n",
    "            ax1.arrow(x+0.3, 2, 1.4, 0, head_width=0.1, head_length=0.1, fc='blue', ec='blue')\n",
    "            ax1.text(x+1, 2.3, 'Forward', ha='center', color='blue', fontweight='bold')\n",
    "    \n",
    "    # Add mathematical operations\n",
    "    operations = ['Wâ‚Â·x + bâ‚\\nÏƒ(zâ‚)', 'Wâ‚‚Â·aâ‚ + bâ‚‚\\nÏƒ(zâ‚‚)', 'Wâ‚ƒÂ·aâ‚‚ + bâ‚ƒ\\nÏƒ(zâ‚ƒ)']\n",
    "    for i, op in enumerate(operations):\n",
    "        ax1.text(layer_x[i] + 1, 1.2, op, ha='center', va='center', fontsize=10,\n",
    "                bbox=dict(boxstyle='round,pad=0.2', facecolor='lightblue', alpha=0.7))\n",
    "    \n",
    "    ax1.set_xlim(-1, 7)\n",
    "    ax1.set_ylim(0.5, 3.5)\n",
    "    ax1.set_title('Forward Pass: Computing Predictions', fontweight='bold')\n",
    "    ax1.axis('off')\n",
    "    \n",
    "    # Backward pass visualization\n",
    "    ax2 = axes[1]\n",
    "    \n",
    "    for i, (layer, x) in enumerate(zip(layers, layer_x)):\n",
    "        # Draw layer\n",
    "        rect = Rectangle((x-0.3, 1), 0.6, 2, facecolor=plt.cm.Set3(i), alpha=0.7, ec='black')\n",
    "        ax2.add_patch(rect)\n",
    "        ax2.text(x, 2, layer, ha='center', va='center', fontweight='bold')\n",
    "        \n",
    "        # Add arrows for backward pass\n",
    "        if i > 0:\n",
    "            ax2.arrow(x-0.3, 2, -1.4, 0, head_width=0.1, head_length=0.1, fc='red', ec='red')\n",
    "            ax2.text(x-1, 1.7, 'Backward', ha='center', color='red', fontweight='bold')\n",
    "    \n",
    "    # Add gradient computations\n",
    "    gradients = ['âˆ‚L/âˆ‚Wâ‚ƒ\\nâˆ‚L/âˆ‚bâ‚ƒ', 'âˆ‚L/âˆ‚Wâ‚‚\\nâˆ‚L/âˆ‚bâ‚‚', 'âˆ‚L/âˆ‚Wâ‚\\nâˆ‚L/âˆ‚bâ‚']\n",
    "    for i, grad in enumerate(gradients):\n",
    "        ax2.text(layer_x[3-i] - 1, 2.8, grad, ha='center', va='center', fontsize=10,\n",
    "                bbox=dict(boxstyle='round,pad=0.2', facecolor='lightcoral', alpha=0.7))\n",
    "    \n",
    "    # Loss function\n",
    "    ax2.text(6.5, 2, 'Loss\\nL(y, Å·)', ha='center', va='center', fontweight='bold',\n",
    "            bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.8))\n",
    "    \n",
    "    ax2.set_xlim(-1, 7.5)\n",
    "    ax2.set_ylim(0.5, 3.5)\n",
    "    ax2.set_title('Backward Pass: Computing Gradients for Learning', fontweight='bold')\n",
    "    ax2.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Run the visualization\n",
    "create_backpropagation_visual()\n",
    "print(\"âœ… Backpropagation visualization completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "summary"
   },
   "source": [
    "## ğŸ“š 6. Comprehensive Summary\n",
    "\n",
    "Key takeaways from today's lesson:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create-summary"
   },
   "outputs": [],
   "source": [
    "def create_summary_slide():\n",
    "    \"\"\"Create a comprehensive summary slide\"\"\"\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(16, 12))\n",
    "    fig.suptitle('ğŸ“š Week 3 Day 3: Neural Network Fundamentals Summary', fontsize=20, fontweight='bold')\n",
    "    \n",
    "    # Remove axes\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Create summary content\n",
    "    summary_content = \"\"\"\n",
    "ğŸ­ ACTIVATION FUNCTIONS MASTERY\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ Classic Functions:                                              â”‚\n",
    "â”‚ â€¢ Sigmoid: Ïƒ(x) = 1/(1+eâ»Ë£) â†’ Range: (0,1) â†’ Binary outputs   â”‚\n",
    "â”‚ â€¢ Tanh: (eË£-eâ»Ë£)/(eË£+eâ»Ë£) â†’ Range: (-1,1) â†’ Zero-centered     â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â”‚ Modern Powerhouses:                                             â”‚\n",
    "â”‚ â€¢ ReLU: f(x) = max(0,x) â†’ Simple, fast, no saturation         â”‚\n",
    "â”‚ â€¢ Leaky ReLU: f(x) = max(Î±x,x) â†’ Prevents dying neurons       â”‚\n",
    "â”‚ â€¢ ELU: Smooth, negative saturation â†’ Better gradient flow      â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "ğŸ§  NEURAL LAYER MATHEMATICS\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ Forward Pass: aâ½Ë¡â¾ = Ïƒ(Wâ½Ë¡â¾aâ½Ë¡â»Â¹â¾ + bâ½Ë¡â¾)                      â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â”‚ Key Components:                                                 â”‚\n",
    "â”‚ â€¢ Weight Matrix W: Learnable parameters                        â”‚\n",
    "â”‚ â€¢ Bias Vector b: Shifts the activation                         â”‚\n",
    "â”‚ â€¢ Activation Ïƒ: Introduces non-linearity                       â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â”‚ Initialization Matters:                                         â”‚\n",
    "â”‚ â€¢ Xavier/Glorot: âˆš(2/(fan_in + fan_out))                      â”‚\n",
    "â”‚ â€¢ He: âˆš(2/fan_in) â†’ Better for ReLU                           â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "ğŸ”„ BACKPROPAGATION FRAMEWORK\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ Chain Rule Application:                                         â”‚\n",
    "â”‚ âˆ‚L/âˆ‚Wâ½Ë¡â¾ = âˆ‚L/âˆ‚aâ½Ë¡â¾ Â· âˆ‚aâ½Ë¡â¾/âˆ‚zâ½Ë¡â¾ Â· âˆ‚zâ½Ë¡â¾/âˆ‚Wâ½Ë¡â¾                   â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â”‚ Gradient Flow:                                                  â”‚\n",
    "â”‚ â€¢ Good activations â†’ Stable gradients                          â”‚\n",
    "â”‚ â€¢ Bad activations â†’ Vanishing/exploding gradients              â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â”‚ Learning Update:                                                â”‚\n",
    "â”‚ Wâ½Ë¡â¾ â† Wâ½Ë¡â¾ - Î· Â· âˆ‚L/âˆ‚Wâ½Ë¡â¾                                    â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "ğŸ¯ TOMORROW'S PREVIEW: Hands-On Implementation\n",
    "â€¢ Build custom activation functions from scratch\n",
    "â€¢ Implement neural layers with TensorFlow\n",
    "â€¢ Create complete neural networks\n",
    "â€¢ Test different activation choices practically\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add the text content\n",
    "    ax.text(0.05, 0.95, summary_content, fontsize=12, fontfamily='monospace',\n",
    "            verticalalignment='top', transform=ax.transAxes)\n",
    "    \n",
    "    # Add decorative elements\n",
    "    from matplotlib.patches import FancyBboxPatch\n",
    "    \n",
    "    # Background box\n",
    "    fancy_box = FancyBboxPatch((0.02, 0.02), 0.96, 0.96, boxstyle=\"round,pad=0.02\",\n",
    "                              facecolor='lightblue', alpha=0.1, transform=ax.transAxes)\n",
    "    ax.add_patch(fancy_box)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Run the summary\n",
    "create_summary_slide()\n",
    "print(\"âœ… Summary slide completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "conclusion"
   },
   "source": [
    "## ğŸ¯ Interactive Exercise\n",
    "\n",
    "Try modifying the activation functions or parameters in the cells above and re-run them to see how the visualizations change!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "interactive-exercise"
   },
   "outputs": [],
   "source": [
    "# ğŸ§ª EXPERIMENT: Try different activation function parameters\n",
    "print(\"ğŸ§ª Interactive Experiment Section\")\n",
    "print(\"=================================\")\n",
    "\n",
    "# Example: Compare different leaky ReLU alpha values\n",
    "x = np.linspace(-5, 5, 1000)\n",
    "alphas = [0.01, 0.1, 0.3]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "for alpha in alphas:\n",
    "    y = leaky_relu(x, alpha)\n",
    "    plt.plot(x, y, linewidth=3, label=f'Leaky ReLU (Î±={alpha})')\n",
    "\n",
    "plt.title('ğŸ”¬ Experiment: Different Leaky ReLU Parameters', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "plt.axvline(x=0, color='k', linestyle='-', alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ¯ Try This:\")\n",
    "print(\"â€¢ Change the alpha values above\")\n",
    "print(\"â€¢ Modify the x range\")\n",
    "print(\"â€¢ Add your own custom activation function\")\n",
    "print(\"â€¢ Experiment with different parameters!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "final-notes"
   },
   "source": [
    "---\n",
    "\n",
    "## ğŸ“– Course Information\n",
    "\n",
    "**Course**: Deep Neural Network Architectures (21CSE558T)  \n",
    "**Credits**: 3 (2L + 1T + 0P)  \n",
    "**Duration**: 15 weeks  \n",
    "**Institution**: SRM University, DSBS  \n",
    "\n",
    "**Next Session**: Hands-on implementation of neural networks with TensorFlow/Keras\n",
    "\n",
    "---\n",
    "\n",
    "ğŸ’¡ **Pro Tip**: Save this notebook to your Google Drive and experiment with the parameters between classes!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}