{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# 🎭 Activation Function Visualizations - Week 3 Day 3\n",
    "\n",
    "**Deep Neural Network Architectures (21CSE558T)**\n",
    "\n",
    "© 2025 Prof. Ramesh Babu | SRM University | DSBS\n",
    "\n",
    "This notebook generates comprehensive visualizations for understanding:\n",
    "- Activation functions and their derivatives\n",
    "- Gradient flow problems\n",
    "- Neural network architecture\n",
    "- Weight initialization strategies\n",
    "- Backpropagation mechanics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## 📦 Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imports"
   },
   "outputs": [],
   "source": "import numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib.patches import Rectangle, FancyBboxPatch\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set up beautiful plotting\nplt.style.use('default')\nsns.set_palette(\"husl\")\nplt.rcParams['figure.figsize'] = (12, 8)\nplt.rcParams['font.size'] = 12\nplt.rcParams['axes.linewidth'] = 1.5\nplt.rcParams['grid.alpha'] = 0.3\n\nprint(\"✅ Libraries imported successfully!\")\nprint(f\"📊 NumPy version: {np.__version__}\")\nprint(f\"📈 Matplotlib version: {matplotlib.__version__}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "helper-functions"
   },
   "source": [
    "## 🔧 Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "activation-functions"
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"Sigmoid activation function\"\"\"\n",
    "    return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    \"\"\"Sigmoid derivative\"\"\"\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    \"\"\"Tanh derivative\"\"\"\n",
    "    t = np.tanh(x)\n",
    "    return 1 - t**2\n",
    "\n",
    "def relu(x):\n",
    "    \"\"\"ReLU activation function\"\"\"\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    \"\"\"ReLU derivative\"\"\"\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "def leaky_relu(x, alpha=0.01):\n",
    "    \"\"\"Leaky ReLU activation function\"\"\"\n",
    "    return np.where(x > 0, x, alpha * x)\n",
    "\n",
    "def leaky_relu_derivative(x, alpha=0.01):\n",
    "    \"\"\"Leaky ReLU derivative\"\"\"\n",
    "    return np.where(x > 0, 1, alpha)\n",
    "\n",
    "print(\"🔧 Activation functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "activation-comparison"
   },
   "source": [
    "## 🎭 1. Activation Functions Comparison\n",
    "\n",
    "Let's visualize the classical activation functions and understand their properties:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create-activation-comparison"
   },
   "outputs": [],
   "source": [
    "def create_activation_comparison():\n",
    "    \"\"\"Create comprehensive activation function comparison\"\"\"\n",
    "    x = np.linspace(-5, 5, 1000)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle('🎭 Classical Activation Functions & Their Derivatives', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Sigmoid\n",
    "    axes[0, 0].plot(x, sigmoid(x), 'purple', linewidth=3, label='Sigmoid')\n",
    "    axes[0, 0].plot(x, sigmoid_derivative(x), 'purple', linestyle='--', linewidth=2, label='Sigmoid Derivative')\n",
    "    axes[0, 0].set_title('Sigmoid: σ(x) = 1/(1 + e⁻ˣ)', fontweight='bold')\n",
    "    axes[0, 0].set_ylabel('f(x)')\n",
    "    axes[0, 0].grid(True)\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "    axes[0, 0].axvline(x=0, color='k', linestyle='-', alpha=0.3)\n",
    "    \n",
    "    # Add annotations for sigmoid\n",
    "    axes[0, 0].annotate('Saturation\\n(Vanishing Gradients)', xy=(-4, 0.02), xytext=(-3, 0.3),\n",
    "                       arrowprops=dict(arrowstyle='->', color='red'), fontsize=10, ha='center',\n",
    "                       bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.7))\n",
    "    \n",
    "    # Tanh\n",
    "    axes[0, 1].plot(x, np.tanh(x), 'teal', linewidth=3, label='Tanh')\n",
    "    axes[0, 1].plot(x, tanh_derivative(x), 'teal', linestyle='--', linewidth=2, label='Tanh Derivative')\n",
    "    axes[0, 1].set_title('Tanh: (eˣ - e⁻ˣ)/(eˣ + e⁻ˣ)', fontweight='bold')\n",
    "    axes[0, 1].grid(True)\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "    axes[0, 1].axvline(x=0, color='k', linestyle='-', alpha=0.3)\n",
    "    \n",
    "    # Add annotations for tanh\n",
    "    axes[0, 1].annotate('Zero-centered\\n(Better than Sigmoid)', xy=(0, 0), xytext=(2, -0.7),\n",
    "                       arrowprops=dict(arrowstyle='->', color='green'), fontsize=10, ha='center',\n",
    "                       bbox=dict(boxstyle='round,pad=0.3', facecolor='lightgreen', alpha=0.7))\n",
    "    \n",
    "    # ReLU\n",
    "    axes[1, 0].plot(x, relu(x), 'red', linewidth=3, label='ReLU')\n",
    "    axes[1, 0].plot(x, relu_derivative(x), 'red', linestyle='--', linewidth=2, label='ReLU Derivative')\n",
    "    axes[1, 0].set_title('ReLU: f(x) = max(0, x)', fontweight='bold')\n",
    "    axes[1, 0].set_xlabel('x')\n",
    "    axes[1, 0].set_ylabel('f(x)')\n",
    "    axes[1, 0].grid(True)\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "    axes[1, 0].axvline(x=0, color='k', linestyle='-', alpha=0.3)\n",
    "    \n",
    "    # Add annotations for ReLU\n",
    "    axes[1, 0].annotate('Dead Neurons\\n(x < 0)', xy=(-2, 0), xytext=(-3, 2),\n",
    "                       arrowprops=dict(arrowstyle='->', color='red'), fontsize=10, ha='center',\n",
    "                       bbox=dict(boxstyle='round,pad=0.3', facecolor='orange', alpha=0.7))\n",
    "    \n",
    "    # Leaky ReLU\n",
    "    axes[1, 1].plot(x, leaky_relu(x), 'orange', linewidth=3, label='Leaky ReLU')\n",
    "    axes[1, 1].plot(x, leaky_relu_derivative(x), 'orange', linestyle='--', linewidth=2, label='Leaky ReLU Derivative')\n",
    "    axes[1, 1].set_title('Leaky ReLU: f(x) = max(0.01x, x)', fontweight='bold')\n",
    "    axes[1, 1].set_xlabel('x')\n",
    "    axes[1, 1].grid(True)\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "    axes[1, 1].axvline(x=0, color='k', linestyle='-', alpha=0.3)\n",
    "    \n",
    "    # Add annotations for Leaky ReLU\n",
    "    axes[1, 1].annotate('Small gradient\\n(Prevents dying)', xy=(-2, -0.02), xytext=(-3, 1.5),\n",
    "                       arrowprops=dict(arrowstyle='->', color='green'), fontsize=10, ha='center',\n",
    "                       bbox=dict(boxstyle='round,pad=0.3', facecolor='lightblue', alpha=0.7))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Run the visualization\n",
    "create_activation_comparison()\n",
    "print(\"✅ Activation functions comparison completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gradient-flow"
   },
   "source": [
    "## 🌊 2. Gradient Flow Analysis\n",
    "\n",
    "Understanding the vanishing gradient problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create-gradient-flow"
   },
   "outputs": [],
   "source": [
    "def create_gradient_flow_demo():\n",
    "    \"\"\"Demonstrate vanishing gradient problem\"\"\"\n",
    "    x = np.linspace(-5, 5, 1000)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    fig.suptitle('🌊 Gradient Flow Analysis: Why ReLU Won the Battle', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Sigmoid gradients\n",
    "    axes[0].plot(x, sigmoid_derivative(x), 'purple', linewidth=3, label='Sigmoid Gradient')\n",
    "    axes[0].fill_between(x, 0, sigmoid_derivative(x), alpha=0.3, color='purple')\n",
    "    axes[0].set_title('Sigmoid Gradients\\n(Vanishing Problem)', fontweight='bold')\n",
    "    axes[0].set_ylabel('Gradient Magnitude')\n",
    "    axes[0].grid(True)\n",
    "    axes[0].axhline(y=0.25, color='red', linestyle='--', alpha=0.7, label='Max Gradient = 0.25')\n",
    "    axes[0].legend()\n",
    "    \n",
    "    # Tanh gradients\n",
    "    axes[1].plot(x, tanh_derivative(x), 'teal', linewidth=3, label='Tanh Gradient')\n",
    "    axes[1].fill_between(x, 0, tanh_derivative(x), alpha=0.3, color='teal')\n",
    "    axes[1].set_title('Tanh Gradients\\n(Better, but still saturates)', fontweight='bold')\n",
    "    axes[1].set_xlabel('x')\n",
    "    axes[1].grid(True)\n",
    "    axes[1].axhline(y=1.0, color='red', linestyle='--', alpha=0.7, label='Max Gradient = 1.0')\n",
    "    axes[1].legend()\n",
    "    \n",
    "    # ReLU gradients\n",
    "    relu_grad = relu_derivative(x)\n",
    "    axes[2].plot(x, relu_grad, 'red', linewidth=3, label='ReLU Gradient')\n",
    "    axes[2].fill_between(x, 0, relu_grad, alpha=0.3, color='red')\n",
    "    axes[2].set_title('ReLU Gradients\\n(No Saturation!)', fontweight='bold')\n",
    "    axes[2].grid(True)\n",
    "    axes[2].set_ylim(-0.1, 1.2)\n",
    "    axes[2].axhline(y=1.0, color='green', linestyle='--', alpha=0.7, label='Constant Gradient = 1.0')\n",
    "    axes[2].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Run the visualization\n",
    "create_gradient_flow_demo()\n",
    "print(\"✅ Gradient flow analysis completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "neural-architecture"
   },
   "source": [
    "## 🧠 3. Neural Network Architecture\n",
    "\n",
    "Visualizing layer structure and matrix operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create-neural-architecture"
   },
   "outputs": [],
   "source": [
    "def create_neural_layer_architecture():\n",
    "    \"\"\"Visualize neural network layer architecture with matrix operations\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "    fig.suptitle('🧠 Neural Network Layer Architecture & Matrix Operations', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Layer architecture diagram\n",
    "    ax1 = axes[0]\n",
    "    \n",
    "    # Input layer\n",
    "    input_y = [1, 2, 3]\n",
    "    for i, y in enumerate(input_y):\n",
    "        circle = plt.Circle((1, y), 0.3, color='lightblue', ec='black', linewidth=2)\n",
    "        ax1.add_patch(circle)\n",
    "        ax1.text(1, y, f'x{i+1}', ha='center', va='center', fontweight='bold')\n",
    "    \n",
    "    ax1.text(1, 0.2, 'Input Layer\\n(3 neurons)', ha='center', fontweight='bold')\n",
    "    \n",
    "    # Hidden layer\n",
    "    hidden_y = [0.5, 1.5, 2.5, 3.5]\n",
    "    for i, y in enumerate(hidden_y):\n",
    "        circle = plt.Circle((3, y), 0.3, color='lightgreen', ec='black', linewidth=2)\n",
    "        ax1.add_patch(circle)\n",
    "        ax1.text(3, y, f'h{i+1}', ha='center', va='center', fontweight='bold')\n",
    "        # Add activation function symbol\n",
    "        ax1.text(3.5, y, 'σ', ha='center', va='center', fontsize=16, color='red', fontweight='bold')\n",
    "    \n",
    "    ax1.text(3, -0.3, 'Hidden Layer\\n(4 neurons)', ha='center', fontweight='bold')\n",
    "    \n",
    "    # Output layer\n",
    "    output_y = [1.5, 2.5]\n",
    "    for i, y in enumerate(output_y):\n",
    "        circle = plt.Circle((5, y), 0.3, color='lightcoral', ec='black', linewidth=2)\n",
    "        ax1.add_patch(circle)\n",
    "        ax1.text(5, y, f'y{i+1}', ha='center', va='center', fontweight='bold')\n",
    "    \n",
    "    ax1.text(5, 0.7, 'Output Layer\\n(2 neurons)', ha='center', fontweight='bold')\n",
    "    \n",
    "    # Connections\n",
    "    for i in input_y:\n",
    "        for j in hidden_y:\n",
    "            ax1.plot([1.3, 2.7], [i, j], 'k-', alpha=0.3, linewidth=1)\n",
    "    \n",
    "    for i in hidden_y:\n",
    "        for j in output_y:\n",
    "            ax1.plot([3.3, 4.7], [i, j], 'k-', alpha=0.3, linewidth=1)\n",
    "    \n",
    "    # Weight matrices labels\n",
    "    ax1.text(2, 4, 'W₁ (3×4)\\nb₁ (4×1)', ha='center', va='center', fontsize=12,\n",
    "             bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.7))\n",
    "    ax1.text(4, 4, 'W₂ (4×2)\\nb₂ (2×1)', ha='center', va='center', fontsize=12,\n",
    "             bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.7))\n",
    "    \n",
    "    ax1.set_xlim(0, 6)\n",
    "    ax1.set_ylim(-0.5, 4.5)\n",
    "    ax1.set_aspect('equal')\n",
    "    ax1.axis('off')\n",
    "    ax1.set_title('Network Architecture', fontweight='bold')\n",
    "    \n",
    "    # Matrix operations visualization\n",
    "    ax2 = axes[1]\n",
    "    \n",
    "    # Create matrix visualization\n",
    "    ax2.text(0.5, 0.9, 'Forward Pass Mathematics:', fontsize=14, fontweight='bold', transform=ax2.transAxes)\n",
    "    \n",
    "    equations = [\n",
    "        'Layer 1: z₁ = x·W₁ + b₁',\n",
    "        '         a₁ = σ(z₁)',\n",
    "        '',\n",
    "        'Layer 2: z₂ = a₁·W₂ + b₂', \n",
    "        '         a₂ = σ(z₂)',\n",
    "        '',\n",
    "        'Matrix Dimensions:',\n",
    "        '• Input: (batch_size, 3)',\n",
    "        '• W₁: (3, 4), b₁: (4,)',\n",
    "        '• Hidden: (batch_size, 4)',\n",
    "        '• W₂: (4, 2), b₂: (2,)',\n",
    "        '• Output: (batch_size, 2)'\n",
    "    ]\n",
    "    \n",
    "    for i, eq in enumerate(equations):\n",
    "        ax2.text(0.05, 0.8 - i*0.06, eq, fontsize=12, transform=ax2.transAxes,\n",
    "                fontweight='bold' if '•' in eq or 'Layer' in eq else 'normal',\n",
    "                color='red' if 'σ' in eq else 'black')\n",
    "    \n",
    "    # Add activation function examples\n",
    "    ax2.text(0.05, 0.25, 'Common Activation Choices:', fontsize=12, fontweight='bold', transform=ax2.transAxes)\n",
    "    ax2.text(0.05, 0.20, '• Hidden layers: ReLU, Tanh, Leaky ReLU', fontsize=11, transform=ax2.transAxes)\n",
    "    ax2.text(0.05, 0.15, '• Binary output: Sigmoid', fontsize=11, transform=ax2.transAxes)\n",
    "    ax2.text(0.05, 0.10, '• Multi-class: Softmax', fontsize=11, transform=ax2.transAxes)\n",
    "    ax2.text(0.05, 0.05, '• Regression: Linear (no activation)', fontsize=11, transform=ax2.transAxes)\n",
    "    \n",
    "    ax2.axis('off')\n",
    "    ax2.set_title('Mathematical Operations', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Run the visualization\n",
    "create_neural_layer_architecture()\n",
    "print(\"✅ Neural layer architecture completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "initialization"
   },
   "source": [
    "## ⚖️ 4. Weight Initialization Strategies\n",
    "\n",
    "Comparing different weight initialization methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create-initialization"
   },
   "outputs": [],
   "source": [
    "def create_initialization_demo():\n",
    "    \"\"\"Demonstrate weight initialization strategies\"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    fig.suptitle('⚖️ Weight Initialization Strategies & Their Impact', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    n_inputs, n_outputs = 100, 50\n",
    "    n_samples = 1000\n",
    "    \n",
    "    # Different initialization strategies\n",
    "    strategies = [\n",
    "        ('Random Normal (σ=1)', np.random.normal(0, 1, (n_inputs, n_outputs))),\n",
    "        ('Random Normal (σ=0.01)', np.random.normal(0, 0.01, (n_inputs, n_outputs))),\n",
    "        ('Xavier/Glorot', np.random.normal(0, np.sqrt(2.0/(n_inputs + n_outputs)), (n_inputs, n_outputs))),\n",
    "        ('He Initialization', np.random.normal(0, np.sqrt(2.0/n_inputs), (n_inputs, n_outputs))),\n",
    "        ('Random Uniform [-1,1]', np.random.uniform(-1, 1, (n_inputs, n_outputs))),\n",
    "        ('Zeros (Bad!)', np.zeros((n_inputs, n_outputs)))\n",
    "    ]\n",
    "    \n",
    "    # Simulate forward pass with different initializations\n",
    "    input_data = np.random.normal(0, 1, (n_samples, n_inputs))\n",
    "    \n",
    "    for idx, (name, weights) in enumerate(strategies):\n",
    "        row = idx // 3\n",
    "        col = idx % 3\n",
    "        ax = axes[row, col]\n",
    "        \n",
    "        # Forward pass\n",
    "        output = np.dot(input_data, weights)\n",
    "        \n",
    "        # Apply ReLU activation\n",
    "        activated_output = np.maximum(0, output)\n",
    "        \n",
    "        # Plot distribution of activations\n",
    "        ax.hist(activated_output.flatten(), bins=50, alpha=0.7, density=True, color=plt.cm.Set3(idx))\n",
    "        ax.set_title(f'{name}\\n(Mean: {activated_output.mean():.3f}, Std: {activated_output.std():.3f})', \n",
    "                    fontweight='bold')\n",
    "        ax.set_xlabel('Activation Values')\n",
    "        ax.set_ylabel('Density')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add annotations for good/bad cases\n",
    "        if 'Xavier' in name or 'He' in name:\n",
    "            ax.text(0.02, 0.95, '✅ Good!', transform=ax.transAxes, fontsize=12, \n",
    "                   bbox=dict(boxstyle='round,pad=0.3', facecolor='lightgreen', alpha=0.8))\n",
    "        elif 'Zeros' in name or 'σ=1' in name:\n",
    "            ax.text(0.02, 0.95, '❌ Bad!', transform=ax.transAxes, fontsize=12,\n",
    "                   bbox=dict(boxstyle='round,pad=0.3', facecolor='lightcoral', alpha=0.8))\n",
    "        else:\n",
    "            ax.text(0.02, 0.95, '⚠️ Okay', transform=ax.transAxes, fontsize=12,\n",
    "                   bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Run the visualization\n",
    "create_initialization_demo()\n",
    "print(\"✅ Initialization strategies demonstration completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "backpropagation"
   },
   "source": [
    "## 🔄 5. Backpropagation Visualization\n",
    "\n",
    "Understanding the learning mechanism:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create-backpropagation"
   },
   "outputs": [],
   "source": [
    "def create_backpropagation_visual():\n",
    "    \"\"\"Create backpropagation visualization\"\"\"\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(16, 12))\n",
    "    fig.suptitle('🔄 Backpropagation: The Learning Engine', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Forward pass visualization\n",
    "    ax1 = axes[0]\n",
    "    \n",
    "    # Network layers\n",
    "    layers = ['Input', 'Hidden 1', 'Hidden 2', 'Output']\n",
    "    layer_x = [0, 2, 4, 6]\n",
    "    \n",
    "    for i, (layer, x) in enumerate(zip(layers, layer_x)):\n",
    "        # Draw layer\n",
    "        rect = Rectangle((x-0.3, 1), 0.6, 2, facecolor=plt.cm.Set3(i), alpha=0.7, ec='black')\n",
    "        ax1.add_patch(rect)\n",
    "        ax1.text(x, 2, layer, ha='center', va='center', fontweight='bold')\n",
    "        \n",
    "        # Add arrows for forward pass\n",
    "        if i < len(layers) - 1:\n",
    "            ax1.arrow(x+0.3, 2, 1.4, 0, head_width=0.1, head_length=0.1, fc='blue', ec='blue')\n",
    "            ax1.text(x+1, 2.3, 'Forward', ha='center', color='blue', fontweight='bold')\n",
    "    \n",
    "    # Add mathematical operations\n",
    "    operations = ['W₁·x + b₁\\nσ(z₁)', 'W₂·a₁ + b₂\\nσ(z₂)', 'W₃·a₂ + b₃\\nσ(z₃)']\n",
    "    for i, op in enumerate(operations):\n",
    "        ax1.text(layer_x[i] + 1, 1.2, op, ha='center', va='center', fontsize=10,\n",
    "                bbox=dict(boxstyle='round,pad=0.2', facecolor='lightblue', alpha=0.7))\n",
    "    \n",
    "    ax1.set_xlim(-1, 7)\n",
    "    ax1.set_ylim(0.5, 3.5)\n",
    "    ax1.set_title('Forward Pass: Computing Predictions', fontweight='bold')\n",
    "    ax1.axis('off')\n",
    "    \n",
    "    # Backward pass visualization\n",
    "    ax2 = axes[1]\n",
    "    \n",
    "    for i, (layer, x) in enumerate(zip(layers, layer_x)):\n",
    "        # Draw layer\n",
    "        rect = Rectangle((x-0.3, 1), 0.6, 2, facecolor=plt.cm.Set3(i), alpha=0.7, ec='black')\n",
    "        ax2.add_patch(rect)\n",
    "        ax2.text(x, 2, layer, ha='center', va='center', fontweight='bold')\n",
    "        \n",
    "        # Add arrows for backward pass\n",
    "        if i > 0:\n",
    "            ax2.arrow(x-0.3, 2, -1.4, 0, head_width=0.1, head_length=0.1, fc='red', ec='red')\n",
    "            ax2.text(x-1, 1.7, 'Backward', ha='center', color='red', fontweight='bold')\n",
    "    \n",
    "    # Add gradient computations\n",
    "    gradients = ['∂L/∂W₃\\n∂L/∂b₃', '∂L/∂W₂\\n∂L/∂b₂', '∂L/∂W₁\\n∂L/∂b₁']\n",
    "    for i, grad in enumerate(gradients):\n",
    "        ax2.text(layer_x[3-i] - 1, 2.8, grad, ha='center', va='center', fontsize=10,\n",
    "                bbox=dict(boxstyle='round,pad=0.2', facecolor='lightcoral', alpha=0.7))\n",
    "    \n",
    "    # Loss function\n",
    "    ax2.text(6.5, 2, 'Loss\\nL(y, ŷ)', ha='center', va='center', fontweight='bold',\n",
    "            bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.8))\n",
    "    \n",
    "    ax2.set_xlim(-1, 7.5)\n",
    "    ax2.set_ylim(0.5, 3.5)\n",
    "    ax2.set_title('Backward Pass: Computing Gradients for Learning', fontweight='bold')\n",
    "    ax2.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Run the visualization\n",
    "create_backpropagation_visual()\n",
    "print(\"✅ Backpropagation visualization completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "summary"
   },
   "source": [
    "## 📚 6. Comprehensive Summary\n",
    "\n",
    "Key takeaways from today's lesson:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create-summary"
   },
   "outputs": [],
   "source": [
    "def create_summary_slide():\n",
    "    \"\"\"Create a comprehensive summary slide\"\"\"\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(16, 12))\n",
    "    fig.suptitle('📚 Week 3 Day 3: Neural Network Fundamentals Summary', fontsize=20, fontweight='bold')\n",
    "    \n",
    "    # Remove axes\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Create summary content\n",
    "    summary_content = \"\"\"\n",
    "🎭 ACTIVATION FUNCTIONS MASTERY\n",
    "┌─────────────────────────────────────────────────────────────────┐\n",
    "│ Classic Functions:                                              │\n",
    "│ • Sigmoid: σ(x) = 1/(1+e⁻ˣ) → Range: (0,1) → Binary outputs   │\n",
    "│ • Tanh: (eˣ-e⁻ˣ)/(eˣ+e⁻ˣ) → Range: (-1,1) → Zero-centered     │\n",
    "│                                                                 │\n",
    "│ Modern Powerhouses:                                             │\n",
    "│ • ReLU: f(x) = max(0,x) → Simple, fast, no saturation         │\n",
    "│ • Leaky ReLU: f(x) = max(αx,x) → Prevents dying neurons       │\n",
    "│ • ELU: Smooth, negative saturation → Better gradient flow      │\n",
    "└─────────────────────────────────────────────────────────────────┘\n",
    "\n",
    "🧠 NEURAL LAYER MATHEMATICS\n",
    "┌─────────────────────────────────────────────────────────────────┐\n",
    "│ Forward Pass: a⁽ˡ⁾ = σ(W⁽ˡ⁾a⁽ˡ⁻¹⁾ + b⁽ˡ⁾)                      │\n",
    "│                                                                 │\n",
    "│ Key Components:                                                 │\n",
    "│ • Weight Matrix W: Learnable parameters                        │\n",
    "│ • Bias Vector b: Shifts the activation                         │\n",
    "│ • Activation σ: Introduces non-linearity                       │\n",
    "│                                                                 │\n",
    "│ Initialization Matters:                                         │\n",
    "│ • Xavier/Glorot: √(2/(fan_in + fan_out))                      │\n",
    "│ • He: √(2/fan_in) → Better for ReLU                           │\n",
    "└─────────────────────────────────────────────────────────────────┘\n",
    "\n",
    "🔄 BACKPROPAGATION FRAMEWORK\n",
    "┌─────────────────────────────────────────────────────────────────┐\n",
    "│ Chain Rule Application:                                         │\n",
    "│ ∂L/∂W⁽ˡ⁾ = ∂L/∂a⁽ˡ⁾ · ∂a⁽ˡ⁾/∂z⁽ˡ⁾ · ∂z⁽ˡ⁾/∂W⁽ˡ⁾                   │\n",
    "│                                                                 │\n",
    "│ Gradient Flow:                                                  │\n",
    "│ • Good activations → Stable gradients                          │\n",
    "│ • Bad activations → Vanishing/exploding gradients              │\n",
    "│                                                                 │\n",
    "│ Learning Update:                                                │\n",
    "│ W⁽ˡ⁾ ← W⁽ˡ⁾ - η · ∂L/∂W⁽ˡ⁾                                    │\n",
    "└─────────────────────────────────────────────────────────────────┘\n",
    "\n",
    "🎯 TOMORROW'S PREVIEW: Hands-On Implementation\n",
    "• Build custom activation functions from scratch\n",
    "• Implement neural layers with TensorFlow\n",
    "• Create complete neural networks\n",
    "• Test different activation choices practically\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add the text content\n",
    "    ax.text(0.05, 0.95, summary_content, fontsize=12, fontfamily='monospace',\n",
    "            verticalalignment='top', transform=ax.transAxes)\n",
    "    \n",
    "    # Add decorative elements\n",
    "    from matplotlib.patches import FancyBboxPatch\n",
    "    \n",
    "    # Background box\n",
    "    fancy_box = FancyBboxPatch((0.02, 0.02), 0.96, 0.96, boxstyle=\"round,pad=0.02\",\n",
    "                              facecolor='lightblue', alpha=0.1, transform=ax.transAxes)\n",
    "    ax.add_patch(fancy_box)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Run the summary\n",
    "create_summary_slide()\n",
    "print(\"✅ Summary slide completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "conclusion"
   },
   "source": [
    "## 🎯 Interactive Exercise\n",
    "\n",
    "Try modifying the activation functions or parameters in the cells above and re-run them to see how the visualizations change!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "interactive-exercise"
   },
   "outputs": [],
   "source": [
    "# 🧪 EXPERIMENT: Try different activation function parameters\n",
    "print(\"🧪 Interactive Experiment Section\")\n",
    "print(\"=================================\")\n",
    "\n",
    "# Example: Compare different leaky ReLU alpha values\n",
    "x = np.linspace(-5, 5, 1000)\n",
    "alphas = [0.01, 0.1, 0.3]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "for alpha in alphas:\n",
    "    y = leaky_relu(x, alpha)\n",
    "    plt.plot(x, y, linewidth=3, label=f'Leaky ReLU (α={alpha})')\n",
    "\n",
    "plt.title('🔬 Experiment: Different Leaky ReLU Parameters', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "plt.axvline(x=0, color='k', linestyle='-', alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n🎯 Try This:\")\n",
    "print(\"• Change the alpha values above\")\n",
    "print(\"• Modify the x range\")\n",
    "print(\"• Add your own custom activation function\")\n",
    "print(\"• Experiment with different parameters!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "final-notes"
   },
   "source": [
    "---\n",
    "\n",
    "## 📖 Course Information\n",
    "\n",
    "**Course**: Deep Neural Network Architectures (21CSE558T)  \n",
    "**Credits**: 3 (2L + 1T + 0P)  \n",
    "**Duration**: 15 weeks  \n",
    "**Institution**: SRM University, DSBS  \n",
    "\n",
    "**Next Session**: Hands-on implementation of neural networks with TensorFlow/Keras\n",
    "\n",
    "---\n",
    "\n",
    "💡 **Pro Tip**: Save this notebook to your Google Drive and experiment with the parameters between classes!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}