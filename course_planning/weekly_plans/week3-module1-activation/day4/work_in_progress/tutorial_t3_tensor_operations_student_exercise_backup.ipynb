{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Tutorial T3: Building Programs to Perform Basic Operations in Tensors\n## Week 3, Day 4 - Deep Neural Network Architectures\n\n---\n\n## 📋 Student Information\n**Please fill in your details before starting:**\n\n| Field | Details |\n|-------|---------|\n| **Student Name** | `[ENTER YOUR NAME HERE]` |\n| **Registration Number** | `[ENTER YOUR REG NO HERE]` |\n| **Branch & Year** | `[e.g., M.Tech DSBS - 1st Year]` |\n| **Date of Submission** | `[ENTER DATE HERE]` |\n| **Lab Session** | `Week 3, Day 4 - Tutorial T3` |\n\n---\n\n**Duration:** 1 Hour | **Format:** Hands-On Tutorial\n\n### Learning Objectives\n- Implement custom activation functions from mathematical principles\n- Understand gradient computation for backpropagation\n- Master tensor operations in TensorFlow\n- Build a neural network layer from scratch\n- Construct a complete multi-layer network\n\n### 📝 Instructions for Students:\n1. **Fill in the student information table above** by replacing the placeholder text\n2. Work through each section systematically \n3. Replace all `# Your code here` and `pass` statements with your implementations\n4. Test your code using the provided test cells\n5. Run the unit tests to verify your implementations\n6. Save your completed notebook with filename: `T3_YourName_RegNo.ipynb`\n7. Submit the completed notebook by the deadline\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "source": "# 📋 STUDENT IDENTIFICATION - MANDATORY\n# Replace the placeholder values with your actual information\n\nSTUDENT_NAME = \"YOUR_NAME_HERE\"  # Replace with your full name\nSTUDENT_REG_NO = \"YOUR_REG_NO_HERE\"  # Replace with your registration number\nSTUDENT_BRANCH = \"M.Tech DSBS\"  # Update if different\nSUBMISSION_DATE = \"DATE_HERE\"  # Replace with actual date (YYYY-MM-DD)\n\n# Automatic validation and setup\nimport datetime\nimport hashlib\n\nif STUDENT_NAME == \"YOUR_NAME_HERE\" or STUDENT_REG_NO == \"YOUR_REG_NO_HERE\":\n    print(\"⚠️ WARNING: Please update your student information above!\")\n    print(\"   Update STUDENT_NAME and STUDENT_REG_NO variables\")\nelse:\n    print(\"✅ Notebook configured successfully!\")\n    print(f\"   Student: {STUDENT_NAME}\")\n    print(f\"   Registration: {STUDENT_REG_NO}\")\n    print(f\"   Branch: {STUDENT_BRANCH}\")\n    print(f\"   Date: {SUBMISSION_DATE}\")\n    \n    # Generate unique student verification code\n    student_hash = hashlib.md5(f\"{STUDENT_NAME}{STUDENT_REG_NO}\".encode()).hexdigest()[:8].upper()\n    print(f\"   Verification Code: {student_hash}\")\n    \n    # Auto-generate proper filename\n    import re\n    safe_name = re.sub(r'[^a-zA-Z0-9]', '_', STUDENT_NAME)\n    safe_reg = re.sub(r'[^a-zA-Z0-9]', '_', STUDENT_REG_NO)\n    SUGGESTED_FILENAME = f\"T3_{safe_name}_{safe_reg}.ipynb\"\n    print(f\"   📁 Save this notebook as: {SUGGESTED_FILENAME}\")\n\nprint(\"\\n\" + \"=\"*60)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Custom Activation Functions (20 minutes)\n",
    "\n",
    "### Task 1A: Implement Basic Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement the sigmoid activation function\n",
    "def sigmoid(x):\n",
    "    \"\"\"Sigmoid activation: σ(x) = 1/(1+e^(-x))\"\"\"\n",
    "    # Your code here\n",
    "    pass\n",
    "\n",
    "# TODO: Implement the tanh activation function\n",
    "def tanh_custom(x):  \n",
    "    \"\"\"Hyperbolic tangent: tanh(x) = (e^x - e^(-x))/(e^x + e^(-x))\"\"\"\n",
    "    # Your code here\n",
    "    pass\n",
    "\n",
    "# TODO: Implement the ReLU activation function\n",
    "def relu_custom(x):\n",
    "    \"\"\"ReLU activation: max(0,x)\"\"\"\n",
    "    # Your code here\n",
    "    pass\n",
    "\n",
    "# TODO: Implement the Leaky ReLU activation function\n",
    "def leaky_relu_custom(x, alpha=0.01):\n",
    "    \"\"\"Leaky ReLU: x if x>0, alpha*x otherwise\"\"\"\n",
    "    # Your code here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Test values\ntest_values = np.array([-2, -1, 0, 1, 2])\n\nprint(\"=\" * 60)\nprint(f\"ACTIVATION FUNCTION TEST RESULTS\")\nprint(f\"Student: {STUDENT_NAME} | Registration: {STUDENT_REG_NO}\")\nprint(f\"Test Date: {SUBMISSION_DATE}\")\nprint(\"=\" * 60)\n\nprint(f\"Input values: {test_values}\")\nprint(f\"Sigmoid results: {sigmoid(test_values)}\")\nprint(f\"Tanh results: {tanh_custom(test_values)}\")\nprint(f\"ReLU results: {relu_custom(test_values)}\")\nprint(f\"Leaky ReLU results: {leaky_relu_custom(test_values)}\")\n\nprint(f\"\\n✅ Test completed by: {STUDENT_NAME}\")\nprint(\"=\" * 60)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test values\n",
    "test_values = np.array([-2, -1, 0, 1, 2])\n",
    "\n",
    "print(\"Testing activation functions:\")\n",
    "print(f\"Input: {test_values}\")\n",
    "print(f\"Sigmoid: {sigmoid(test_values)}\")\n",
    "print(f\"Tanh: {tanh_custom(test_values)}\")\n",
    "print(f\"ReLU: {relu_custom(test_values)}\")\n",
    "print(f\"Leaky ReLU: {leaky_relu_custom(test_values)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1B: Gradient Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement gradient functions\n",
    "def sigmoid_gradient(x):\n",
    "    \"\"\"Sigmoid derivative: σ(x) * (1 - σ(x))\"\"\"\n",
    "    # Your code here\n",
    "    pass\n",
    "\n",
    "def tanh_gradient(x):  \n",
    "    \"\"\"Tanh derivative: 1 - tanh²(x)\"\"\"\n",
    "    # Your code here\n",
    "    pass\n",
    "\n",
    "def relu_gradient(x):\n",
    "    \"\"\"ReLU derivative: 1 if x>0, 0 otherwise\"\"\"\n",
    "    # Your code here\n",
    "    pass\n",
    "\n",
    "def leaky_relu_gradient(x, alpha=0.01):\n",
    "    \"\"\"Leaky ReLU derivative: 1 if x>0, alpha otherwise\"\"\"\n",
    "    # Your code here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "def plot_activation_and_gradient(func, grad_func, name):\n    \"\"\"Plot activation function and its gradient with student attribution\"\"\"\n    x = np.linspace(-5, 5, 100)\n    y = func(x)\n    dy = grad_func(x)\n    \n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n    \n    # Add student info to main title\n    fig.suptitle(f'{name} Analysis - {STUDENT_NAME} ({STUDENT_REG_NO})', \n                 fontsize=16, fontweight='bold')\n    \n    # Plot activation function\n    ax1.plot(x, y, 'b-', linewidth=2)\n    ax1.set_title(f'{name} Activation Function', fontsize=14)\n    ax1.set_xlabel('x')\n    ax1.set_ylabel('f(x)')\n    ax1.grid(True, alpha=0.3)\n    ax1.axhline(y=0, color='k', linewidth=0.5)\n    ax1.axvline(x=0, color='k', linewidth=0.5)\n    \n    # Plot gradient\n    ax2.plot(x, dy, 'r-', linewidth=2)\n    ax2.set_title(f'{name} Gradient', fontsize=14)\n    ax2.set_xlabel('x')\n    ax2.set_ylabel(\"f'(x)\")\n    ax2.grid(True, alpha=0.3)\n    ax2.axhline(y=0, color='k', linewidth=0.5)\n    ax2.axvline(x=0, color='k', linewidth=0.5)\n    \n    # Add student watermark in corner\n    if 'YOUR_NAME_HERE' not in STUDENT_NAME:\n        fig.text(0.99, 0.01, f'{STUDENT_NAME} | {SUBMISSION_DATE}', \n                 ha='right', va='bottom', fontsize=8, alpha=0.6,\n                 bbox=dict(boxstyle=\"round,pad=0.3\", facecolor='white', alpha=0.7))\n    \n    plt.tight_layout()\n    plt.show()\n    \n    print(f\"📊 Plot generated by: {STUDENT_NAME} ({STUDENT_REG_NO})\")\n\n# Generate plots with student attribution\nprint(\"=\" * 60)\nprint(f\"ACTIVATION FUNCTION VISUALIZATION\")\nprint(f\"Student: {STUDENT_NAME} | Registration: {STUDENT_REG_NO}\")\nprint(\"=\" * 60)\n\nplot_activation_and_gradient(sigmoid, sigmoid_gradient, 'Sigmoid')\nplot_activation_and_gradient(tanh_custom, tanh_gradient, 'Tanh')\nplot_activation_and_gradient(relu_custom, relu_gradient, 'ReLU')\nplot_activation_and_gradient(lambda x: leaky_relu_custom(x), \n                            lambda x: leaky_relu_gradient(x), 'Leaky ReLU')\n\nprint(f\"\\n✅ All visualizations completed by: {STUDENT_NAME}\")\nprint(\"=\" * 60)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_activation_and_gradient(func, grad_func, name):\n",
    "    x = np.linspace(-5, 5, 100)\n",
    "    y = func(x)\n",
    "    dy = grad_func(x)\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    ax1.plot(x, y, 'b-', linewidth=2)\n",
    "    ax1.set_title(f'{name} Activation', fontsize=14)\n",
    "    ax1.set_xlabel('x')\n",
    "    ax1.set_ylabel('f(x)')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.axhline(y=0, color='k', linewidth=0.5)\n",
    "    ax1.axvline(x=0, color='k', linewidth=0.5)\n",
    "    \n",
    "    ax2.plot(x, dy, 'r-', linewidth=2)\n",
    "    ax2.set_title(f'{name} Gradient', fontsize=14)\n",
    "    ax2.set_xlabel('x')\n",
    "    ax2.set_ylabel(\"f'(x)\")\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.axhline(y=0, color='k', linewidth=0.5)\n",
    "    ax2.axvline(x=0, color='k', linewidth=0.5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot all activation functions\n",
    "plot_activation_and_gradient(sigmoid, sigmoid_gradient, 'Sigmoid')\n",
    "plot_activation_and_gradient(tanh_custom, tanh_gradient, 'Tanh')\n",
    "plot_activation_and_gradient(relu_custom, relu_gradient, 'ReLU')\n",
    "plot_activation_and_gradient(lambda x: leaky_relu_custom(x), \n",
    "                            lambda x: leaky_relu_gradient(x), 'Leaky ReLU')"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# 1. Create tensors of different dimensions\nscalar = tf.constant(5.0)\nvector = tf.constant([1, 2, 3], dtype=tf.float32)\nmatrix = tf.constant([[1, 2], [3, 4]], dtype=tf.float32)\ntensor_3d = tf.constant([[[1, 2], [3, 4]], [[5, 6], [7, 8]]], dtype=tf.float32)\n\nprint(\"=\" * 60)\nprint(f\"TENSOR OPERATIONS ANALYSIS\")\nprint(f\"Student: {STUDENT_NAME} | Registration: {STUDENT_REG_NO}\")\nprint(f\"Analysis Date: {SUBMISSION_DATE}\")\nprint(\"=\" * 60)\n\nprint(\"📐 Tensor Shapes and Properties:\")\nprint(f\"Scalar: shape={scalar.shape}, value={scalar.numpy()}\")\nprint(f\"Vector: shape={vector.shape}, values={vector.numpy()}\")\nprint(f\"Matrix: shape={matrix.shape}\")\nprint(f\"3D Tensor: shape={tensor_3d.shape}\")\n\nprint(f\"\\n✅ Tensor analysis by: {STUDENT_NAME}\")\nprint(\"=\" * 60)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create tensors of different dimensions\n",
    "scalar = tf.constant(5.0)\n",
    "vector = tf.constant([1, 2, 3], dtype=tf.float32)\n",
    "matrix = tf.constant([[1, 2], [3, 4]], dtype=tf.float32)\n",
    "tensor_3d = tf.constant([[[1, 2], [3, 4]], [[5, 6], [7, 8]]], dtype=tf.float32)\n",
    "\n",
    "print(\"Tensor Shapes:\")\n",
    "print(f\"Scalar: {scalar.shape}\")\n",
    "print(f\"Vector: {vector.shape}\")\n",
    "print(f\"Matrix: {matrix.shape}\")\n",
    "print(f\"3D Tensor: {tensor_3d.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Matrix multiplication exercises\n",
    "A = tf.constant([[1, 2], [3, 4]], dtype=tf.float32)\n",
    "B = tf.constant([[5, 6], [7, 8]], dtype=tf.float32)\n",
    "\n",
    "# TODO: Perform element-wise multiplication\n",
    "element_wise = None  # Your code here\n",
    "\n",
    "# TODO: Perform matrix multiplication\n",
    "matrix_mult = None  # Your code here\n",
    "\n",
    "print(\"Element-wise multiplication:\")\n",
    "print(element_wise)\n",
    "print(\"\\nMatrix multiplication:\")\n",
    "print(matrix_mult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Broadcasting operations\n",
    "vec = tf.constant([1, 2], dtype=tf.float32)\n",
    "mat = tf.constant([[1, 2], [3, 4], [5, 6]], dtype=tf.float32)\n",
    "\n",
    "# TODO: Add vector to each row of matrix (broadcasting)\n",
    "broadcasted_add = None  # Your code here\n",
    "\n",
    "print(\"Original matrix:\")\n",
    "print(mat)\n",
    "print(\"\\nVector to add:\")\n",
    "print(vec)\n",
    "print(\"\\nResult after broadcasting:\")\n",
    "print(broadcasted_add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Shape manipulation\n",
    "original = tf.constant([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "# TODO: Reshape to 3x2\n",
    "reshaped = None  # Your code here\n",
    "\n",
    "# TODO: Flatten to 1D\n",
    "flattened = None  # Your code here\n",
    "\n",
    "# TODO: Transpose the matrix\n",
    "transposed = None  # Your code here\n",
    "\n",
    "print(f\"Original shape: {original.shape}\")\n",
    "print(f\"Reshaped (3x2): {reshaped.shape if reshaped is not None else 'Not implemented'}\")\n",
    "print(f\"Flattened: {flattened.shape if flattened is not None else 'Not implemented'}\")\n",
    "print(f\"Transposed: {transposed.shape if transposed is not None else 'Not implemented'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2B: Dense Layer from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleDenseLayer:\n",
    "    def __init__(self, input_dim, output_dim, activation='relu'):\n",
    "        \"\"\"Initialize a dense layer with weights, bias, and activation\"\"\"\n",
    "        # TODO: Initialize weights using Xavier/Glorot initialization\n",
    "        self.weights = None  # Your code here\n",
    "        \n",
    "        # TODO: Initialize bias as zeros\n",
    "        self.bias = None  # Your code here\n",
    "        \n",
    "        # Store activation function name\n",
    "        self.activation_name = activation\n",
    "        \n",
    "        # Activation function dictionary\n",
    "        self.activation_funcs = {\n",
    "            'relu': relu_custom,\n",
    "            'sigmoid': sigmoid,\n",
    "            'tanh': tanh_custom,\n",
    "            'leaky_relu': leaky_relu_custom,\n",
    "            'linear': lambda x: x,\n",
    "            'softmax': lambda x: np.exp(x) / np.sum(np.exp(x), axis=-1, keepdims=True)\n",
    "        }\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        \"\"\"Forward pass: output = activation(inputs @ weights + bias)\"\"\"\n",
    "        # Store inputs for potential backward pass\n",
    "        self.inputs = inputs\n",
    "        \n",
    "        # TODO: Implement linear transformation (z = inputs @ weights + bias)\n",
    "        self.z = None  # Your code here\n",
    "        \n",
    "        # TODO: Apply activation function\n",
    "        activation_func = self.activation_funcs.get(self.activation_name, lambda x: x)\n",
    "        self.output = None  # Your code here\n",
    "        \n",
    "        return self.output\n",
    "    \n",
    "    def __call__(self, inputs):\n",
    "        \"\"\"Make layer callable\"\"\"\n",
    "        return self.forward(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the layer\n",
    "layer = SimpleDenseLayer(input_dim=784, output_dim=128, activation='relu')\n",
    "test_input = np.random.randn(32, 784)  # Batch of 32 samples\n",
    "output = layer(test_input)\n",
    "\n",
    "print(f\"Input shape: {test_input.shape}\")\n",
    "print(f\"Output shape: {output.shape if output is not None else 'Not implemented'}\")\n",
    "print(f\"Weights shape: {layer.weights.shape if layer.weights is not None else 'Not implemented'}\")\n",
    "print(f\"Bias shape: {layer.bias.shape if layer.bias is not None else 'Not implemented'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Complete Neural Network Construction (10 minutes)\n",
    "\n",
    "### Task 3: Build Multi-Layer Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNeuralNetwork:\n",
    "    def __init__(self, layer_sizes, activations):\n",
    "        \"\"\"Build a multi-layer neural network\n",
    "        Args:\n",
    "            layer_sizes: List of layer dimensions [input_dim, hidden1, hidden2, ..., output_dim]\n",
    "            activations: List of activation functions for each layer\n",
    "        \"\"\"\n",
    "        assert len(layer_sizes) >= 2, \"Need at least input and output dimensions\"\n",
    "        assert len(activations) == len(layer_sizes) - 1, \"Need activation for each layer\"\n",
    "        \n",
    "        # TODO: Create list of layers\n",
    "        self.layers = []\n",
    "        # Your code here to build layers\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass through all layers\"\"\"\n",
    "        # TODO: Pass input through each layer sequentially\n",
    "        # Your code here\n",
    "        pass\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and test the network\n",
    "network = SimpleNeuralNetwork(\n",
    "    layer_sizes=[784, 128, 64, 10],\n",
    "    activations=['relu', 'relu', 'softmax']\n",
    ")\n",
    "\n",
    "# Test with dummy MNIST-like data\n",
    "batch_size = 32\n",
    "test_data = np.random.randn(batch_size, 784)\n",
    "output = network(test_data)\n",
    "\n",
    "print(f\"Network architecture:\")\n",
    "print(f\"  Input: 784 dimensions\")\n",
    "print(f\"  Hidden 1: 128 neurons (ReLU)\")\n",
    "print(f\"  Hidden 2: 64 neurons (ReLU)\")\n",
    "print(f\"  Output: 10 classes (Softmax)\")\n",
    "print(f\"\\nOutput shape: {output.shape if output is not None else 'Not implemented'}\")\n",
    "if output is not None:\n",
    "    print(f\"Output sum per sample (should be ~1 for softmax): {output.sum(axis=1)[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare with TensorFlow/Keras Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build equivalent Keras model\n",
    "def build_keras_equivalent():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(128, activation='relu', input_shape=(784,)),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "keras_model = build_keras_equivalent()\n",
    "keras_output = keras_model(test_data)\n",
    "\n",
    "print(f\"Keras model output shape: {keras_output.shape}\")\n",
    "print(f\"Keras model parameter count: {keras_model.count_params()}\")\n",
    "\n",
    "# Display model summary\n",
    "keras_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unit Tests\n",
    "\n",
    "Run these tests to verify your implementations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_unit_tests():\n",
    "    \"\"\"Comprehensive testing suite for student implementations\"\"\"\n",
    "    print(\"Running Unit Tests...\\n\")\n",
    "    \n",
    "    tests_passed = 0\n",
    "    tests_total = 0\n",
    "    \n",
    "    # Test 1: Activation functions\n",
    "    print(\"Testing activation functions:\")\n",
    "    try:\n",
    "        tests_total += 3\n",
    "        assert abs(sigmoid(0) - 0.5) < 1e-6, \"Sigmoid(0) should be 0.5\"\n",
    "        tests_passed += 1\n",
    "        assert relu_custom(-1) == 0 and relu_custom(1) == 1, \"ReLU test failed\"\n",
    "        tests_passed += 1\n",
    "        assert abs(leaky_relu_custom(-1, 0.01) - (-0.01)) < 1e-6, \"Leaky ReLU test failed\"\n",
    "        tests_passed += 1\n",
    "        print(\"✓ Activation functions pass all tests\\n\")\n",
    "    except AssertionError as e:\n",
    "        print(f\"✗ Activation function test failed: {e}\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Activation functions not implemented\\n\")\n",
    "    \n",
    "    # Test 2: Gradient functions\n",
    "    print(\"Testing gradient functions:\")\n",
    "    try:\n",
    "        tests_total += 2\n",
    "        assert abs(sigmoid_gradient(0) - 0.25) < 1e-6, \"Sigmoid gradient at 0 should be 0.25\"\n",
    "        tests_passed += 1\n",
    "        assert relu_gradient(1) == 1 and relu_gradient(-1) == 0, \"ReLU gradient test failed\"\n",
    "        tests_passed += 1\n",
    "        print(\"✓ Gradient functions pass all tests\\n\")\n",
    "    except AssertionError as e:\n",
    "        print(f\"✗ Gradient function test failed: {e}\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Gradient functions not implemented\\n\")\n",
    "    \n",
    "    # Test 3: Layer construction\n",
    "    print(\"Testing layer construction:\")\n",
    "    try:\n",
    "        tests_total += 1\n",
    "        layer = SimpleDenseLayer(10, 5, activation='relu')\n",
    "        test_input = np.random.randn(2, 10)\n",
    "        output = layer(test_input)\n",
    "        assert output.shape == (2, 5), f\"Layer output shape mismatch: {output.shape}\"\n",
    "        tests_passed += 1\n",
    "        print(\"✓ Layer construction passes all tests\\n\")\n",
    "    except AssertionError as e:\n",
    "        print(f\"✗ Layer test failed: {e}\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Layer not implemented correctly\\n\")\n",
    "    \n",
    "    print(f\"\\n{'='*40}\")\n",
    "    print(f\"Tests passed: {tests_passed}/{tests_total}\")\n",
    "    if tests_passed == tests_total:\n",
    "        print(\"🎉 All tests passed successfully!\")\n",
    "    else:\n",
    "        print(f\"Keep working! {tests_total - tests_passed} tests still need to pass.\")\n",
    "\n",
    "# Run the tests\n",
    "run_unit_tests()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Exercises\n",
    "\n",
    "If you finish early, try these additional challenges:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Implement Advanced Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elu_custom(x, alpha=1.0):\n",
    "    \"\"\"ELU activation: x if x>0, alpha*(e^x - 1) otherwise\"\"\"\n",
    "    # TODO: Implement ELU\n",
    "    pass\n",
    "\n",
    "def swish_custom(x, beta=1.0):\n",
    "    \"\"\"Swish activation: x * sigmoid(beta*x)\"\"\"\n",
    "    # TODO: Implement Swish\n",
    "    pass\n",
    "\n",
    "def gelu_custom(x):\n",
    "    \"\"\"GELU activation: x * Φ(x) where Φ is the CDF of standard normal\"\"\"\n",
    "    # TODO: Implement GELU (approximation is fine)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Implement Backward Pass for Your Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a backward method to SimpleDenseLayer\n",
    "def backward(self, grad_output, learning_rate=0.01):\n",
    "    \"\"\"Backward pass for gradient computation\"\"\"\n",
    "    # TODO: Implement backward pass\n",
    "    # 1. Compute gradient of activation\n",
    "    # 2. Compute gradient w.r.t weights and bias\n",
    "    # 3. Update weights and bias\n",
    "    # 4. Return gradient w.r.t input\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Implement a Simple Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(network, X, y, epochs=10, learning_rate=0.01):\n",
    "    \"\"\"Simple training loop for the network\"\"\"\n",
    "    # TODO: Implement a basic training loop\n",
    "    # 1. Forward pass\n",
    "    # 2. Compute loss (e.g., MSE or cross-entropy)\n",
    "    # 3. Backward pass (if implemented)\n",
    "    # 4. Update weights\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Reflection\n",
    "\n",
    "In this tutorial, you have:\n",
    "1. ✅ Implemented custom activation functions from mathematical principles\n",
    "2. ✅ Computed gradients for backpropagation\n",
    "3. ✅ Mastered tensor operations in TensorFlow\n",
    "4. ✅ Built a neural network layer from scratch\n",
    "5. ✅ Constructed a complete multi-layer network\n",
    "\n",
    "### Key Takeaways:\n",
    "- Understanding the mathematics behind activation functions is crucial\n",
    "- Gradient computation is essential for training neural networks\n",
    "- Building layers from scratch helps understand how frameworks like TensorFlow work\n",
    "- Proper weight initialization and activation selection affect network performance\n",
    "\n",
    "### Next Steps:\n",
    "- Module 2 will cover optimization algorithms that use these gradients\n",
    "- We'll explore advanced regularization techniques\n",
    "- You'll learn how to train these networks effectively"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}