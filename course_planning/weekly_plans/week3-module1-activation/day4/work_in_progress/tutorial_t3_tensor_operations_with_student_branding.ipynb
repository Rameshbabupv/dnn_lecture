{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial T3: Building Programs to Perform Basic Operations in Tensors\n",
    "## Week 3, Day 4 - Deep Neural Network Architectures (Enhanced Student Version)\n",
    "\n",
    "**Duration:** 1 Hour | **Format:** Hands-On Tutorial with Student Attribution\n",
    "\n",
    "### Learning Objectives\n",
    "- Implement custom activation functions from mathematical principles\n",
    "- Understand gradient computation for backpropagation\n",
    "- Master tensor operations in TensorFlow\n",
    "- Build a neural network layer from scratch\n",
    "- Construct a complete multi-layer network\n",
    "\n",
    "### üéØ Special Features:\n",
    "- **Student Attribution**: All outputs will be branded with your information\n",
    "- **Professional Portfolio Ready**: Outputs suitable for your portfolio\n",
    "- **Anti-Plagiarism**: Unique identification throughout\n",
    "- **Academic Integrity**: Clear ownership tracking\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üî• MANDATORY: Student Information Setup\n",
    "\n",
    "**‚ö†Ô∏è CRITICAL: You MUST update the variables below with your information before proceeding!**\n",
    "\n",
    "This system will embed your identity throughout ALL outputs, plots, and results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìã STUDENT IDENTIFICATION - MANDATORY\n",
    "# ========================================\n",
    "# Replace the placeholder values with your actual information\n",
    "\n",
    "STUDENT_NAME = \"YOUR_NAME_HERE\"  # Replace with your full name\n",
    "STUDENT_REG_NO = \"YOUR_REG_NO_HERE\"  # Replace with your registration number\n",
    "STUDENT_BRANCH = \"M.Tech DSBS\"  # Update if different\n",
    "SUBMISSION_DATE = \"DATE_HERE\"  # Replace with actual date (YYYY-MM-DD)\n",
    "\n",
    "# ========================================\n",
    "# Automatic validation and setup\n",
    "import datetime\n",
    "import hashlib\n",
    "import re\n",
    "\n",
    "print(\"\" + \"=\"*70)\n",
    "print(\"üéì TUTORIAL T3 - TENSOR OPERATIONS & NEURAL NETWORKS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if STUDENT_NAME == \"YOUR_NAME_HERE\" or STUDENT_REG_NO == \"YOUR_REG_NO_HERE\":\n",
    "    print(\"‚ö†Ô∏è  CRITICAL WARNING: Please update your student information above!\")\n",
    "    print(\"   1. Update STUDENT_NAME with your full name\")\n",
    "    print(\"   2. Update STUDENT_REG_NO with your registration number\")\n",
    "    print(\"   3. Update SUBMISSION_DATE with today's date\")\n",
    "    print(\"   4. Re-run this cell after making changes\")\n",
    "    print(\"\\n‚ùå Cannot proceed until student information is provided!\")\n",
    "else:\n",
    "    print(\"‚úÖ STUDENT AUTHENTICATION SUCCESSFUL!\")\n",
    "    print(f\"   üìù Student: {STUDENT_NAME}\")\n",
    "    print(f\"   üÜî Registration: {STUDENT_REG_NO}\")\n",
    "    print(f\"   üéØ Branch: {STUDENT_BRANCH}\")\n",
    "    print(f\"   üìÖ Date: {SUBMISSION_DATE}\")\n",
    "    \n",
    "    # Generate unique student verification code for anti-plagiarism\n",
    "    student_hash = hashlib.md5(f\"{STUDENT_NAME}{STUDENT_REG_NO}\".encode()).hexdigest()[:8].upper()\n",
    "    print(f\"   üîê Verification Code: {student_hash}\")\n",
    "    \n",
    "    # Auto-generate proper filename for submission\n",
    "    safe_name = re.sub(r'[^a-zA-Z0-9]', '_', STUDENT_NAME)\n",
    "    safe_reg = re.sub(r'[^a-zA-Z0-9]', '_', STUDENT_REG_NO)\n",
    "    SUGGESTED_FILENAME = f\"T3_Enhanced_{safe_name}_{safe_reg}.ipynb\"\n",
    "    print(f\"   üìÅ Suggested filename: {SUGGESTED_FILENAME}\")\n",
    "    \n",
    "    # Global branding function for all outputs\n",
    "    def get_student_header():\n",
    "        return f\"Student: {STUDENT_NAME} | Registration: {STUDENT_REG_NO} | Date: {SUBMISSION_DATE}\"\n",
    "    \n",
    "    def get_student_watermark():\n",
    "        return f\"{STUDENT_NAME} | {STUDENT_REG_NO}\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(\"üîß ENVIRONMENT SETUP\")\n",
    "print(\"=\"*50)\n",
    "if 'STUDENT_NAME' in globals() and STUDENT_NAME != \"YOUR_NAME_HERE\":\n",
    "    print(get_student_header())\n",
    "    print(\"=\"*50)\n",
    "print(f\"‚úÖ TensorFlow version: {tf.__version__}\")\n",
    "print(f\"‚úÖ NumPy version: {np.__version__}\")\n",
    "print(f\"‚úÖ Random seeds set for reproducibility\")\n",
    "print(f\"‚úÖ Environment ready for {STUDENT_NAME if 'STUDENT_NAME' in globals() else 'Student'}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Custom Activation Functions (20 minutes)\n",
    "\n",
    "### Task 1A: Implement Basic Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement the sigmoid activation function\n",
    "def sigmoid(x):\n",
    "    \"\"\"Sigmoid activation: œÉ(x) = 1/(1+e^(-x))\n",
    "    Implemented by: {STUDENT_NAME}\n",
    "    \"\"\"\n",
    "    # Your code here - replace 'pass' with implementation\n",
    "    pass\n",
    "\n",
    "# TODO: Implement the tanh activation function\n",
    "def tanh_custom(x):  \n",
    "    \"\"\"Hyperbolic tangent: tanh(x) = (e^x - e^(-x))/(e^x + e^(-x))\n",
    "    Implemented by: {STUDENT_NAME}\n",
    "    \"\"\"\n",
    "    # Your code here - replace 'pass' with implementation\n",
    "    pass\n",
    "\n",
    "# TODO: Implement the ReLU activation function\n",
    "def relu_custom(x):\n",
    "    \"\"\"ReLU activation: max(0,x)\n",
    "    Implemented by: {STUDENT_NAME}\n",
    "    \"\"\"\n",
    "    # Your code here - replace 'pass' with implementation\n",
    "    pass\n",
    "\n",
    "# TODO: Implement the Leaky ReLU activation function\n",
    "def leaky_relu_custom(x, alpha=0.01):\n",
    "    \"\"\"Leaky ReLU: x if x>0, alpha*x otherwise\n",
    "    Implemented by: {STUDENT_NAME}\n",
    "    \"\"\"\n",
    "    # Your code here - replace 'pass' with implementation\n",
    "    pass\n",
    "\n",
    "# Confirmation message\n",
    "if 'STUDENT_NAME' in globals() and STUDENT_NAME != \"YOUR_NAME_HERE\":\n",
    "    print(f\"üìù Activation functions template prepared for: {STUDENT_NAME}\")\n",
    "    print(f\"üéØ Implement the functions above and run the next cell to test!\")\nelse:\n",
    "    print(\"‚ö†Ô∏è  Please complete student information setup first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Your Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test values\n",
    "test_values = np.array([-2, -1, 0, 1, 2])\n",
    "\n",
    "print(\"\" + \"=\"*70)\n",
    "print(\"üß™ ACTIVATION FUNCTION TEST RESULTS\")\n",
    "if 'STUDENT_NAME' in globals() and STUDENT_NAME != \"YOUR_NAME_HERE\":\n",
    "    print(get_student_header())\n",
    "    print(f\"Verification Code: {student_hash}\")\nprint(\"=\"*70)\n",
    "\n",
    "print(f\"üìä Test Input Values: {test_values}\")\n",
    "print(\"\\nüîç Function Results:\")\n",
    "\n",
    "try:\n",
    "    sigmoid_result = sigmoid(test_values)\n",
    "    print(f\"‚úÖ Sigmoid:    {sigmoid_result}\")\nexcept:\n",
    "    print(f\"‚ùå Sigmoid:    Not implemented yet\")\n",
    "\n",
    "try:\n",
    "    tanh_result = tanh_custom(test_values)\n",
    "    print(f\"‚úÖ Tanh:       {tanh_result}\")\nexcept:\n",
    "    print(f\"‚ùå Tanh:       Not implemented yet\")\n",
    "\n",
    "try:\n",
    "    relu_result = relu_custom(test_values)\n",
    "    print(f\"‚úÖ ReLU:       {relu_result}\")\nexcept:\n",
    "    print(f\"‚ùå ReLU:       Not implemented yet\")\n",
    "\n",
    "try:\n",
    "    leaky_result = leaky_relu_custom(test_values)\n",
    "    print(f\"‚úÖ Leaky ReLU: {leaky_result}\")\nexcept:\n",
    "    print(f\"‚ùå Leaky ReLU: Not implemented yet\")\n",
    "\n",
    "if 'STUDENT_NAME' in globals() and STUDENT_NAME != \"YOUR_NAME_HERE\":\n",
    "    print(f\"\\nüéì Test completed by: {STUDENT_NAME} ({STUDENT_REG_NO})\")\n",
    "    print(f\"üìÖ Timestamp: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\nprint(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1B: Gradient Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement gradient functions\n",
    "def sigmoid_gradient(x):\n",
    "    \"\"\"Sigmoid derivative: œÉ(x) * (1 - œÉ(x))\n",
    "    Implemented by: {STUDENT_NAME}\n",
    "    \"\"\"\n",
    "    # Your code here - replace 'pass' with implementation\n",
    "    pass\n",
    "\n",
    "def tanh_gradient(x):  \n",
    "    \"\"\"Tanh derivative: 1 - tanh¬≤(x)\n",
    "    Implemented by: {STUDENT_NAME}\n",
    "    \"\"\"\n",
    "    # Your code here - replace 'pass' with implementation\n",
    "    pass\n",
    "\n",
    "def relu_gradient(x):\n",
    "    \"\"\"ReLU derivative: 1 if x>0, 0 otherwise\n",
    "    Implemented by: {STUDENT_NAME}\n",
    "    \"\"\"\n",
    "    # Your code here - replace 'pass' with implementation\n",
    "    pass\n",
    "\n",
    "def leaky_relu_gradient(x, alpha=0.01):\n",
    "    \"\"\"Leaky ReLU derivative: 1 if x>0, alpha otherwise\n",
    "    Implemented by: {STUDENT_NAME}\n",
    "    \"\"\"\n",
    "    # Your code here - replace 'pass' with implementation\n",
    "    pass\n",
    "\n",
    "if 'STUDENT_NAME' in globals() and STUDENT_NAME != \"YOUR_NAME_HERE\":\n",
    "    print(f\"üéØ Gradient functions template ready for: {STUDENT_NAME}\")\nelse:\n",
    "    print(\"‚ö†Ô∏è  Please complete student information setup first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Activations and Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_activation_and_gradient(func, grad_func, name):\n",
    "    \"\"\"Plot activation function and its gradient with student attribution\"\"\"\n",
    "    x = np.linspace(-5, 5, 200)\n",
    "    \n",
    "    try:\n",
    "        y = func(x)\n",
    "        dy = grad_func(x)\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "        \n",
    "        # Enhanced styling with student branding\n",
    "        if 'STUDENT_NAME' in globals() and STUDENT_NAME != \"YOUR_NAME_HERE\":\n",
    "            fig.suptitle(f'{name} Analysis by {STUDENT_NAME} ({STUDENT_REG_NO})', \n",
    "                         fontsize=16, fontweight='bold', color='navy')\n",
    "        else:\n",
    "            fig.suptitle(f'{name} Analysis', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        # Plot activation function\n",
    "        ax1.plot(x, y, 'b-', linewidth=3, alpha=0.8)\n",
    "        ax1.set_title(f'{name} Activation Function', fontsize=14, fontweight='bold')\n",
    "        ax1.set_xlabel('x', fontsize=12)\n",
    "        ax1.set_ylabel('f(x)', fontsize=12)\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        ax1.axhline(y=0, color='black', linewidth=0.5)\n",
    "        ax1.axvline(x=0, color='black', linewidth=0.5)\n",
    "        ax1.set_facecolor('#f8f9fa')\n",
    "        \n",
    "        # Plot gradient\n",
    "        ax2.plot(x, dy, 'r-', linewidth=3, alpha=0.8)\n",
    "        ax2.set_title(f'{name} Gradient', fontsize=14, fontweight='bold')\n",
    "        ax2.set_xlabel('x', fontsize=12)\n",
    "        ax2.set_ylabel(\"f'(x)\", fontsize=12)\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        ax2.axhline(y=0, color='black', linewidth=0.5)\n",
    "        ax2.axvline(x=0, color='black', linewidth=0.5)\n",
    "        ax2.set_facecolor('#f8f9fa')\n",
    "        \n",
    "        # Add student watermark\n",
    "        if 'STUDENT_NAME' in globals() and STUDENT_NAME != \"YOUR_NAME_HERE\":\n",
    "            fig.text(0.99, 0.01, get_student_watermark(), \n",
    "                     ha='right', va='bottom', fontsize=9, alpha=0.7,\n",
    "                     bbox=dict(boxstyle=\"round,pad=0.3\", facecolor='white', alpha=0.8))\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        if 'STUDENT_NAME' in globals() and STUDENT_NAME != \"YOUR_NAME_HERE\":\n",
    "            print(f\"üìä {name} plot generated by: {STUDENT_NAME} ({STUDENT_REG_NO})\")\n",
    "            print(f\"üìÖ Generated on: {SUBMISSION_DATE}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Could not plot {name}: Function not implemented yet\")\n",
    "        print(f\"   Error: {str(e)}\")\n",
    "\n",
    "# Generate plots with comprehensive student attribution\n",
    "print(\"\" + \"=\"*70)\n",
    "print(\"üìà ACTIVATION FUNCTION VISUALIZATION SUITE\")\n",
    "if 'STUDENT_NAME' in globals() and STUDENT_NAME != \"YOUR_NAME_HERE\":\n",
    "    print(get_student_header())\n",
    "    print(f\"Verification Code: {student_hash}\")\nprint(\"=\"*70)\n",
    "\n",
    "activation_functions = [\n",
    "    (sigmoid, sigmoid_gradient, 'Sigmoid'),\n",
    "    (tanh_custom, tanh_gradient, 'Tanh'),\n",
    "    (relu_custom, relu_gradient, 'ReLU'),\n",
    "    (lambda x: leaky_relu_custom(x), lambda x: leaky_relu_gradient(x), 'Leaky ReLU')\n",
    "]\n",
    "\n",
    "for func, grad_func, name in activation_functions:\n",
    "    print(f\"\\nüé® Generating {name} visualization...\")\n",
    "    plot_activation_and_gradient(func, grad_func, name)\n",
    "\n",
    "if 'STUDENT_NAME' in globals() and STUDENT_NAME != \"YOUR_NAME_HERE\":\n",
    "    print(f\"\\n‚úÖ All visualizations completed by: {STUDENT_NAME}\")\n",
    "    print(f\"üèÜ Professional-quality plots ready for portfolio!\")\nprint(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Tensor Operations & Layer Construction (25 minutes)\n",
    "\n",
    "### Task 2A: Basic Tensor Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create tensors of different dimensions\n",
    "scalar = tf.constant(5.0)\n",
    "vector = tf.constant([1, 2, 3], dtype=tf.float32)\n",
    "matrix = tf.constant([[1, 2], [3, 4]], dtype=tf.float32)\n",
    "tensor_3d = tf.constant([[[1, 2], [3, 4]], [[5, 6], [7, 8]]], dtype=tf.float32)\n",
    "\n",
    "print(\"\" + \"=\"*70)\n",
    "print(\"üî¢ TENSOR OPERATIONS ANALYSIS\")\n",
    "if 'STUDENT_NAME' in globals() and STUDENT_NAME != \"YOUR_NAME_HERE\":\n",
    "    print(get_student_header())\n",
    "    print(f\"Verification Code: {student_hash}\")\nprint(\"=\"*70)\n",
    "\n",
    "print(\"üìê Tensor Shapes and Properties:\")\n",
    "print(f\"   Scalar: shape={scalar.shape}, ndim={scalar.ndim}, value={scalar.numpy()}\")\n",
    "print(f\"   Vector: shape={vector.shape}, ndim={vector.ndim}, values={vector.numpy()}\")\n",
    "print(f\"   Matrix: shape={matrix.shape}, ndim={matrix.ndim}\")\n",
    "print(f\"   3D Tensor: shape={tensor_3d.shape}, ndim={tensor_3d.ndim}\")\n",
    "\n",
    "print(f\"\\nüìä Tensor Content Details:\")\n",
    "print(f\"   Matrix:\\n{matrix.numpy()}\")\n",
    "print(f\"   3D Tensor interpretation: (batch_size=2, height=2, width=2)\")\n",
    "\n",
    "if 'STUDENT_NAME' in globals() and STUDENT_NAME != \"YOUR_NAME_HERE\":\n",
    "    print(f\"\\n‚úÖ Tensor analysis completed by: {STUDENT_NAME} ({STUDENT_REG_NO})\")\nprint(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Matrix multiplication exercises\n",
    "A = tf.constant([[1, 2], [3, 4]], dtype=tf.float32)\n",
    "B = tf.constant([[5, 6], [7, 8]], dtype=tf.float32)\n",
    "\n",
    "print(\"üî¢ MATRIX OPERATIONS DEMONSTRATION\")\n",
    "if 'STUDENT_NAME' in globals() and STUDENT_NAME != \"YOUR_NAME_HERE\":\n",
    "    print(f\"Analyst: {STUDENT_NAME} | Registration: {STUDENT_REG_NO}\")\nprint(\"-\" * 50)\n",
    "\n",
    "print(f\"Matrix A:\\n{A.numpy()}\")\n",
    "print(f\"\\nMatrix B:\\n{B.numpy()}\")\n",
    "\n",
    "# TODO: Perform element-wise multiplication\n",
    "element_wise = None  # Replace with: tf.multiply(A, B) or A * B\n",
    "\n",
    "# TODO: Perform matrix multiplication  \n",
    "matrix_mult = None   # Replace with: tf.matmul(A, B) or A @ B\n",
    "\n",
    "print(\"\\nüìä Results:\")\n",
    "if element_wise is not None:\n",
    "    print(f\"Element-wise multiplication (A ‚äô B):\\n{element_wise.numpy()}\")\nelse:\n",
    "    print(\"‚ùå Element-wise multiplication: Not implemented yet\")\n",
    "\n",
    "if matrix_mult is not None:\n",
    "    print(f\"\\nMatrix multiplication (A @ B):\\n{matrix_mult.numpy()}\")\n",
    "    # Manual verification\n",
    "    print(f\"\\nüßÆ Manual verification:\")\n",
    "    print(f\"   A[0,0]*B[0,0] + A[0,1]*B[1,0] = {A[0,0]*B[0,0] + A[0,1]*B[1,0]} = {matrix_mult[0,0] if matrix_mult is not None else 'N/A'}\")\nelse:\n",
    "    print(\"‚ùå Matrix multiplication: Not implemented yet\")\n",
    "\n",
    "if 'STUDENT_NAME' in globals() and STUDENT_NAME != \"YOUR_NAME_HERE\":\n",
    "    print(f\"\\n‚úÖ Matrix operations by: {STUDENT_NAME}\")\nprint(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Broadcasting operations\n",
    "vec = tf.constant([1, 2], dtype=tf.float32)  # Shape: (2,)\n",
    "mat = tf.constant([[1, 2], [3, 4], [5, 6]], dtype=tf.float32)  # Shape: (3, 2)\n",
    "\n",
    "print(\"üì° BROADCASTING OPERATIONS\")\n",
    "if 'STUDENT_NAME' in globals() and STUDENT_NAME != \"YOUR_NAME_HERE\":\n",
    "    print(f\"Analyst: {STUDENT_NAME} | Registration: {STUDENT_REG_NO}\")\nprint(\"-\" * 50)\n",
    "\n",
    "print(f\"Vector shape: {vec.shape} | Matrix shape: {mat.shape}\")\n",
    "print(f\"\\nOriginal matrix:\\n{mat.numpy()}\")\n",
    "print(f\"Vector to broadcast: {vec.numpy()}\")\n",
    "\n",
    "# TODO: Add vector to each row of matrix (broadcasting)\n",
    "broadcasted_add = None  # Replace with: mat + vec\n",
    "\n",
    "print(\"\\nüìä Broadcasting Results:\")\n",
    "if broadcasted_add is not None:\n",
    "    print(f\"Result after broadcasting (mat + vec):\\n{broadcasted_add.numpy()}\")\n",
    "    print(f\"\\nüîç Explanation: Vector {vec.numpy()} was added to each row of the matrix\")\nelse:\n",
    "    print(\"‚ùå Broadcasting operation: Not implemented yet\")\n",
    "\n",
    "# Demonstrate other broadcasting patterns\n",
    "col_vec = tf.constant([[1], [2], [3]], dtype=tf.float32)  # Shape: (3, 1)\n",
    "print(f\"\\nüîÑ Advanced Broadcasting:\")\n",
    "print(f\"Column vector shape: {col_vec.shape}\")\n",
    "try:\n",
    "    broadcast_2d = mat + col_vec  # Shape: (3,2) + (3,1) -> (3,2)\n",
    "    print(f\"2D Broadcasting result:\\n{broadcast_2d.numpy()}\")\nexcept:\n",
    "    print(\"Advanced broadcasting demonstration skipped\")\n",
    "\n",
    "if 'STUDENT_NAME' in globals() and STUDENT_NAME != \"YOUR_NAME_HERE\":\n",
    "    print(f\"\\n‚úÖ Broadcasting analysis by: {STUDENT_NAME}\")\nprint(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Shape manipulation\n",
    "original = tf.constant([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "print(\"üìè SHAPE MANIPULATION OPERATIONS\")\n",
    "if 'STUDENT_NAME' in globals() and STUDENT_NAME != \"YOUR_NAME_HERE\":\n",
    "    print(f\"Analyst: {STUDENT_NAME} | Registration: {STUDENT_REG_NO}\")\nprint(\"-\" * 50)\n",
    "\n",
    "print(f\"Original tensor:\\n{original.numpy()}\")\n",
    "print(f\"Original shape: {original.shape}\")\n",
    "\n",
    "# TODO: Reshape to 3x2\n",
    "reshaped = None  # Replace with: tf.reshape(original, [3, 2])\n",
    "\n",
    "# TODO: Flatten to 1D\n",
    "flattened = None  # Replace with: tf.reshape(original, [-1])\n",
    "\n",
    "# TODO: Transpose the matrix\n",
    "transposed = None  # Replace with: tf.transpose(original)\n",
    "\n",
    "print(\"\\nüîÑ Shape Transformation Results:\")\n",
    "\n",
    "if reshaped is not None:\n",
    "    print(f\"Reshaped to 3x2: {reshaped.shape}\")\n",
    "    print(f\"Content:\\n{reshaped.numpy()}\")\nelse:\n",
    "    print(\"‚ùå Reshape operation: Not implemented yet\")\n",
    "\n",
    "if flattened is not None:\n",
    "    print(f\"\\nFlattened to 1D: {flattened.shape}\")\n",
    "    print(f\"Content: {flattened.numpy()}\")\nelse:\n",
    "    print(\"‚ùå Flatten operation: Not implemented yet\")\n",
    "\n",
    "if transposed is not None:\n",
    "    print(f\"\\nTransposed: {transposed.shape}\")\n",
    "    print(f\"Content:\\n{transposed.numpy()}\")\nelse:\n",
    "    print(\"‚ùå Transpose operation: Not implemented yet\")\n",
    "\n",
    "# Demonstrate advanced operations\n",
    "print(f\"\\nüöÄ Advanced Operations:\")\n",
    "if flattened is not None:\n",
    "    expanded = tf.expand_dims(flattened, axis=0)  # Add batch dimension\n",
    "    print(f\"Expanded dims (add batch): {expanded.shape}\")\n",
    "    squeezed = tf.squeeze(expanded)\n",
    "    print(f\"Squeezed back: {squeezed.shape}\")\n",
    "\n",
    "if 'STUDENT_NAME' in globals() and STUDENT_NAME != \"YOUR_NAME_HERE\":\n",
    "    print(f\"\\n‚úÖ Shape operations by: {STUDENT_NAME}\")\nprint(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2B: Dense Layer from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleDenseLayer:\n",
    "    \"\"\"Enhanced dense layer with student attribution and comprehensive features\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, activation='relu', weight_init='xavier'):\n",
    "        \"\"\"Initialize a dense layer with weights, bias, and activation\n",
    "        \n",
    "        Args:\n",
    "            input_dim: Number of input features\n",
    "            output_dim: Number of output features\n",
    "            activation: Activation function name\n",
    "            weight_init: Weight initialization strategy\n",
    "        \"\"\"\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.activation_name = activation\n",
    "        \n",
    "        # Store creator information\n",
    "        if 'STUDENT_NAME' in globals() and STUDENT_NAME != \"YOUR_NAME_HERE\":\n",
    "            self.creator = STUDENT_NAME\n",
    "            self.creator_reg = STUDENT_REG_NO\n",
    "            self.creation_date = SUBMISSION_DATE\n",
    "        else:\n",
    "            self.creator = \"Unknown\"\n",
    "            self.creator_reg = \"Unknown\"\n",
    "            self.creation_date = \"Unknown\"\n",
    "        \n",
    "        # TODO: Initialize weights using Xavier/Glorot initialization\n",
    "        # Hint: For Xavier init, use np.random.randn(input_dim, output_dim) * np.sqrt(2.0 / (input_dim + output_dim))\n",
    "        self.weights = None  # Your code here\n",
    "        \n",
    "        # TODO: Initialize bias as zeros\n",
    "        # Hint: Use np.zeros((1, output_dim))\n",
    "        self.bias = None  # Your code here\n",
    "        \n",
    "        # Activation function dictionary\n",
    "        self.activation_funcs = {\n",
    "            'relu': relu_custom,\n",
    "            'sigmoid': sigmoid,\n",
    "            'tanh': tanh_custom,\n",
    "            'leaky_relu': leaky_relu_custom,\n",
    "            'linear': lambda x: x,\n",
    "            'softmax': self._softmax\n",
    "        }\n",
    "        \n",
    "        # Print creation confirmation\n",
    "        if self.weights is not None and self.bias is not None:\n",
    "            print(f\"‚úÖ Dense layer created by {self.creator}\")\n",
    "            print(f\"   Architecture: {input_dim} -> {output_dim} ({activation})\")\n",
    "            print(f\"   Parameters: {self.weights.size + self.bias.size:,}\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è  Layer template created - implement weight and bias initialization!\")\n",
    "    \n",
    "    def _softmax(self, x):\n",
    "        \"\"\"Numerically stable softmax implementation\"\"\"\n",
    "        exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "        return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        \"\"\"Forward pass: output = activation(inputs @ weights + bias)\"\"\"\n",
    "        # Store for potential backward pass\n",
    "        self.last_input = inputs\n",
    "        \n",
    "        # TODO: Implement linear transformation (z = inputs @ weights + bias)\n",
    "        # Hint: Use np.dot(inputs, self.weights) + self.bias\n",
    "        self.z = None  # Your code here\n",
    "        \n",
    "        # TODO: Apply activation function\n",
    "        activation_func = self.activation_funcs.get(self.activation_name, lambda x: x)\n",
    "        self.output = None  # Your code here: activation_func(self.z)\n",
    "        \n",
    "        return self.output\n",
    "    \n",
    "    def get_info(self):\n",
    "        \"\"\"Get layer information with creator attribution\"\"\"\n",
    "        return {\n",
    "            'creator': self.creator,\n",
    "            'creator_reg': self.creator_reg,\n",
    "            'creation_date': self.creation_date,\n",
    "            'architecture': f\"{self.input_dim} -> {self.output_dim}\",\n",
    "            'activation': self.activation_name,\n",
    "            'parameters': self.weights.size + self.bias.size if self.weights is not None and self.bias is not None else 0\n",
    "        }\n",
    "    \n",
    "    def __call__(self, inputs):\n",
    "        \"\"\"Make layer callable\"\"\"\n",
    "        return self.forward(inputs)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"SimpleDenseLayer({self.input_dim}, {self.output_dim}, '{self.activation_name}') by {self.creator}\"\n",
    "\n",
    "print(\"üèóÔ∏è  SimpleDenseLayer class ready for implementation!\")\nif 'STUDENT_NAME' in globals() and STUDENT_NAME != \"YOUR_NAME_HERE\":\n",
    "    print(f\"üë®‚Äçüíª Template prepared for: {STUDENT_NAME}\")\nelse:\n",
    "    print(\"‚ö†Ô∏è  Please complete student setup first!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the layer with comprehensive reporting\n",
    "print(\"\" + \"=\"*70)\n",
    "print(\"üß™ DENSE LAYER TESTING SUITE\")\n",
    "if 'STUDENT_NAME' in globals() and STUDENT_NAME != \"YOUR_NAME_HERE\":\n",
    "    print(get_student_header())\nprint(\"=\"*70)\n",
    "\n",
    "# Create and test layer\n",
    "layer = SimpleDenseLayer(input_dim=784, output_dim=128, activation='relu')\n",
    "test_input = np.random.randn(32, 784)  # Batch of 32 samples\n",
    "\n",
    "print(f\"\\nüìä Layer Configuration:\")\n",
    "info = layer.get_info()\n",
    "for key, value in info.items():\n",
    "    print(f\"   {key.capitalize().replace('_', ' ')}: {value}\")\n",
    "\n",
    "print(f\"\\nüîç Testing Forward Pass:\")\n",
    "print(f\"   Input shape: {test_input.shape}\")\n",
    "print(f\"   Input statistics: mean={test_input.mean():.3f}, std={test_input.std():.3f}\")\n",
    "\n",
    "try:\n",
    "    output = layer(test_input)\n",
    "    if output is not None:\n",
    "        print(f\"   ‚úÖ Output shape: {output.shape}\")\n",
    "        print(f\"   ‚úÖ Output statistics: mean={output.mean():.3f}, std={output.std():.3f}\")\n",
    "        print(f\"   ‚úÖ ReLU property (non-negative): {np.all(output >= 0)}\")\n",
    "        print(f\"   ‚úÖ Sparsity: {(output == 0).mean()*100:.1f}% zeros\")\n",
    "    else:\n",
    "        print(f\"   ‚ùå Forward pass not implemented\")\nexcept Exception as e:\n",
    "    print(f\"   ‚ùå Forward pass failed: {str(e)}\")\n",
    "\n",
    "if layer.weights is not None and layer.bias is not None:\n",
    "    print(f\"\\nüìà Parameter Analysis:\")\n",
    "    print(f\"   Weights shape: {layer.weights.shape}\")\n",
    "    print(f\"   Weights stats: mean={layer.weights.mean():.4f}, std={layer.weights.std():.4f}\")\n",
    "    print(f\"   Bias shape: {layer.bias.shape}\")\n",
    "    print(f\"   Bias stats: mean={layer.bias.mean():.4f}, std={layer.bias.std():.4f}\")\n",
    "    print(f\"   Total parameters: {layer.weights.size + layer.bias.size:,}\")\nelse:\n",
    "    print(f\"\\n‚ùå Parameters not initialized\")\n",
    "\n",
    "if 'STUDENT_NAME' in globals() and STUDENT_NAME != \"YOUR_NAME_HERE\":\n",
    "    print(f\"\\nüéì Layer testing completed by: {STUDENT_NAME} ({STUDENT_REG_NO})\")\nprint(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Complete Neural Network Construction (10 minutes)\n",
    "\n",
    "### Task 3: Build Multi-Layer Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNeuralNetwork:\n",
    "    \"\"\"Enhanced neural network with comprehensive student attribution\"\"\"\n",
    "    \n",
    "    def __init__(self, layer_sizes, activations, weight_init='xavier'):\n",
    "        \"\"\"Build a multi-layer neural network\n",
    "        \n",
    "        Args:\n",
    "            layer_sizes: List of layer dimensions [input_dim, hidden1, hidden2, ..., output_dim]\n",
    "            activations: List of activation functions for each layer\n",
    "            weight_init: Weight initialization strategy\n",
    "        \"\"\"\n",
    "        assert len(layer_sizes) >= 2, \"Need at least input and output dimensions\"\n",
    "        assert len(activations) == len(layer_sizes) - 1, \"Need activation for each layer\"\n",
    "        \n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.activations = activations\n",
    "        self.num_layers = len(layer_sizes) - 1\n",
    "        \n",
    "        # Store creator information\n",
    "        if 'STUDENT_NAME' in globals() and STUDENT_NAME != \"YOUR_NAME_HERE\":\n",
    "            self.architect = STUDENT_NAME\n",
    "            self.architect_reg = STUDENT_REG_NO\n",
    "            self.design_date = SUBMISSION_DATE\n",
    "            self.verification_code = student_hash if 'student_hash' in globals() else 'N/A'\n",
    "        else:\n",
    "            self.architect = \"Unknown\"\n",
    "            self.architect_reg = \"Unknown\" \n",
    "            self.design_date = \"Unknown\"\n",
    "            self.verification_code = \"N/A\"\n",
    "        \n",
    "        # TODO: Create list of layers\n",
    "        self.layers = []\n",
    "        # Your code here to build layers\n",
    "        # Hint: Use a loop to create SimpleDenseLayer objects\n",
    "        # for i in range(self.num_layers):\n",
    "        #     layer = SimpleDenseLayer(\n",
    "        #         input_dim=layer_sizes[i],\n",
    "        #         output_dim=layer_sizes[i+1], \n",
    "        #         activation=activations[i],\n",
    "        #         weight_init=weight_init\n",
    "        #     )\n",
    "        #     self.layers.append(layer)\n",
    "        \n",
    "        # Print architecture summary\n",
    "        if len(self.layers) > 0:\n",
    "            print(f\"\\nüèóÔ∏è  Neural Network Architecture by {self.architect}:\")\n",
    "            total_params = 0\n",
    "            for i, (size, activation) in enumerate(zip(layer_sizes[1:], activations)):\n",
    "                layer_params = layer_sizes[i] * size + size  # weights + bias\n",
    "                total_params += layer_params\n",
    "                print(f\"   Layer {i+1}: {layer_sizes[i]} -> {size} ({activation}) | {layer_params:,} params\")\n",
    "            print(f\"   Total parameters: {total_params:,}\")\n",
    "            print(f\"   Verification Code: {self.verification_code}\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è  Network template created - implement layer construction!\")\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass through all layers\"\"\"\n",
    "        # TODO: Pass input through each layer sequentially\n",
    "        # Your code here\n",
    "        # Hint:\n",
    "        # current_input = x\n",
    "        # for layer in self.layers:\n",
    "        #     current_input = layer.forward(current_input)\n",
    "        # return current_input\n",
    "        pass\n",
    "    \n",
    "    def get_architecture_summary(self):\n",
    "        \"\"\"Get detailed architecture summary with attribution\"\"\"\n",
    "        summary = {\n",
    "            'architect': self.architect,\n",
    "            'architect_reg': self.architect_reg,\n",
    "            'design_date': self.design_date,\n",
    "            'verification_code': self.verification_code,\n",
    "            'layer_count': self.num_layers,\n",
    "            'architecture': ' -> '.join(map(str, self.layer_sizes)),\n",
    "            'activations': ' -> '.join(self.activations),\n",
    "            'total_parameters': sum(layer.get_info()['parameters'] for layer in self.layers) if self.layers else 0\n",
    "        }\n",
    "        return summary\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"SimpleNeuralNetwork({self.layer_sizes}, {self.activations}) by {self.architect}\"\n",
    "\n",
    "print(\"üöÄ SimpleNeuralNetwork class ready for implementation!\")\nif 'STUDENT_NAME' in globals() and STUDENT_NAME != \"YOUR_NAME_HERE\":\n",
    "    print(f\"üß† Neural network template prepared for: {STUDENT_NAME}\")\nelse:\n",
    "    print(\"‚ö†Ô∏è  Please complete student setup first!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and test the network with comprehensive analysis\n",
    "print(\"\" + \"=\"*70)\n",
    "print(\"üß† NEURAL NETWORK CONSTRUCTION & TESTING\")\n",
    "if 'STUDENT_NAME' in globals() and STUDENT_NAME != \"YOUR_NAME_HERE\":\n",
    "    print(get_student_header())\nprint(\"=\"*70)\n",
    "\n",
    "# Create network for MNIST-like classification\n",
    "network = SimpleNeuralNetwork(\n",
    "    layer_sizes=[784, 128, 64, 10],\n",
    "    activations=['relu', 'relu', 'softmax']\n",
    ")\n",
    "\n",
    "# Display architecture details\n",
    "summary = network.get_architecture_summary()\n",
    "print(f\"\\nüìã Network Specifications:\")\n",
    "for key, value in summary.items():\n",
    "    print(f\"   {key.replace('_', ' ').title()}: {value}\")\n",
    "\n",
    "# Test with dummy MNIST-like data\n",
    "batch_size = 32\n",
    "test_data = np.random.randn(batch_size, 784)\n",
    "\n",
    "print(f\"\\nüß™ Forward Pass Testing:\")\n",
    "print(f\"   Input shape: {test_data.shape}\")\n",
    "print(f\"   Input statistics: mean={test_data.mean():.3f}, std={test_data.std():.3f}\")\n",
    "\n",
    "try:\n",
    "    output = network(test_data)\n",
    "    if output is not None:\n",
    "        print(f\"   ‚úÖ Output shape: {output.shape}\")\n",
    "        print(f\"   ‚úÖ Output statistics: mean={output.mean():.3f}, std={output.std():.3f}\")\n",
    "        \n",
    "        # Check softmax properties\n",
    "        if network.activations[-1] == 'softmax':\n",
    "            sums = output.sum(axis=1)\n",
    "            print(f\"   ‚úÖ Softmax sum check: {np.allclose(sums, 1.0)} (should be True)\")\n",
    "            print(f\"   ‚úÖ Probability range: [{output.min():.4f}, {output.max():.4f}]\")\n",
    "            \n",
    "            # Show sample predictions\n",
    "            print(f\"\\nüîç Sample Predictions (first 3 samples):\")\n",
    "            for i in range(min(3, output.shape[0])):\n",
    "                pred_class = np.argmax(output[i])\n",
    "                confidence = output[i][pred_class]\n",
    "                print(f\"      Sample {i+1}: Class {pred_class}, Confidence: {confidence:.3f}\")\n",
    "    else:\n",
    "        print(f\"   ‚ùå Forward pass not implemented\")\nexcept Exception as e:\n",
    "    print(f\"   ‚ùå Forward pass failed: {str(e)}\")\n",
    "    print(f\"   üí° Hint: Make sure to implement the layer construction and forward pass\")\n",
    "\n",
    "# Network validation\n",
    "print(f\"\\nüìä Network Validation:\")\n",
    "if len(network.layers) == network.num_layers:\n",
    "    print(f\"   ‚úÖ Layer count correct: {len(network.layers)} layers\")\n",
    "    for i, layer in enumerate(network.layers):\n",
    "        expected_in = network.layer_sizes[i]\n",
    "        expected_out = network.layer_sizes[i+1]\n",
    "        if hasattr(layer, 'input_dim') and hasattr(layer, 'output_dim'):\n",
    "            print(f\"   ‚úÖ Layer {i+1}: {layer.input_dim}‚Üí{layer.output_dim} ({layer.activation_name})\")\n",
    "        else:\n",
    "            print(f\"   ‚ùå Layer {i+1}: Invalid layer structure\")\nelse:\n",
    "    print(f\"   ‚ùå Layer construction incomplete\")\n",
    "\n",
    "if 'STUDENT_NAME' in globals() and STUDENT_NAME != \"YOUR_NAME_HERE\":\n",
    "    print(f\"\\nüèÜ Network testing completed by: {STUDENT_NAME} ({STUDENT_REG_NO})\")\n",
    "    print(f\"üìÖ Test date: {SUBMISSION_DATE}\")\n",
    "    print(f\"üîê Verification: {network.verification_code}\")\nprint(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare with TensorFlow/Keras Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build equivalent Keras model for comparison\n",
    "def build_keras_equivalent():\n",
    "    \"\"\"Build equivalent network using Keras\"\"\"\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(128, activation='relu', input_shape=(784,)),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "print(\"\" + \"=\"*70)\n",
    "print(\"‚öñÔ∏è  IMPLEMENTATION COMPARISON: CUSTOM vs KERAS\")\n",
    "if 'STUDENT_NAME' in globals() and STUDENT_NAME != \"YOUR_NAME_HERE\":\n",
    "    print(get_student_header())\nprint(\"=\"*70)\n",
    "\n",
    "keras_model = build_keras_equivalent()\n",
    "keras_output = keras_model(test_data)\n",
    "\n",
    "print(f\"\\nüìä Architecture Comparison:\")\n",
    "print(f\"   Custom Model Architecture: {' -> '.join(map(str, network.layer_sizes))}\")\n",
    "print(f\"   Keras Model Layers: {len(keras_model.layers)} layers\")\n",
    "\n",
    "print(f\"\\nüî¢ Parameter Comparison:\")\n",
    "keras_params = keras_model.count_params()\n",
    "custom_params = network.get_architecture_summary()['total_parameters']\n",
    "print(f\"   Custom implementation: {custom_params:,} parameters\")\n",
    "print(f\"   Keras model: {keras_params:,} parameters\")\n",
    "print(f\"   Parameter match: {custom_params == keras_params}\")\n",
    "\n",
    "print(f\"\\nüìà Output Comparison:\")\n",
    "print(f\"   Custom output shape: {output.shape if 'output' in locals() and output is not None else 'Not available'}\")\n",
    "print(f\"   Keras output shape: {keras_output.shape}\")\n",
    "\n",
    "if 'output' in locals() and output is not None:\n",
    "    print(f\"\\nüîç Sample Output Analysis:\")\n",
    "    print(f\"   Custom model - first sample: {output[0][:5]}\")\n",
    "    print(f\"   Keras model - first sample: {keras_output.numpy()[0][:5]}\")\n",
    "    print(f\"   üìù Note: Values differ due to different weight initialization\")\n",
    "    print(f\"        but shapes and properties should match!\")\n\nprint(f\"\\nüìã Keras Model Summary:\")\nkeras_model.summary()\n",
    "\n",
    "if 'STUDENT_NAME' in globals() and STUDENT_NAME != \"YOUR_NAME_HERE\":\n",
    "    print(f\"\\nüéØ Comparison analysis by: {STUDENT_NAME} ({STUDENT_REG_NO})\")\n",
    "    print(f\"‚ú® Your custom implementation matches industry-standard Keras!\")\nprint(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comprehensive Unit Tests with Student Attribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_comprehensive_unit_tests():\n",
    "    \"\"\"Enhanced testing suite with student attribution and detailed reporting\"\"\"\n",
    "    \n",
    "    print(\"\" + \"=\"*80)\n",
    "    print(\"üß™ COMPREHENSIVE UNIT TESTING SUITE\")\n",
    "    if 'STUDENT_NAME' in globals() and STUDENT_NAME != \"YOUR_NAME_HERE\":\n",
    "        print(get_student_header())\n",
    "        print(f\"Verification Code: {student_hash}\")\n",
    "        print(f\"Test Execution Time: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    tests_passed = 0\n",
    "    tests_total = 0\n",
    "    test_results = []\n",
    "    \n",
    "    # Test 1: Activation Functions\n",
    "    print(\"\\nüß© TEST SECTION 1: ACTIVATION FUNCTIONS\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    activation_tests = [\n",
    "        (\"Sigmoid(0) = 0.5\", lambda: abs(sigmoid(0) - 0.5) < 1e-6, sigmoid),\n",
    "        (\"Sigmoid saturation\", lambda: sigmoid(100) > 0.99, sigmoid),\n",
    "        (\"ReLU basic functionality\", lambda: relu_custom(-1) == 0 and relu_custom(1) == 1, relu_custom),\n",
    "        (\"Leaky ReLU negative slope\", lambda: abs(leaky_relu_custom(-1, 0.01) - (-0.01)) < 1e-6, leaky_relu_custom),\n",
    "        (\"Tanh zero-centered\", lambda: abs(tanh_custom(0)) < 1e-6, tanh_custom)\n",
    "    ]\n",
    "    \n",
    "    for test_name, test_func, target_func in activation_tests:\n",
    "        tests_total += 1\n",
    "        try:\n",
    "            result = test_func()\n",
    "            if result:\n",
    "                print(f\"   ‚úÖ {test_name}\")\n",
    "                tests_passed += 1\n",
    "                test_results.append((test_name, \"PASS\", None))\n",
    "            else:\n",
    "                print(f\"   ‚ùå {test_name} - Logic error\")\n",
    "                test_results.append((test_name, \"FAIL\", \"Logic error\"))\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå {test_name} - Not implemented: {str(e)[:50]}\")\n",
    "            test_results.append((test_name, \"FAIL\", \"Not implemented\"))\n",
    "    \n",
    "    # Test 2: Gradient Functions\n",
    "    print(\"\\nüìà TEST SECTION 2: GRADIENT FUNCTIONS\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    gradient_tests = [\n",
    "        (\"Sigmoid gradient maximum\", lambda: abs(sigmoid_gradient(0) - 0.25) < 1e-6),\n",
    "        (\"Tanh gradient maximum\", lambda: abs(tanh_gradient(0) - 1.0) < 1e-6),\n",
    "        (\"ReLU gradient step function\", lambda: relu_gradient(1) == 1 and relu_gradient(-1) == 0),\n",
    "        (\"Leaky ReLU gradient consistency\", lambda: leaky_relu_gradient(1) == 1 and leaky_relu_gradient(-1, 0.01) == 0.01)\n",
    "    ]\n",
    "    \n",
    "    for test_name, test_func in gradient_tests:\n",
    "        tests_total += 1\n",
    "        try:\n",
    "            result = test_func()\n",
    "            if result:\n",
    "                print(f\"   ‚úÖ {test_name}\")\n",
    "                tests_passed += 1\n",
    "                test_results.append((test_name, \"PASS\", None))\n",
    "            else:\n",
    "                print(f\"   ‚ùå {test_name} - Logic error\")\n",
    "                test_results.append((test_name, \"FAIL\", \"Logic error\"))\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå {test_name} - Not implemented: {str(e)[:50]}\")\n",
    "            test_results.append((test_name, \"FAIL\", \"Not implemented\"))\n",
    "    \n",
    "    # Test 3: Tensor Operations\n",
    "    print(\"\\nüî¢ TEST SECTION 3: TENSOR OPERATIONS\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    try:\n",
    "        # Check if tensor operations were implemented\n",
    "        A = tf.constant([[1, 2], [3, 4]], dtype=tf.float32)\n",
    "        B = tf.constant([[5, 6], [7, 8]], dtype=tf.float32)\n",
    "        \n",
    "        # These should be implemented in the tensor operations section\n",
    "        tests_total += 1\n",
    "        print(f\"   ‚úÖ Tensor creation and manipulation\")\n",
    "        tests_passed += 1\n",
    "        test_results.append((\"Tensor operations\", \"PASS\", None))\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Tensor operations - Error: {str(e)[:50]}\")\n",
    "        test_results.append((\"Tensor operations\", \"FAIL\", str(e)[:50]))\n",
    "    \n",
    "    # Test 4: Layer Construction\n",
    "    print(\"\\nüèóÔ∏è  TEST SECTION 4: LAYER CONSTRUCTION\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    try:\n",
    "        tests_total += 1\n",
    "        test_layer = SimpleDenseLayer(10, 5, activation='relu')\n",
    "        test_input = np.random.randn(2, 10)\n",
    "        \n",
    "        if test_layer.weights is not None and test_layer.bias is not None:\n",
    "            layer_output = test_layer(test_input)\n",
    "            \n",
    "            if layer_output is not None and layer_output.shape == (2, 5):\n",
    "                print(f\"   ‚úÖ Layer construction and forward pass\")\n",
    "                tests_passed += 1\n",
    "                test_results.append((\"Layer construction\", \"PASS\", None))\n",
    "                \n",
    "                # Additional layer tests\n",
    "                tests_total += 1\n",
    "                if test_layer.activation_name == 'relu' and np.all(layer_output >= 0):\n",
    "                    print(f\"   ‚úÖ ReLU activation property\")\n",
    "                    tests_passed += 1\n",
    "                    test_results.append((\"ReLU activation property\", \"PASS\", None))\n",
    "                else:\n",
    "                    print(f\"   ‚ùå ReLU activation property\")\n",
    "                    test_results.append((\"ReLU activation property\", \"FAIL\", \"Negative outputs found\"))\n",
    "            else:\n",
    "                print(f\"   ‚ùå Layer forward pass - Incorrect output\")\n",
    "                test_results.append((\"Layer construction\", \"FAIL\", \"Forward pass failed\"))\n",
    "        else:\n",
    "            print(f\"   ‚ùå Layer construction - Weights/bias not initialized\")\n",
    "            test_results.append((\"Layer construction\", \"FAIL\", \"Parameters not initialized\"))\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Layer construction - Error: {str(e)[:50]}\")\n",
    "        test_results.append((\"Layer construction\", \"FAIL\", str(e)[:50]))\n",
    "    \n",
    "    # Test 5: Complete Network\n",
    "    print(\"\\nüß† TEST SECTION 5: COMPLETE NETWORK\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    try:\n",
    "        tests_total += 1\n",
    "        test_net = SimpleNeuralNetwork([10, 8, 3], ['relu', 'softmax'])\n",
    "        test_input = np.random.randn(2, 10)\n",
    "        \n",
    "        if len(test_net.layers) > 0:\n",
    "            net_output = test_net(test_input)\n",
    "            \n",
    "            if net_output is not None and net_output.shape == (2, 3):\n",
    "                print(f\"   ‚úÖ Network construction and forward pass\")\n",
    "                tests_passed += 1\n",
    "                test_results.append((\"Network construction\", \"PASS\", None))\n",
    "                \n",
    "                # Softmax test\n",
    "                tests_total += 1\n",
    "                if np.allclose(net_output.sum(axis=1), 1.0, rtol=1e-4):\n",
    "                    print(f\"   ‚úÖ Softmax probability distribution\")\n",
    "                    tests_passed += 1\n",
    "                    test_results.append((\"Softmax distribution\", \"PASS\", None))\n",
    "                else:\n",
    "                    print(f\"   ‚ùå Softmax probability distribution\")\n",
    "                    test_results.append((\"Softmax distribution\", \"FAIL\", \"Probabilities don't sum to 1\"))\n",
    "            else:\n",
    "                print(f\"   ‚ùå Network forward pass - Incorrect output\")\n",
    "                test_results.append((\"Network construction\", \"FAIL\", \"Forward pass failed\"))\n",
    "        else:\n",
    "            print(f\"   ‚ùå Network construction - No layers created\")\n",
    "            test_results.append((\"Network construction\", \"FAIL\", \"No layers created\"))\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Network construction - Error: {str(e)[:50]}\")\n",
    "        test_results.append((\"Network construction\", \"FAIL\", str(e)[:50]))\n",
    "    \n",
    "    # Final Results with Student Attribution\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üìä FINAL TEST RESULTS & CERTIFICATION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    success_rate = (tests_passed / tests_total * 100) if tests_total > 0 else 0\n",
    "    \n",
    "    print(f\"\\nüìà Test Statistics:\")\n",
    "    print(f\"   Tests Passed: {tests_passed}/{tests_total}\")\n",
    "    print(f\"   Success Rate: {success_rate:.1f}%\")\n",
    "    \n",
    "    if 'STUDENT_NAME' in globals() and STUDENT_NAME != \"YOUR_NAME_HERE\":\n",
    "        print(f\"\\nüë§ Student Information:\")\n",
    "        print(f\"   Name: {STUDENT_NAME}\")\n",
    "        print(f\"   Registration: {STUDENT_REG_NO}\")\n",
    "        print(f\"   Branch: {STUDENT_BRANCH}\")\n",
    "        print(f\"   Submission Date: {SUBMISSION_DATE}\")\n",
    "        print(f\"   Verification Code: {student_hash}\")\n",
    "    \n",
    "    # Grade assignment\n",
    "    if success_rate >= 90:\n",
    "        grade = \"A+\"\n",
    "        message = \"üèÜ OUTSTANDING PERFORMANCE!\"\n",
    "    elif success_rate >= 80:\n",
    "        grade = \"A\"\n",
    "        message = \"üéâ EXCELLENT WORK!\"\n",
    "    elif success_rate >= 70:\n",
    "        grade = \"B+\"\n",
    "        message = \"üëç GOOD PERFORMANCE!\"\n",
    "    elif success_rate >= 60:\n",
    "        grade = \"B\"\n",
    "        message = \"‚úÖ SATISFACTORY WORK!\"\n",
    "    else:\n",
    "        grade = \"Needs Improvement\"\n",
    "        message = \"üí™ KEEP WORKING - YOU'RE GETTING THERE!\"\n",
    "    \n",
    "    print(f\"\\nüéØ Performance Grade: {grade}\")\n",
    "    print(f\"üéä {message}\")\n",
    "    \n",
    "    if tests_passed == tests_total:\n",
    "        print(\"\\n\" + \"üéâ\" * 20)\n",
    "        print(\"üèÜ PERFECT SCORE ACHIEVEMENT! üèÜ\")\n",
    "        if 'STUDENT_NAME' in globals() and STUDENT_NAME != \"YOUR_NAME_HERE\":\n",
    "            print(f\"\\nüìú CERTIFICATE OF COMPLETION\")\n",
    "            print(f\"‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\")\n",
    "            print(f\"This certifies that {STUDENT_NAME}\")\n",
    "            print(f\"Registration Number: {STUDENT_REG_NO}\")\n",
    "            print(f\"has successfully completed Tutorial T3\")\n",
    "            print(f\"with a PERFECT SCORE of {tests_passed}/{tests_total}\")\n",
    "            print(f\"\")\n",
    "            print(f\"Date: {SUBMISSION_DATE}\")\n",
    "            print(f\"Verification: {student_hash}\")\n",
    "            print(f\"Grade: {grade}\")\n",
    "            print(f\"‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\")\n",
    "        print(\"üéâ\" * 20)\n",
    "        print(\"üöÄ YOU'RE READY FOR MODULE 2: OPTIMIZATION ALGORITHMS!\")\n",
    "    else:\n",
    "        failed_tests = tests_total - tests_passed\n",
    "        print(f\"\\nüîß Areas for Improvement:\")\n",
    "        for test_name, result, error in test_results:\n",
    "            if result == \"FAIL\":\n",
    "                print(f\"   ‚Ä¢ {test_name}: {error if error else 'Review implementation'}\")\n",
    "        print(f\"\\nüí° Keep working on the {failed_tests} remaining test(s)!\")\n",
    "        print(f\"üìö Review the corresponding sections and try again.\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    \n",
    "    return tests_passed, tests_total, grade, success_rate\n",
    "\n",
    "# Run the comprehensive test suite\n",
    "test_results = run_comprehensive_unit_tests()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì FINAL SUBMISSION CHECKLIST\n",
    "\n",
    "Before submitting your notebook, ensure you have completed:\n",
    "\n",
    "### ‚úÖ Implementation Checklist:\n",
    "- [ ] **Student Information**: Updated all student variables at the top\n",
    "- [ ] **Activation Functions**: Implemented sigmoid, tanh, ReLU, and leaky_relu\n",
    "- [ ] **Gradient Functions**: Implemented all corresponding gradient functions\n",
    "- [ ] **Tensor Operations**: Completed element-wise mult, matrix mult, broadcasting, reshaping\n",
    "- [ ] **Dense Layer**: Implemented weight initialization, bias, and forward pass\n",
    "- [ ] **Neural Network**: Implemented layer construction and forward pass\n",
    "- [ ] **Unit Tests**: All tests passing (or at least attempted)\n",
    "\n",
    "### üìä Quality Indicators:\n",
    "- All outputs should display your name and registration number\n",
    "- All plots should have your watermark\n",
    "- Test results should show your verification code\n",
    "- Network architecture should be attributed to you\n",
    "\n",
    "### üíæ Submission Requirements:\n",
    "1. **Save this notebook** with the suggested filename from the first cell\n",
    "2. **Run all cells** to ensure everything works\n",
    "3. **Check that your name appears** in all major outputs\n",
    "4. **Verify your verification code** is consistent throughout\n",
    "5. **Submit the .ipynb file** through the designated platform\n",
    "\n",
    "### üèÜ Success Criteria:\n",
    "- Unit tests show 80%+ pass rate\n",
    "- All visualizations display properly with your attribution\n",
    "- Neural network produces sensible outputs\n",
    "- Code is clean and well-commented\n",
    "\n",
    "**üéâ Congratulations on completing Tutorial T3! Your work demonstrates deep understanding of neural network fundamentals and tensor operations.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}