{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 6 Day 3: Interactive Code Demonstrations\n",
    "## Overfitting, Underfitting & Classical Regularization\n",
    "\n",
    "**Course:** 21CSE558T - Deep Neural Network Architectures  \n",
    "**Instructor:** Prof. Ramesh Babu  \n",
    "**Date:** September 15, 2025\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "- Visualize bias-variance tradeoff with interactive examples\n",
    "- Detect overfitting patterns through code demonstrations  \n",
    "- Implement L1 and L2 regularization in TensorFlow\n",
    "- Compare regularization techniques with real data\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì¶ Essential Imports - Your ML Toolkit\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from sklearn.datasets import make_regression, make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for beautiful plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"üöÄ All libraries loaded successfully!\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üçΩÔ∏è HOUR 1: BIAS-VARIANCE TRADEOFF & OVERFITTING\n",
    "\n",
    "## üéØ Demo 1: The Archer's Target - Bias-Variance Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_bias_variance_archery():\n",
    "    \"\"\"\n",
    "    üèπ The Archer's Target: Visual representation of bias-variance tradeoff\n",
    "    \n",
    "    Analogy: Different archers shooting at a target represent different model behaviors\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    fig.suptitle('üèπ The Archer\\'s Target: Bias-Variance Tradeoff', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Target center (true value)\n",
    "    target_x, target_y = 0, 0\n",
    "    \n",
    "    scenarios = [\n",
    "        {'title': 'Low Bias, Low Variance\\n‚úÖ Skilled Archer', 'bias': (0, 0), 'variance': 0.3, 'color': 'green'},\n",
    "        {'title': 'Low Bias, High Variance\\n‚ö†Ô∏è Inconsistent Archer', 'bias': (0, 0), 'variance': 1.2, 'color': 'orange'},\n",
    "        {'title': 'High Bias, Low Variance\\nüìç Systematic Error', 'bias': (2, 1.5), 'variance': 0.3, 'color': 'red'},\n",
    "        {'title': 'High Bias, High Variance\\n‚ùå Worst Case', 'bias': (2, 1.5), 'variance': 1.2, 'color': 'darkred'}\n",
    "    ]\n",
    "    \n",
    "    for idx, scenario in enumerate(scenarios):\n",
    "        row, col = idx // 2, idx % 2\n",
    "        ax = axes[row, col]\n",
    "        \n",
    "        # Draw target circles\n",
    "        for radius in [1, 2, 3]:\n",
    "            circle = plt.Circle((target_x, target_y), radius, fill=False, color='gray', alpha=0.5)\n",
    "            ax.add_patch(circle)\n",
    "        \n",
    "        # Mark bullseye\n",
    "        ax.plot(target_x, target_y, 'ko', markersize=8, label='True Value')\n",
    "        \n",
    "        # Generate arrow shots with bias and variance\n",
    "        bias_x, bias_y = scenario['bias']\n",
    "        variance = scenario['variance']\n",
    "        \n",
    "        # 20 shots per archer\n",
    "        shots_x = np.random.normal(bias_x, variance, 20)\n",
    "        shots_y = np.random.normal(bias_y, variance, 20)\n",
    "        \n",
    "        ax.scatter(shots_x, shots_y, alpha=0.7, s=50, color=scenario['color'], \n",
    "                  label=f'Arrows (n=20)')\n",
    "        \n",
    "        # Mark average shot position\n",
    "        avg_x, avg_y = np.mean(shots_x), np.mean(shots_y)\n",
    "        ax.plot(avg_x, avg_y, 'x', markersize=12, color='black', markeredgewidth=3,\n",
    "               label='Average Shot')\n",
    "        \n",
    "        ax.set_xlim(-4, 5)\n",
    "        ax.set_ylim(-4, 5)\n",
    "        ax.set_aspect('equal')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.set_title(scenario['title'], fontweight='bold')\n",
    "        ax.legend()\n",
    "        \n",
    "        # Add metrics\n",
    "        bias_magnitude = np.sqrt(bias_x**2 + bias_y**2)\n",
    "        ax.text(0.02, 0.98, f'Bias: {bias_magnitude:.1f}\\nVariance: {variance:.1f}', \n",
    "               transform=ax.transAxes, verticalalignment='top',\n",
    "               bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"üéØ Key Insights:\")\n",
    "    print(\"‚úÖ Green: Ideal model - accurate and consistent\")\n",
    "    print(\"‚ö†Ô∏è Orange: High variance - needs more data or regularization\")\n",
    "    print(\"üìç Red: High bias - needs more model complexity\")\n",
    "    print(\"‚ùå Dark Red: Worst case - needs complete redesign\")\n",
    "\n",
    "# Run the demonstration\n",
    "visualize_bias_variance_archery()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üçΩÔ∏è Demo 2: The Chef's Learning Curve - Polynomial Overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_marie_kondo_vs_equal_opportunity():\n",
    "    \"\"\"\n",
    "    ‚ú® Marie Kondo (L1) vs Equal Opportunity Employer (L2)\n",
    "    \n",
    "    Visual comparison of how L1 and L2 regularization affect feature selection\n",
    "    \"\"\"\n",
    "    # Create synthetic dataset with many features (some relevant, some noise)\n",
    "    np.random.seed(42)\n",
    "    n_samples, n_features = 100, 20\n",
    "    \n",
    "    # Create features: first 5 are relevant, rest are noise\n",
    "    X_relevant = np.random.randn(n_samples, 5)\n",
    "    X_noise = np.random.randn(n_samples, 15) * 0.1  # Low-signal noise features\n",
    "    X = np.column_stack([X_relevant, X_noise])\n",
    "    \n",
    "    # Create target: only depends on first 5 features\n",
    "    true_weights = np.array([2, -1.5, 1, -0.5, 0.8] + [0]*15)  # Only first 5 matter\n",
    "    y = X @ true_weights + np.random.randn(n_samples) * 0.1\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Build models with different regularization\n",
    "    models = {}\n",
    "    \n",
    "    # No regularization (Hoarder)\n",
    "    hoarder_key = 'Hoarder\\n(No Regularization)'\n",
    "    models[hoarder_key] = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(1, input_shape=(n_features,), use_bias=False)\n",
    "    ])\n",
    "    \n",
    "    # L1 regularization (Marie Kondo)\n",
    "    marie_key = 'Marie Kondo\\n(L1 Regularization)'\n",
    "    models[marie_key] = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(1, input_shape=(n_features,), use_bias=False,\n",
    "                            kernel_regularizer=tf.keras.regularizers.l1(0.1))\n",
    "    ])\n",
    "    \n",
    "    # L2 regularization (Equal Opportunity)\n",
    "    equal_key = 'Equal Opportunity\\n(L2 Regularization)'\n",
    "    models[equal_key] = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(1, input_shape=(n_features,), use_bias=False,\n",
    "                            kernel_regularizer=tf.keras.regularizers.l2(0.1))\n",
    "    ])\n",
    "    \n",
    "    # Train all models\n",
    "    results = {}\n",
    "    for name, model in models.items():\n",
    "        model.compile(optimizer='adam', loss='mse')\n",
    "        model.fit(X_train, y_train, epochs=200, verbose=0)\n",
    "        \n",
    "        weights = model.layers[0].get_weights()[0].flatten()\n",
    "        train_pred = model.predict(X_train, verbose=0).flatten()\n",
    "        test_pred = model.predict(X_test, verbose=0).flatten()\n",
    "        \n",
    "        results[name] = {\n",
    "            'weights': weights,\n",
    "            'train_mse': mean_squared_error(y_train, train_pred),\n",
    "            'test_mse': mean_squared_error(y_test, test_pred),\n",
    "            'sparsity': np.sum(np.abs(weights) < 0.01) / len(weights)\n",
    "        }\n",
    "    \n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    fig.suptitle('‚ú® Marie Kondo vs Equal Opportunity Employer: Feature Management Styles', \n",
    "                fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Top row: Weight distributions\n",
    "    feature_names = [f'Important_{i+1}' if i < 5 else f'Noise_{i-4}' for i in range(n_features)]\n",
    "    colors = ['red' if i < 5 else 'gray' for i in range(n_features)]\n",
    "    \n",
    "    for idx, (name, result) in enumerate(results.items()):\n",
    "        ax = axes[0, idx]\n",
    "        bars = ax.bar(range(n_features), result['weights'], color=colors, alpha=0.7)\n",
    "        ax.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "        ax.set_title(f'{name}\\nFeature Weights', fontweight='bold')\n",
    "        ax.set_xlabel('Features')\n",
    "        ax.set_ylabel('Weight Value')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add performance metrics\n",
    "        ax.text(0.02, 0.98, \n",
    "               f'Train MSE: {result[\"train_mse\"]:.3f}\\nTest MSE: {result[\"test_mse\"]:.3f}\\nSparsity: {result[\"sparsity\"]:.1%}',\n",
    "               transform=ax.transAxes, verticalalignment='top',\n",
    "               bbox=dict(boxstyle='round', facecolor='white', alpha=0.9))\n",
    "    \n",
    "    # Bottom row: Weight magnitude comparisons\n",
    "    weight_comparison = np.array([result['weights'] for result in results.values()])\n",
    "    \n",
    "    # Histogram of weight magnitudes\n",
    "    ax = axes[1, 0]\n",
    "    for idx, (name, result) in enumerate(results.items()):\n",
    "        ax.hist(np.abs(result['weights']), bins=20, alpha=0.6, label=name)\n",
    "    ax.set_title('Weight Magnitude Distribution', fontweight='bold')\n",
    "    ax.set_xlabel('|Weight Value|')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Performance comparison\n",
    "    ax = axes[1, 1]\n",
    "    model_names = list(results.keys())\n",
    "    train_errors = [results[name]['train_mse'] for name in model_names]\n",
    "    test_errors = [results[name]['test_mse'] for name in model_names]\n",
    "    \n",
    "    x = np.arange(len(model_names))\n",
    "    width = 0.35\n",
    "    \n",
    "    ax.bar(x - width/2, train_errors, width, label='Training MSE', alpha=0.8)\n",
    "    ax.bar(x + width/2, test_errors, width, label='Test MSE', alpha=0.8)\n",
    "    ax.set_title('Performance Comparison', fontweight='bold')\n",
    "    ax.set_ylabel('Mean Squared Error')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels([name.split('\\n')[0] for name in model_names], rotation=15)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Sparsity comparison\n",
    "    ax = axes[1, 2]\n",
    "    sparsities = [results[name]['sparsity'] for name in model_names]\n",
    "    bars = ax.bar(range(len(model_names)), sparsities, \n",
    "                 color=['red', 'green', 'blue'], alpha=0.7)\n",
    "    ax.set_title('Feature Selection Efficiency\\n(% Features Eliminated)', fontweight='bold')\n",
    "    ax.set_ylabel('Sparsity (%)')\n",
    "    ax.set_xticks(range(len(model_names)))\n",
    "    ax.set_xticklabels([name.split('\\n')[0] for name in model_names], rotation=15)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add percentage labels on bars\n",
    "    for bar, sparsity in zip(bars, sparsities):\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, \n",
    "               f'{sparsity:.1%}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"‚ú® Management Style Analysis:\")\n",
    "    print(f\"üè† Hoarder: Keeps everything, sparsity = {results[hoarder_key]['sparsity']:.1%}\")\n",
    "    print(f\"‚ú® Marie Kondo: Eliminates clutter, sparsity = {results[marie_key]['sparsity']:.1%}\")\n",
    "    print(f\"ü§ù Equal Opportunity: Gives everyone a chance, sparsity = {results[equal_key]['sparsity']:.1%}\")\n",
    "    print(\"\\nüí° Key Insight: L1 does feature selection, L2 does feature balancing!\")\n",
    "\n",
    "# Run the demonstration\n",
    "demonstrate_marie_kondo_vs_equal_opportunity()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Demo 3: Student Learning Patterns - Overfitting Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_student_learning_patterns():\n",
    "    \"\"\"\n",
    "    üìö Student Learning Patterns: Cramming vs Understanding\n",
    "    \n",
    "    Shows how different learning approaches lead to different generalization patterns\n",
    "    \"\"\"\n",
    "    # Simulate learning curves for different students\n",
    "    epochs = np.arange(1, 101)\n",
    "    \n",
    "    # Student A: The Understander (Healthy Learning)\n",
    "    train_acc_healthy = 0.95 * (1 - np.exp(-epochs/20)) + np.random.normal(0, 0.01, len(epochs))\n",
    "    val_acc_healthy = 0.92 * (1 - np.exp(-epochs/25)) + np.random.normal(0, 0.015, len(epochs))\n",
    "    \n",
    "    # Student B: The Crammer (Overfitting)\n",
    "    train_acc_overfit = 0.99 * (1 - np.exp(-epochs/15))\n",
    "    val_acc_overfit = 0.85 * (1 - np.exp(-epochs/30)) - 0.1 * np.maximum(0, (epochs-40)/60)\n",
    "    val_acc_overfit += np.random.normal(0, 0.02, len(epochs))\n",
    "    \n",
    "    # Student C: The Struggling (Underfitting)\n",
    "    train_acc_underfit = 0.7 * (1 - np.exp(-epochs/50)) + np.random.normal(0, 0.01, len(epochs))\n",
    "    val_acc_underfit = 0.68 * (1 - np.exp(-epochs/55)) + np.random.normal(0, 0.015, len(epochs))\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    fig.suptitle('üìö Student Learning Patterns: How Different Approaches Affect Performance', \n",
    "                fontsize=16, fontweight='bold')\n",
    "    \n",
    "    students = [\n",
    "        {\n",
    "            'name': 'Student A: The Understander\\n‚úÖ Healthy Learning',\n",
    "            'train': train_acc_healthy,\n",
    "            'val': val_acc_healthy,\n",
    "            'color': 'green',\n",
    "            'diagnosis': 'Balanced Learning'\n",
    "        },\n",
    "        {\n",
    "            'name': 'Student B: The Crammer\\n‚ö†Ô∏è Overfitting',\n",
    "            'train': train_acc_overfit,\n",
    "            'val': val_acc_overfit,\n",
    "            'color': 'red',\n",
    "            'diagnosis': 'Memorization Problem'\n",
    "        },\n",
    "        {\n",
    "            'name': 'Student C: The Struggling\\nüìà Underfitting',\n",
    "            'train': train_acc_underfit,\n",
    "            'val': val_acc_underfit,\n",
    "            'color': 'blue',\n",
    "            'diagnosis': 'Needs More Complexity'\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for idx, student in enumerate(students):\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        # Plot learning curves\n",
    "        ax.plot(epochs, student['train'], color=student['color'], linewidth=3, \n",
    "               label='Practice Test Score', alpha=0.8)\n",
    "        ax.plot(epochs, student['val'], color=student['color'], linewidth=3, \n",
    "               linestyle='--', label='Real Exam Score', alpha=0.8)\n",
    "        \n",
    "        ax.set_title(student['name'], fontweight='bold')\n",
    "        ax.set_xlabel('Study Time (epochs)')\n",
    "        ax.set_ylabel('Accuracy')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.set_ylim(0.5, 1.0)\n",
    "        \n",
    "        # Calculate final performance gap\n",
    "        final_gap = abs(student['train'][-1] - student['val'][-1])\n",
    "        final_train = student['train'][-1]\n",
    "        final_val = student['val'][-1]\n",
    "        \n",
    "        # Add diagnosis\n",
    "        ax.text(0.02, 0.98, \n",
    "               f'Final Practice: {final_train:.2f}\\nFinal Exam: {final_val:.2f}\\nGap: {final_gap:.2f}\\n\\nDiagnosis: {student[\"diagnosis\"]}',\n",
    "               transform=ax.transAxes, verticalalignment='top',\n",
    "               bbox=dict(boxstyle='round', facecolor='white', alpha=0.9))\n",
    "        \n",
    "        # Highlight overfitting point for Student B\n",
    "        if idx == 1:  # Student B\n",
    "            overfit_point = 40\n",
    "            ax.axvline(x=overfit_point, color='red', linestyle=':', alpha=0.7, linewidth=2)\n",
    "            ax.text(overfit_point+2, 0.9, 'Overfitting\\nStarts Here!', \n",
    "                   bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.7))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"üìä Learning Pattern Analysis:\")\n",
    "    print(\"‚úÖ Student A: Practice and exam scores stay close - healthy learning\")\n",
    "    print(\"‚ö†Ô∏è Student B: Perfect practice scores, declining exam scores - overfitting\")\n",
    "    print(\"üìà Student C: Both scores low but close - needs more study time/better method\")\n",
    "    print(\"\\nüéØ Teaching Insight: Monitor the GAP between practice and real performance!\")\n",
    "\n",
    "# Run the demonstration\n",
    "simulate_student_learning_patterns()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üõ°Ô∏è HOUR 2: CLASSICAL REGULARIZATION TECHNIQUES\n",
    "\n",
    "## ‚ú® Demo 4: Marie Kondo vs Equal Opportunity - L1 vs L2 Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_marie_kondo_vs_equal_opportunity():\n",
    "    \"\"\"\n",
    "    ‚ú® Marie Kondo (L1) vs Equal Opportunity Employer (L2)\n",
    "    \n",
    "    Visual comparison of how L1 and L2 regularization affect feature selection\n",
    "    \"\"\"\n",
    "    # Create synthetic dataset with many features (some relevant, some noise)\n",
    "    np.random.seed(42)\n",
    "    n_samples, n_features = 100, 20\n",
    "    \n",
    "    # Create features: first 5 are relevant, rest are noise\n",
    "    X_relevant = np.random.randn(n_samples, 5)\n",
    "    X_noise = np.random.randn(n_samples, 15) * 0.1  # Low-signal noise features\n",
    "    X = np.column_stack([X_relevant, X_noise])\n",
    "    \n",
    "    # Create target: only depends on first 5 features\n",
    "    true_weights = np.array([2, -1.5, 1, -0.5, 0.8] + [0]*15)  # Only first 5 matter\n",
    "    y = X @ true_weights + np.random.randn(n_samples) * 0.1\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Build models with different regularization\n",
    "    models = {}\n",
    "    \n",
    "    # No regularization (Hoarder)\n",
    "    hoarder_key = 'Hoarder\\n(No Regularization)'\n",
    "    models[hoarder_key] = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(1, input_shape=(n_features,), use_bias=False)\n",
    "    ])\n",
    "    \n",
    "    # L1 regularization (Marie Kondo)\n",
    "    marie_key = 'Marie Kondo\\n(L1 Regularization)'\n",
    "    models[marie_key] = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(1, input_shape=(n_features,), use_bias=False,\n",
    "                            kernel_regularizer=tf.keras.regularizers.l1(0.1))\n",
    "    ])\n",
    "    \n",
    "    # L2 regularization (Equal Opportunity)\n",
    "    equal_key = 'Equal Opportunity\\n(L2 Regularization)'\n",
    "    models[equal_key] = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(1, input_shape=(n_features,), use_bias=False,\n",
    "                            kernel_regularizer=tf.keras.regularizers.l2(0.1))\n",
    "    ])\n",
    "    \n",
    "    # Train all models\n",
    "    results = {}\n",
    "    for name, model in models.items():\n",
    "        model.compile(optimizer='adam', loss='mse')\n",
    "        model.fit(X_train, y_train, epochs=200, verbose=0)\n",
    "        \n",
    "        weights = model.layers[0].get_weights()[0].flatten()\n",
    "        train_pred = model.predict(X_train, verbose=0).flatten()\n",
    "        test_pred = model.predict(X_test, verbose=0).flatten()\n",
    "        \n",
    "        results[name] = {\n",
    "            'weights': weights,\n",
    "            'train_mse': mean_squared_error(y_train, train_pred),\n",
    "            'test_mse': mean_squared_error(y_test, test_pred),\n",
    "            'sparsity': np.sum(np.abs(weights) < 0.01) / len(weights)\n",
    "        }\n",
    "    \n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    fig.suptitle('‚ú® Marie Kondo vs Equal Opportunity Employer: Feature Management Styles', \n",
    "                fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Top row: Weight distributions\n",
    "    feature_names = [f'Important_{i+1}' if i < 5 else f'Noise_{i-4}' for i in range(n_features)]\n",
    "    colors = ['red' if i < 5 else 'gray' for i in range(n_features)]\n",
    "    \n",
    "    for idx, (name, result) in enumerate(results.items()):\n",
    "        ax = axes[0, idx]\n",
    "        bars = ax.bar(range(n_features), result['weights'], color=colors, alpha=0.7)\n",
    "        ax.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "        ax.set_title(f'{name}\\nFeature Weights', fontweight='bold')\n",
    "        ax.set_xlabel('Features')\n",
    "        ax.set_ylabel('Weight Value')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add performance metrics\n",
    "        ax.text(0.02, 0.98, \n",
    "               f'Train MSE: {result[\"train_mse\"]:.3f}\\nTest MSE: {result[\"test_mse\"]:.3f}\\nSparsity: {result[\"sparsity\"]:.1%}',\n",
    "               transform=ax.transAxes, verticalalignment='top',\n",
    "               bbox=dict(boxstyle='round', facecolor='white', alpha=0.9))\n",
    "    \n",
    "    # Bottom row: Weight magnitude comparisons\n",
    "    weight_comparison = np.array([result['weights'] for result in results.values()])\n",
    "    \n",
    "    # Histogram of weight magnitudes\n",
    "    ax = axes[1, 0]\n",
    "    for idx, (name, result) in enumerate(results.items()):\n",
    "        ax.hist(np.abs(result['weights']), bins=20, alpha=0.6, label=name)\n",
    "    ax.set_title('Weight Magnitude Distribution', fontweight='bold')\n",
    "    ax.set_xlabel('|Weight Value|')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Performance comparison\n",
    "    ax = axes[1, 1]\n",
    "    model_names = list(results.keys())\n",
    "    train_errors = [results[name]['train_mse'] for name in model_names]\n",
    "    test_errors = [results[name]['test_mse'] for name in model_names]\n",
    "    \n",
    "    x = np.arange(len(model_names))\n",
    "    width = 0.35\n",
    "    \n",
    "    ax.bar(x - width/2, train_errors, width, label='Training MSE', alpha=0.8)\n",
    "    ax.bar(x + width/2, test_errors, width, label='Test MSE', alpha=0.8)\n",
    "    ax.set_title('Performance Comparison', fontweight='bold')\n",
    "    ax.set_ylabel('Mean Squared Error')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels([name.split('\\n')[0] for name in model_names], rotation=15)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Sparsity comparison\n",
    "    ax = axes[1, 2]\n",
    "    sparsities = [results[name]['sparsity'] for name in model_names]\n",
    "    bars = ax.bar(range(len(model_names)), sparsities, \n",
    "                 color=['red', 'green', 'blue'], alpha=0.7)\n",
    "    ax.set_title('Feature Selection Efficiency\\n(% Features Eliminated)', fontweight='bold')\n",
    "    ax.set_ylabel('Sparsity (%)')\n",
    "    ax.set_xticks(range(len(model_names)))\n",
    "    ax.set_xticklabels([name.split('\\n')[0] for name in model_names], rotation=15)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add percentage labels on bars\n",
    "    for bar, sparsity in zip(bars, sparsities):\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, \n",
    "               f'{sparsity:.1%}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"‚ú® Management Style Analysis:\")\n",
    "    print(f\"üè† Hoarder: Keeps everything, sparsity = {results[hoarder_key]['sparsity']:.1%}\")\n",
    "    print(f\"‚ú® Marie Kondo: Eliminates clutter, sparsity = {results[marie_key]['sparsity']:.1%}\")\n",
    "    print(f\"ü§ù Equal Opportunity: Gives everyone a chance, sparsity = {results[equal_key]['sparsity']:.1%}\")\n",
    "    print(\"\\nüí° Key Insight: L1 does feature selection, L2 does feature balancing!\")\n",
    "\n",
    "# Run the demonstration\n",
    "demonstrate_marie_kondo_vs_equal_opportunity()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíº Demo 5: Investment Portfolio Theory - L2 Regularization Deep Dive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_investment_portfolio_theory():\n",
    "    \"\"\"\n",
    "    üíº Investment Portfolio Theory: L2 Regularization as Risk Management\n",
    "    \n",
    "    Shows how L2 regularization distributes weight risk like a diversified portfolio\n",
    "    \"\"\"\n",
    "    # Create correlated features (like correlated stocks)\n",
    "    np.random.seed(42)\n",
    "    n_samples = 200\n",
    "    \n",
    "    # Create base features (market sectors)\n",
    "    tech_sector = np.random.randn(n_samples)\n",
    "    health_sector = np.random.randn(n_samples)\n",
    "    energy_sector = np.random.randn(n_samples)\n",
    "    \n",
    "    # Create correlated features within each sector\n",
    "    tech_stocks = np.column_stack([\n",
    "        tech_sector + np.random.randn(n_samples) * 0.3,  # Apple-like\n",
    "        tech_sector + np.random.randn(n_samples) * 0.3,  # Google-like\n",
    "        tech_sector + np.random.randn(n_samples) * 0.3,  # Microsoft-like\n",
    "    ])\n",
    "    \n",
    "    health_stocks = np.column_stack([\n",
    "        health_sector + np.random.randn(n_samples) * 0.3,  # Pfizer-like\n",
    "        health_sector + np.random.randn(n_samples) * 0.3,  # J&J-like\n",
    "    ])\n",
    "    \n",
    "    energy_stocks = np.column_stack([\n",
    "        energy_sector + np.random.randn(n_samples) * 0.3,  # Exxon-like\n",
    "        energy_sector + np.random.randn(n_samples) * 0.3,  # Chevron-like\n",
    "    ])\n",
    "    \n",
    "    # Combine all stocks\n",
    "    X = np.column_stack([tech_stocks, health_stocks, energy_stocks])\n",
    "    n_features = X.shape[1]\n",
    "    \n",
    "    # Create target (portfolio returns)\n",
    "    true_returns = np.array([1.2, 1.0, 0.8, 0.9, 1.1, 0.7, 0.6])  # Expected returns per stock\n",
    "    y = X @ true_returns + np.random.randn(n_samples) * 0.1\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Different investment strategies (regularization strengths)\n",
    "    strategies = {\n",
    "        'Risky Investor\\n(Œª=0)': 0.0,\n",
    "        'Balanced Investor\\n(Œª=0.01)': 0.01,\n",
    "        'Conservative Investor\\n(Œª=0.1)': 0.1,\n",
    "        'Ultra Conservative\\n(Œª=1.0)': 1.0\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for strategy_name, lambda_value in strategies.items():\n",
    "        # Build model with L2 regularization\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(1, input_shape=(n_features,), use_bias=False,\n",
    "                                kernel_regularizer=tf.keras.regularizers.l2(lambda_value))\n",
    "        ])\n",
    "        \n",
    "        model.compile(optimizer='adam', loss='mse')\n",
    "        model.fit(X_train, y_train, epochs=200, verbose=0)\n",
    "        \n",
    "        weights = model.layers[0].get_weights()[0].flatten()\n",
    "        train_pred = model.predict(X_train, verbose=0).flatten()\n",
    "        test_pred = model.predict(X_test, verbose=0).flatten()\n",
    "        \n",
    "        # Calculate portfolio metrics\n",
    "        portfolio_concentration = np.max(np.abs(weights)) / np.mean(np.abs(weights))  # Risk concentration\n",
    "        portfolio_volatility = np.std(weights)  # Weight volatility\n",
    "        \n",
    "        results[strategy_name] = {\n",
    "            'weights': weights,\n",
    "            'train_mse': mean_squared_error(y_train, train_pred),\n",
    "            'test_mse': mean_squared_error(y_test, test_pred),\n",
    "            'concentration': portfolio_concentration,\n",
    "            'volatility': portfolio_volatility,\n",
    "            'lambda': lambda_value\n",
    "        }\n",
    "    \n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('üíº Investment Portfolio Theory: L2 Regularization as Risk Management', \n",
    "                fontsize=16, fontweight='bold')\n",
    "    \n",
    "    stock_names = ['AAPL', 'GOOGL', 'MSFT', 'PFE', 'JNJ', 'XOM', 'CVX']\n",
    "    colors = ['blue', 'blue', 'blue', 'green', 'green', 'red', 'red']  # Sector colors\n",
    "    \n",
    "    # Portfolio allocation for each strategy\n",
    "    ax = axes[0, 0]\n",
    "    x = np.arange(len(stock_names))\n",
    "    width = 0.2\n",
    "    \n",
    "    for idx, (strategy_name, result) in enumerate(results.items()):\n",
    "        ax.bar(x + idx*width, result['weights'], width, \n",
    "              label=strategy_name.split('\\n')[0], alpha=0.8)\n",
    "    \n",
    "    ax.set_title('Portfolio Allocation by Strategy', fontweight='bold')\n",
    "    ax.set_xlabel('Stocks')\n",
    "    ax.set_ylabel('Weight')\n",
    "    ax.set_xticks(x + width * 1.5)\n",
    "    ax.set_xticklabels(stock_names)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Risk-Return Analysis\n",
    "    ax = axes[0, 1]\n",
    "    concentrations = [result['concentration'] for result in results.values()]\n",
    "    test_errors = [result['test_mse'] for result in results.values()]\n",
    "    lambdas = [result['lambda'] for result in results.values()]\n",
    "    \n",
    "    scatter = ax.scatter(concentrations, test_errors, s=200, c=lambdas, \n",
    "                        cmap='viridis', alpha=0.8)\n",
    "    \n",
    "    for i, strategy in enumerate(results.keys()):\n",
    "        ax.annotate(strategy.split('\\n')[0], \n",
    "                   (concentrations[i], test_errors[i]),\n",
    "                   xytext=(5, 5), textcoords='offset points',\n",
    "                   fontsize=9, alpha=0.8)\n",
    "    \n",
    "    ax.set_title('Risk-Return Analysis', fontweight='bold')\n",
    "    ax.set_xlabel('Portfolio Concentration (Risk)')\n",
    "    ax.set_ylabel('Test Error (Return Quality)')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.colorbar(scatter, ax=ax, label='Lambda (Regulation Strength)')\n",
    "    \n",
    "    # Lambda vs Performance\n",
    "    ax = axes[1, 0]\n",
    "    lambdas_plot = [result['lambda'] for result in results.values()]\n",
    "    train_errors = [result['train_mse'] for result in results.values()]\n",
    "    test_errors = [result['test_mse'] for result in results.values()]\n",
    "    \n",
    "    ax.plot(lambdas_plot, train_errors, 'o-', label='Training Error', linewidth=2, markersize=8)\n",
    "    ax.plot(lambdas_plot, test_errors, 's-', label='Test Error', linewidth=2, markersize=8)\n",
    "    ax.set_title('Regularization Strength vs Performance', fontweight='bold')\n",
    "    ax.set_xlabel('Lambda (Regularization Strength)')\n",
    "    ax.set_ylabel('Mean Squared Error')\n",
    "    ax.set_xscale('log')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Portfolio diversification metrics\n",
    "    ax = axes[1, 1]\n",
    "    strategies_short = [name.split('\\n')[0] for name in results.keys()]\n",
    "    concentrations = [result['concentration'] for result in results.values()]\n",
    "    volatilities = [result['volatility'] for result in results.values()]\n",
    "    \n",
    "    x = np.arange(len(strategies_short))\n",
    "    width = 0.35\n",
    "    \n",
    "    ax.bar(x - width/2, concentrations, width, label='Concentration Risk', alpha=0.8)\n",
    "    ax2 = ax.twinx()\n",
    "    ax2.bar(x + width/2, volatilities, width, label='Weight Volatility', \n",
    "           alpha=0.8, color='orange')\n",
    "    \n",
    "    ax.set_title('Portfolio Risk Metrics', fontweight='bold')\n",
    "    ax.set_xlabel('Investment Strategy')\n",
    "    ax.set_ylabel('Concentration Risk', color='blue')\n",
    "    ax2.set_ylabel('Weight Volatility', color='orange')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(strategies_short, rotation=15)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add legends\n",
    "    ax.legend(loc='upper left')\n",
    "    ax2.legend(loc='upper right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"üíº Investment Strategy Analysis:\")\n",
    "    for strategy, result in results.items():\n",
    "        print(f\"{strategy}: Concentration={result['concentration']:.2f}, Volatility={result['volatility']:.3f}, Test MSE={result['test_mse']:.3f}\")\n",
    "    \n",
    "    print(\"\\nüí° Key Insights:\")\n",
    "    print(\"üìà Higher Œª ‚Üí More diversified portfolio (lower concentration)\")\n",
    "    print(\"‚öñÔ∏è L2 regularization prevents over-investment in any single feature\")\n",
    "    print(\"üéØ Sweet spot balances bias (diversification) vs variance (specialization)\")\n",
    "\n",
    "# Run the demonstration\n",
    "demonstrate_investment_portfolio_theory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üè• Demo 6: The Model Doctor - Complete Diagnostic System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelDoctor:\n",
    "    \"\"\"\n",
    "    üè• The Model Doctor: Complete diagnostic and treatment system for ML models\n",
    "    \n",
    "    Like a medical doctor, diagnoses problems and prescribes treatments\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.diagnostic_history = []\n",
    "        \n",
    "    def examine_patient(self, train_acc, val_acc, train_loss, val_loss, epochs):\n",
    "        \"\"\"\n",
    "        üîç Comprehensive health examination of the model\n",
    "        \"\"\"\n",
    "        symptoms = {\n",
    "            'final_train_acc': train_acc[-1],\n",
    "            'final_val_acc': val_acc[-1],\n",
    "            'final_train_loss': train_loss[-1],\n",
    "            'final_val_loss': val_loss[-1],\n",
    "            'acc_gap': abs(train_acc[-1] - val_acc[-1]),\n",
    "            'loss_gap': abs(train_loss[-1] - val_loss[-1]),\n",
    "            'val_trend': self._calculate_trend(val_acc[-20:]),  # Last 20 epochs\n",
    "            'train_trend': self._calculate_trend(train_acc[-20:])\n",
    "        }\n",
    "        \n",
    "        return symptoms\n",
    "    \n",
    "    def _calculate_trend(self, values):\n",
    "        \"\"\"Calculate if trend is improving, stable, or declining\"\"\"\n",
    "        if len(values) < 5:\n",
    "            return 'insufficient_data'\n",
    "        \n",
    "        # Simple linear trend\n",
    "        x = np.arange(len(values))\n",
    "        slope = np.polyfit(x, values, 1)[0]\n",
    "        \n",
    "        if slope > 0.001:\n",
    "            return 'improving'\n",
    "        elif slope < -0.001:\n",
    "            return 'declining'\n",
    "        else:\n",
    "            return 'stable'\n",
    "    \n",
    "    def diagnose(self, symptoms):\n",
    "        \"\"\"\n",
    "        ü©∫ Diagnose the model's condition\n",
    "        \"\"\"\n",
    "        diagnosis = []\n",
    "        severity = 'healthy'\n",
    "        \n",
    "        # Check for overfitting\n",
    "        if (symptoms['final_train_acc'] > 0.9 and \n",
    "            symptoms['final_val_acc'] < 0.8 and \n",
    "            symptoms['acc_gap'] > 0.15):\n",
    "            diagnosis.append(\"Severe Overfitting - Patient is memorizing instead of learning\")\n",
    "            severity = 'critical'\n",
    "        elif symptoms['acc_gap'] > 0.1:\n",
    "            diagnosis.append(\"Mild Overfitting - Some memorization detected\")\n",
    "            severity = 'moderate'\n",
    "        \n",
    "        # Check for underfitting\n",
    "        if (symptoms['final_train_acc'] < 0.8 and \n",
    "            symptoms['final_val_acc'] < 0.8 and \n",
    "            symptoms['acc_gap'] < 0.05):\n",
    "            diagnosis.append(\"Underfitting - Patient needs more complexity to learn\")\n",
    "            severity = 'moderate'\n",
    "        \n",
    "        # Check training progress\n",
    "        if symptoms['val_trend'] == 'declining':\n",
    "            diagnosis.append(\"Performance Degradation - Validation getting worse\")\n",
    "            severity = 'moderate' if severity == 'healthy' else severity\n",
    "        \n",
    "        if not diagnosis:\n",
    "            diagnosis.append(\"Healthy Model - Patient is learning well\")\n",
    "        \n",
    "        return {\n",
    "            'diagnosis': diagnosis,\n",
    "            'severity': severity,\n",
    "            'confidence': self._calculate_confidence(symptoms)\n",
    "        }\n",
    "    \n",
    "    def _calculate_confidence(self, symptoms):\n",
    "        \"\"\"Calculate diagnostic confidence based on clear symptoms\"\"\"\n",
    "        confidence = 0.5  # Base confidence\n",
    "        \n",
    "        # Clear performance gap increases confidence\n",
    "        if symptoms['acc_gap'] > 0.2:\n",
    "            confidence += 0.3\n",
    "        elif symptoms['acc_gap'] > 0.1:\n",
    "            confidence += 0.2\n",
    "        \n",
    "        # Clear trends increase confidence\n",
    "        if symptoms['val_trend'] in ['declining', 'improving']:\n",
    "            confidence += 0.2\n",
    "        \n",
    "        return min(confidence, 1.0)\n",
    "    \n",
    "    def prescribe_treatment(self, diagnosis_result):\n",
    "        \"\"\"\n",
    "        üíä Prescribe treatment based on diagnosis\n",
    "        \"\"\"\n",
    "        treatments = []\n",
    "        \n",
    "        for diagnosis in diagnosis_result['diagnosis']:\n",
    "            if 'Overfitting' in diagnosis:\n",
    "                if 'Severe' in diagnosis:\n",
    "                    treatments.extend([\n",
    "                        \"üßπ L1 Regularization (Œª=0.01-0.1) - Marie Kondo treatment\",\n",
    "                        \"‚öñÔ∏è L2 Regularization (Œª=0.001-0.01) - Portfolio balancing\",\n",
    "                        \"üõë Early Stopping - Prevent further memorization\",\n",
    "                        \"üíΩ More Training Data - Increase experience diversity\",\n",
    "                        \"üéØ Reduce Model Complexity - Simplify architecture\"\n",
    "                    ])\n",
    "                else:\n",
    "                    treatments.extend([\n",
    "                        \"‚öñÔ∏è L2 Regularization (Œª=0.001) - Light portfolio balancing\",\n",
    "                        \"üõë Early Stopping - Monitor and stop at optimal point\",\n",
    "                        \"üíΩ Data Augmentation - Increase data diversity\"\n",
    "                    ])\n",
    "            \n",
    "            elif 'Underfitting' in diagnosis:\n",
    "                treatments.extend([\n",
    "                    \"üß† Increase Model Complexity - Add more layers/neurons\",\n",
    "                    \"üìâ Reduce Regularization - Lower Œª values\",\n",
    "                    \"üîß Feature Engineering - Create more informative features\",\n",
    "                    \"‚è∞ Train Longer - Increase epochs\",\n",
    "                    \"üìä Check Data Quality - Ensure sufficient signal\"\n",
    "                ])\n",
    "            \n",
    "            elif 'Performance Degradation' in diagnosis:\n",
    "                treatments.extend([\n",
    "                    \"üõë Early Stopping - Stop before degradation\",\n",
    "                    \"üìâ Learning Rate Scheduling - Reduce learning rate\",\n",
    "                    \"üîÑ Model Checkpointing - Save best version\"\n",
    "                ])\n",
    "            \n",
    "            elif 'Healthy' in diagnosis:\n",
    "                treatments.extend([\n",
    "                    \"‚úÖ Continue Current Approach - Model is healthy\",\n",
    "                    \"üîç Monitor Closely - Watch for changes\",\n",
    "                    \"üöÄ Consider Deployment - Ready for production\",\n",
    "                    \"üéØ Fine-tune Hyperparameters - Optimize further\"\n",
    "                ])\n",
    "        \n",
    "        return list(set(treatments))  # Remove duplicates\n",
    "    \n",
    "    def generate_medical_report(self, symptoms, diagnosis_result, treatments):\n",
    "        \"\"\"\n",
    "        üìã Generate comprehensive medical report\n",
    "        \"\"\"\n",
    "        report = f\"\"\"\n",
    "        üè• MODEL MEDICAL REPORT\n",
    "        ========================\n",
    "        \n",
    "        üìä VITAL SIGNS:\n",
    "        - Training Accuracy: {symptoms['final_train_acc']:.3f}\n",
    "        - Validation Accuracy: {symptoms['final_val_acc']:.3f}\n",
    "        - Performance Gap: {symptoms['acc_gap']:.3f}\n",
    "        - Validation Trend: {symptoms['val_trend']}\n",
    "        \n",
    "        ü©∫ DIAGNOSIS:\n",
    "        Severity: {diagnosis_result['severity'].upper()}\n",
    "        Confidence: {diagnosis_result['confidence']:.1%}\n",
    "        \n",
    "        Conditions Detected:\n",
    "        \"\"\"\n",
    "        \n",
    "        for i, diag in enumerate(diagnosis_result['diagnosis'], 1):\n",
    "            report += f\"        {i}. {diag}\\n\"\n",
    "        \n",
    "        report += f\"\"\"\n",
    "        üíä PRESCRIBED TREATMENTS:\n",
    "        \"\"\"\n",
    "        \n",
    "        for i, treatment in enumerate(treatments, 1):\n",
    "            report += f\"        {i}. {treatment}\\n\"\n",
    "        \n",
    "        report += f\"\"\"\n",
    "        üìÖ FOLLOW-UP:\n",
    "        - Monitor training progress closely\n",
    "        - Re-evaluate after implementing treatments\n",
    "        - Schedule check-up in 10-20 epochs\n",
    "        \n",
    "        üë®‚Äç‚öïÔ∏è Dr. Neural Network, MD\n",
    "        Specialist in Deep Learning Health\n",
    "        \"\"\"\n",
    "        \n",
    "        return report\n",
    "\n",
    "def demonstrate_model_doctor():\n",
    "    \"\"\"\n",
    "    üè• Demonstrate the Model Doctor in action with different patient cases\n",
    "    \"\"\"\n",
    "    print(\"üè• MODEL DOCTOR DEMONSTRATION\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    doctor = ModelDoctor()\n",
    "    \n",
    "    # Create different patient scenarios\n",
    "    patients = {\n",
    "        \"Patient A - The Healthy Student\": {\n",
    "            'train_acc': np.concatenate([np.linspace(0.5, 0.85, 80), np.full(20, 0.85)]),\n",
    "            'val_acc': np.concatenate([np.linspace(0.5, 0.82, 80), np.full(20, 0.82)]),\n",
    "            'train_loss': np.concatenate([np.linspace(1.5, 0.4, 80), np.full(20, 0.4)]),\n",
    "            'val_loss': np.concatenate([np.linspace(1.5, 0.45, 80), np.full(20, 0.45)])\n",
    "        },\n",
    "        \"Patient B - The Crammer (Overfitting)\": {\n",
    "            'train_acc': np.concatenate([np.linspace(0.5, 0.99, 50), np.full(50, 0.99)]),\n",
    "            'val_acc': np.concatenate([np.linspace(0.5, 0.78, 30), np.linspace(0.78, 0.65, 70)]),\n",
    "            'train_loss': np.concatenate([np.linspace(1.5, 0.01, 50), np.full(50, 0.01)]),\n",
    "            'val_loss': np.concatenate([np.linspace(1.5, 0.6, 30), np.linspace(0.6, 1.2, 70)])\n",
    "        },\n",
    "        \"Patient C - The Struggling (Underfitting)\": {\n",
    "            'train_acc': np.full(100, 0.65),\n",
    "            'val_acc': np.full(100, 0.63),\n",
    "            'train_loss': np.full(100, 0.9),\n",
    "            'val_loss': np.full(100, 0.95)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    for patient_name, metrics in patients.items():\n",
    "        print(f\"\\nüè• EXAMINING {patient_name.upper()}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        # Examine patient\n",
    "        symptoms = doctor.examine_patient(\n",
    "            metrics['train_acc'], metrics['val_acc'],\n",
    "            metrics['train_loss'], metrics['val_loss'],\n",
    "            epochs=100\n",
    "        )\n",
    "        \n",
    "        # Diagnose\n",
    "        diagnosis_result = doctor.diagnose(symptoms)\n",
    "        \n",
    "        # Prescribe treatment\n",
    "        treatments = doctor.prescribe_treatment(diagnosis_result)\n",
    "        \n",
    "        # Generate and print report\n",
    "        report = doctor.generate_medical_report(symptoms, diagnosis_result, treatments)\n",
    "        print(report)\n",
    "        \n",
    "        # Visual diagnosis\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        epochs = range(1, 101)\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(epochs, metrics['train_acc'], 'b-', label='Training Accuracy', linewidth=2)\n",
    "        plt.plot(epochs, metrics['val_acc'], 'r--', label='Validation Accuracy', linewidth=2)\n",
    "        plt.title(f'{patient_name}\\nAccuracy Progression')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(epochs, metrics['train_loss'], 'b-', label='Training Loss', linewidth=2)\n",
    "        plt.plot(epochs, metrics['val_loss'], 'r--', label='Validation Loss', linewidth=2)\n",
    "        plt.title('Loss Progression')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    print(\"\\nüéØ MODEL DOCTOR SUMMARY:\")\n",
    "    print(\"‚úÖ Healthy models show small, stable train-validation gaps\")\n",
    "    print(\"‚ö†Ô∏è Overfitting shows large gaps and declining validation performance\")\n",
    "    print(\"üìà Underfitting shows poor performance on both training and validation\")\n",
    "    print(\"üíä Each condition requires different treatment approaches\")\n",
    "\n",
    "# Run the demonstration\n",
    "demonstrate_model_doctor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéØ ASSESSMENT & INTERACTIVE EXERCISES\n",
    "\n",
    "## üß© Exercise 1: Identify the Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive_problem_identification():\n",
    "    \"\"\"\n",
    "    üß© Interactive exercise: Students identify problems from learning curves\n",
    "    \"\"\"\n",
    "    print(\"üéØ INTERACTIVE EXERCISE: Problem Identification\")\n",
    "    print(\"Study the learning curves and identify the problem!\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Create mystery cases\n",
    "    np.random.seed(123)\n",
    "    epochs = range(1, 101)\n",
    "    \n",
    "    cases = {\n",
    "        \"Mystery Case A\": {\n",
    "            'train_acc': 0.95 * (1 - np.exp(-np.array(epochs)/15)) + np.random.normal(0, 0.01, 100),\n",
    "            'val_acc': 0.88 * (1 - np.exp(-np.array(epochs)/20)) + np.random.normal(0, 0.015, 100),\n",
    "            'answer': 'Healthy Learning'\n",
    "        },\n",
    "        \"Mystery Case B\": {\n",
    "            'train_acc': 0.99 * (1 - np.exp(-np.array(epochs)/10)),\n",
    "            'val_acc': np.concatenate([0.8 * (1 - np.exp(-np.array(epochs[:40])/15)), \n",
    "                                     0.8 - 0.15 * np.array(epochs[40:])/60]),\n",
    "            'answer': 'Overfitting'\n",
    "        },\n",
    "        \"Mystery Case C\": {\n",
    "            'train_acc': 0.7 * np.ones(100) + np.random.normal(0, 0.02, 100),\n",
    "            'val_acc': 0.68 * np.ones(100) + np.random.normal(0, 0.02, 100),\n",
    "            'answer': 'Underfitting'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    fig.suptitle('üîç Mystery Cases: What\\'s Wrong With These Models?', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    for idx, (case_name, data) in enumerate(cases.items()):\n",
    "        ax = axes[idx]\n",
    "        ax.plot(epochs, data['train_acc'], 'b-', label='Training Accuracy', linewidth=2)\n",
    "        ax.plot(epochs, data['val_acc'], 'r--', label='Validation Accuracy', linewidth=2)\n",
    "        ax.set_title(case_name, fontweight='bold')\n",
    "        ax.set_xlabel('Epochs')\n",
    "        ax.set_ylabel('Accuracy')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.set_ylim(0.5, 1.0)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nü§î QUESTIONS FOR STUDENTS:\")\n",
    "    print(\"1. Which case shows healthy learning? Why?\")\n",
    "    print(\"2. Which case needs regularization? What type?\")\n",
    "    print(\"3. Which case needs more model complexity?\")\n",
    "    print(\"4. What would you do to fix each problematic case?\")\n",
    "    \n",
    "    # Reveal answers after students discuss\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"üéØ ANSWERS:\")\n",
    "    for case_name, data in cases.items():\n",
    "        print(f\"- {case_name}: {data['answer']}\")\n",
    "    \n",
    "    print(\"\\nüí° TREATMENT RECOMMENDATIONS:\")\n",
    "    print(\"- Case A: Continue current approach ‚úÖ\")\n",
    "    print(\"- Case B: Apply L1/L2 regularization, early stopping üõë\")\n",
    "    print(\"- Case C: Increase model complexity, reduce regularization üìà\")\n",
    "\n",
    "# Run the exercise\n",
    "interactive_problem_identification()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Exercise 2: Regularization Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regularization_tuning_exercise():\n",
    "    \"\"\"\n",
    "    üìä Interactive exercise: Find the optimal regularization parameter\n",
    "    \"\"\"\n",
    "    print(\"üéõÔ∏è INTERACTIVE EXERCISE: Regularization Parameter Tuning\")\n",
    "    print(\"Find the sweet spot for lambda (Œª) values!\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Create overfitting-prone dataset\n",
    "    np.random.seed(42)\n",
    "    X, y = make_regression(n_samples=100, n_features=50, noise=0.1, random_state=42)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Test different lambda values\n",
    "    lambda_values = [0.0, 0.001, 0.01, 0.1, 1.0, 10.0]\n",
    "    \n",
    "    l1_results = {'train_mse': [], 'test_mse': [], 'sparsity': []}\n",
    "    l2_results = {'train_mse': [], 'test_mse': [], 'sparsity': []}\n",
    "    \n",
    "    print(\"üîÑ Testing different lambda values...\")\n",
    "    \n",
    "    for lambda_val in lambda_values:\n",
    "        # L1 model\n",
    "        if lambda_val == 0.0:\n",
    "            l1_model = tf.keras.Sequential([tf.keras.layers.Dense(1, input_shape=(50,))])\n",
    "        else:\n",
    "            l1_model = tf.keras.Sequential([\n",
    "                tf.keras.layers.Dense(1, input_shape=(50,),\n",
    "                                    kernel_regularizer=tf.keras.regularizers.l1(lambda_val))\n",
    "            ])\n",
    "        \n",
    "        l1_model.compile(optimizer='adam', loss='mse')\n",
    "        l1_model.fit(X_train, y_train, epochs=100, verbose=0)\n",
    "        \n",
    "        l1_train_pred = l1_model.predict(X_train, verbose=0)\n",
    "        l1_test_pred = l1_model.predict(X_test, verbose=0)\n",
    "        l1_weights = l1_model.layers[0].get_weights()[0].flatten()\n",
    "        \n",
    "        l1_results['train_mse'].append(mean_squared_error(y_train, l1_train_pred))\n",
    "        l1_results['test_mse'].append(mean_squared_error(y_test, l1_test_pred))\n",
    "        l1_results['sparsity'].append(np.sum(np.abs(l1_weights) < 0.01) / len(l1_weights))\n",
    "        \n",
    "        # L2 model\n",
    "        if lambda_val == 0.0:\n",
    "            l2_model = tf.keras.Sequential([tf.keras.layers.Dense(1, input_shape=(50,))])\n",
    "        else:\n",
    "            l2_model = tf.keras.Sequential([\n",
    "                tf.keras.layers.Dense(1, input_shape=(50,),\n",
    "                                    kernel_regularizer=tf.keras.regularizers.l2(lambda_val))\n",
    "            ])\n",
    "        \n",
    "        l2_model.compile(optimizer='adam', loss='mse')\n",
    "        l2_model.fit(X_train, y_train, epochs=100, verbose=0)\n",
    "        \n",
    "        l2_train_pred = l2_model.predict(X_train, verbose=0)\n",
    "        l2_test_pred = l2_model.predict(X_test, verbose=0)\n",
    "        l2_weights = l2_model.layers[0].get_weights()[0].flatten()\n",
    "        \n",
    "        l2_results['train_mse'].append(mean_squared_error(y_train, l2_train_pred))\n",
    "        l2_results['test_mse'].append(mean_squared_error(y_test, l2_test_pred))\n",
    "        l2_results['sparsity'].append(np.sum(np.abs(l2_weights) < 0.01) / len(l2_weights))\n",
    "    \n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('üéõÔ∏è Lambda Tuning: Finding the Sweet Spot', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Training vs Test Error for L1\n",
    "    ax = axes[0, 0]\n",
    "    ax.plot(lambda_values, l1_results['train_mse'], 'o-', label='Training MSE', linewidth=2, markersize=8)\n",
    "    ax.plot(lambda_values, l1_results['test_mse'], 's-', label='Test MSE', linewidth=2, markersize=8)\n",
    "    ax.set_title('L1 Regularization: Performance vs Lambda', fontweight='bold')\n",
    "    ax.set_xlabel('Lambda (Œª)')\n",
    "    ax.set_ylabel('Mean Squared Error')\n",
    "    ax.set_xscale('log')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Mark optimal point for L1\n",
    "    optimal_l1_idx = np.argmin(l1_results['test_mse'])\n",
    "    ax.scatter([lambda_values[optimal_l1_idx]], [l1_results['test_mse'][optimal_l1_idx]], \n",
    "              color='red', s=200, marker='*', zorder=5, label='Optimal Œª')\n",
    "    \n",
    "    # Training vs Test Error for L2\n",
    "    ax = axes[0, 1]\n",
    "    ax.plot(lambda_values, l2_results['train_mse'], 'o-', label='Training MSE', linewidth=2, markersize=8)\n",
    "    ax.plot(lambda_values, l2_results['test_mse'], 's-', label='Test MSE', linewidth=2, markersize=8)\n",
    "    ax.set_title('L2 Regularization: Performance vs Lambda', fontweight='bold')\n",
    "    ax.set_xlabel('Lambda (Œª)')\n",
    "    ax.set_ylabel('Mean Squared Error')\n",
    "    ax.set_xscale('log')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Mark optimal point for L2\n",
    "    optimal_l2_idx = np.argmin(l2_results['test_mse'])\n",
    "    ax.scatter([lambda_values[optimal_l2_idx]], [l2_results['test_mse'][optimal_l2_idx]], \n",
    "              color='red', s=200, marker='*', zorder=5, label='Optimal Œª')\n",
    "    \n",
    "    # Sparsity comparison\n",
    "    ax = axes[1, 0]\n",
    "    ax.plot(lambda_values, l1_results['sparsity'], 'go-', label='L1 Sparsity', linewidth=3, markersize=8)\n",
    "    ax.plot(lambda_values, l2_results['sparsity'], 'bo-', label='L2 Sparsity', linewidth=3, markersize=8)\n",
    "    ax.set_title('Feature Selection: Sparsity vs Lambda', fontweight='bold')\n",
    "    ax.set_xlabel('Lambda (Œª)')\n",
    "    ax.set_ylabel('Sparsity (% features eliminated)')\n",
    "    ax.set_xscale('log')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Bias-Variance Tradeoff Visualization\n",
    "    ax = axes[1, 1]\n",
    "    l1_gaps = np.array(l1_results['test_mse']) - np.array(l1_results['train_mse'])\n",
    "    l2_gaps = np.array(l2_results['test_mse']) - np.array(l2_results['train_mse'])\n",
    "    \n",
    "    ax.plot(lambda_values, l1_gaps, 'ro-', label='L1 Generalization Gap', linewidth=3, markersize=8)\n",
    "    ax.plot(lambda_values, l2_gaps, 'bo-', label='L2 Generalization Gap', linewidth=3, markersize=8)\n",
    "    ax.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "    ax.set_title('Generalization Gap vs Lambda', fontweight='bold')\n",
    "    ax.set_xlabel('Lambda (Œª)')\n",
    "    ax.set_ylabel('Test MSE - Train MSE')\n",
    "    ax.set_xscale('log')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nüéØ TUNING RESULTS:\")\n",
    "    print(f\"üìç Optimal L1 Lambda: {lambda_values[optimal_l1_idx]} (Test MSE: {l1_results['test_mse'][optimal_l1_idx]:.3f})\")\n",
    "    print(f\"üìç Optimal L2 Lambda: {lambda_values[optimal_l2_idx]} (Test MSE: {l2_results['test_mse'][optimal_l2_idx]:.3f})\")\n",
    "    \n",
    "    print(\"\\nüìä LAMBDA TUNING GUIDELINES:\")\n",
    "    print(\"üîç Too Small (Œª < 0.001): Minimal regularization effect\")\n",
    "    print(\"‚öñÔ∏è Just Right (Œª = 0.001-0.1): Balanced bias-variance tradeoff\")\n",
    "    print(\"üõë Too Large (Œª > 1.0): Over-regularization, high bias\")\n",
    "    \n",
    "    print(\"\\nüí° PRACTICAL TIPS:\")\n",
    "    print(\"1. Start with cross-validation to find optimal Œª\")\n",
    "    print(\"2. Monitor both training and validation performance\")\n",
    "    print(\"3. L1 typically needs higher Œª values than L2\")\n",
    "    print(\"4. The optimal Œª depends on your dataset and model\")\n",
    "\n",
    "# Run the exercise\n",
    "regularization_tuning_exercise()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéì SUMMARY & NEXT STEPS\n",
    "\n",
    "## üéØ Key Takeaways from Today's Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_session_summary():\n",
    "    \"\"\"\n",
    "    üéì Generate comprehensive summary of today's learning\n",
    "    \"\"\"\n",
    "    print(\"üéì SESSION SUMMARY: Week 6 Day 3\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    summary = {\n",
    "        \"üçΩÔ∏è Core Analogies Learned\": [\n",
    "            \"Restaurant Chef ‚Üí Bias-Variance Tradeoff\",\n",
    "            \"Archer's Target ‚Üí Error Components\", \n",
    "            \"Student Cramming ‚Üí Overfitting Detection\",\n",
    "            \"Marie Kondo ‚Üí L1 Regularization\",\n",
    "            \"Equal Opportunity ‚Üí L2 Regularization\",\n",
    "            \"Investment Portfolio ‚Üí Risk Diversification\"\n",
    "        ],\n",
    "        \n",
    "        \"üìä Mathematical Concepts Mastered\": [\n",
    "            \"Total Error = Bias¬≤ + Variance + Irreducible Error\",\n",
    "            \"L1 Penalty: Œª‚àë|w·µ¢| (Diamond constraint)\",\n",
    "            \"L2 Penalty: Œª‚àëw·µ¢¬≤ (Circle constraint)\",\n",
    "            \"Sparsity vs Weight Smoothing\",\n",
    "            \"Hyperparameter Tuning Strategies\"\n",
    "        ],\n",
    "        \n",
    "        \"üíª Practical Skills Developed\": [\n",
    "            \"TensorFlow L1/L2 implementation\",\n",
    "            \"Overfitting detection systems\",\n",
    "            \"Learning curve analysis\",\n",
    "            \"Model diagnostic tools\",\n",
    "            \"Regularization parameter tuning\"\n",
    "        ],\n",
    "        \n",
    "        \"üéØ Assessment Readiness\": [\n",
    "            \"Unit Test 1 preparation (Sep 19)\",\n",
    "            \"Tutorial T6 implementation skills\",\n",
    "            \"CO-1 & CO-2 learning outcomes\",\n",
    "            \"Real-world problem solving\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    for category, items in summary.items():\n",
    "        print(f\"\\n{category}:\")\n",
    "        for item in items:\n",
    "            print(f\"  ‚úÖ {item}\")\n",
    "    \n",
    "    print(\"\\nüè† HOMEWORK FOR TONIGHT:\")\n",
    "    homework = [\n",
    "        \"Complete Tutorial T6 - L1/L2 implementation\",\n",
    "        \"Practice identifying overfitting from learning curves\", \n",
    "        \"Review mathematical derivations for penalties\",\n",
    "        \"Explain concepts to a friend using our analogies\",\n",
    "        \"Prepare for advanced regularization (Day 4)\"\n",
    "    ]\n",
    "    \n",
    "    for i, task in enumerate(homework, 1):\n",
    "        print(f\"  {i}. {task}\")\n",
    "    \n",
    "    print(\"\\nüîÆ PREVIEW: Day 4 - Advanced Regularization\")\n",
    "    preview = [\n",
    "        \"Dropout: The Random Absence Policy\",\n",
    "        \"Batch Normalization: Team Coordination\",\n",
    "        \"Data Augmentation: Experience Multiplier\",\n",
    "        \"Early Stopping: Perfect Timing Strategy\"\n",
    "    ]\n",
    "    \n",
    "    for technique in preview:\n",
    "        print(f\"  üéØ {technique}\")\n",
    "    \n",
    "    print(\"\\nüí¨ FINAL WISDOM:\")\n",
    "    print('\"The art of machine learning is not in building perfect models,\"')\n",
    "    print('\"but in building models that fail gracefully and generalize beautifully.\"')\n",
    "    print(\"\\nüë®‚Äçüè´ Remember: Every expert was once a beginner!\")\n",
    "\n",
    "# Generate summary\n",
    "generate_session_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìö Resources for Continued Learning\n",
    "\n",
    "### üìñ Required Reading\n",
    "- **Goodfellow et al., \"Deep Learning\" (2016)**: Chapter 7 - Regularization for Deep Learning\n",
    "- **Chollet, \"Deep Learning with Python\" (2018)**: Chapter 4.4 - Overfitting and Underfitting\n",
    "\n",
    "### üîó Online Resources\n",
    "- [TensorFlow Regularizers Documentation](https://www.tensorflow.org/api_docs/python/tf/keras/regularizers)\n",
    "- [Interactive Bias-Variance Decomposition](http://www.r2d3.us/visual-intro-to-machine-learning-part-2/)\n",
    "- [Regularization in Neural Networks (Visual Guide)](https://towardsdatascience.com/regularization-in-neural-networks-explained-with-code-8f11f7a4a0d3)\n",
    "\n",
    "### üí° Additional Practice\n",
    "- Experiment with different Œª values on your own datasets\n",
    "- Try combining L1 and L2 (Elastic Net regularization)\n",
    "- Implement regularization from scratch using NumPy\n",
    "\n",
    "---\n",
    "\n",
    "*¬© 2025 Prof. Ramesh Babu | SRM University | Deep Neural Network Architectures*  \n",
    "*\"Making AI Accessible Through Analogies\"*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
