# Week 6 Day 3: Visual Presentation Slides
## Overfitting, Underfitting & Classical Regularization

**Course:** 21CSE558T - Deep Neural Network Architectures
**Duration:** 2 Hours
**Date:** September 15, 2025
**Instructor:** Prof. Ramesh Babu

---

# Slide 1: Welcome & Session Overview

## ğŸ¯ Today's Learning Journey

**What We'll Master:**
- **WHY** models fail in the real world
- **WHAT** bias-variance tradeoff really means
- **HOW** to use L1 & L2 regularization effectively

**Our Learning Philosophy:**
> *"Learn through stories, understand through analogies, master through practice"*

**Session Structure:**
- **Hour 1:** The Problem (Overfitting & Underfitting)
- **Hour 2:** The Solution (Classical Regularization)

---

# Slide 2: The Restaurant Chef Dilemma

## ğŸ½ï¸ Why Do Smart Models Fail?

### Three Chefs, Three Approaches:

**Chef A - The Simplifier** ğŸ¥ª
- *"Salt + Pepper + Heat = Good Food"*
- Simple but limited (High Bias)

**Chef B - The Balanced** ğŸ‘¨â€ğŸ³
- Learns principles, adapts to situations
- The sweet spot (Good Fit)

**Chef C - The Perfectionist** ğŸ¤–
- Memorizes every detail perfectly
- Fails with new customers (High Variance)

### ğŸ’¡ The Big Question:
*Which chef would you hire for your restaurant?*

---

# Slide 3: The Archer's Target

## ğŸ¹ Understanding Bias & Variance

```
Total Error = BiasÂ² + Variance + Irreducible Error
```

### Visual Guide:

| Scenario | Bias | Variance | Result |
|----------|------|----------|---------|
| **Skilled Archer** âœ… | Low | Low | Clustered at bullseye |
| **Systematic Error** ğŸ“ | High | Low | Clustered away from target |
| **Inconsistent Archer** âš ï¸ | Low | High | Scattered around bullseye |
| **Worst Case** âŒ | High | High | Scattered away from target |

### ğŸ¯ Key Insight:
*Perfect practice doesn't mean perfect performance!*

---

# Slide 4: Mathematical Foundation

## ğŸ“Š The Bias-Variance Decomposition

### **Bias** - Systematic Error
```
Bias = E[fÌ‚(x)] - f(x)
```
- Model's tendency to consistently miss the true value
- Like always shooting left of the target

### **Variance** - Inconsistency
```
Variance = E[(fÌ‚(x) - E[fÌ‚(x)])Â²]
```
- Model's sensitivity to training data changes
- Like arrows scattered everywhere

### **The Fundamental Tradeoff:**
- â†‘ Model Complexity â†’ â†“ Bias, â†‘ Variance
- â†“ Model Complexity â†’ â†‘ Bias, â†“ Variance

---

# Slide 5: Student Learning Patterns

## ğŸ“š The Cramming vs Understanding Analogy

### **Student A - The Crammer** (Overfitting) âš ï¸
- Memorizes exact practice questions
- Perfect practice scores â†’ Fails real exam
- **ML Translation:** High training acc, low validation acc

### **Student B - The Understander** (Good Fit) âœ…
- Learns underlying principles
- Good practice scores â†’ Adapts to real exam
- **ML Translation:** Balanced train-validation performance

### **Student C - The Struggling** (Underfitting) ğŸ“ˆ
- Poor performance on both practice and exam
- **ML Translation:** Low training and validation accuracy

---

# Slide 6: Overfitting Detection

## ğŸš¨ Early Warning Signs

### **The Relationship Red Flags:**
1. **Too Perfect** â†’ 100% training accuracy
2. **No Flexibility** â†’ Poor on new data
3. **Memorization** â†’ Knows details, misses big picture
4. **Social Anxiety** â†’ Terrible with test data

### **Mathematical Symptoms:**
```
âš ï¸ Training Accuracy: 98%+
ğŸš¨ Validation Accuracy: <80%
ğŸ’¥ Gap: >15% difference
ğŸ“ˆğŸ“‰ Learning Curves: Diverging trends
```

### **Diagnostic Questions:**
- Is your model *too good* on training data?
- Are training and validation curves diverging?

---

# Slide 7: Hour 1 Summary & Transition

## âœ… What We've Learned So Far

### **Diagnosed the Problems:**
- **Bias-Variance Tradeoff** â†’ Restaurant chef learning
- **Overfitting** â†’ Student cramming behavior
- **Detection Methods** â†’ Early warning systems

### **Now We Need Treatment:**
> *"We've learned to diagnose the disease.
> Now let's learn the medicine!"*

---

# HOUR 2: THE MEDICINE

---

# Slide 8: Marie Kondo for Neural Networks

## âœ¨ L1 Regularization - The Decluttering Expert

### **The Philosophy:**
> *"Keep only features that spark joy (improve prediction)"*

### **Before L1** (Cluttered House) ğŸ 
```
Kitchen: [coffee_maker, toaster, 47_unused_gadgets, moldy_cheese]
Features: [useful_1, useful_2, 9,998_random_features]
```

### **After L1** (Minimalist Paradise) âœ¨
```
Kitchen: [coffee_maker, toaster]  # Only joy-sparking items
Features: [useful_1, useful_2]  # Only predictive features
```

### **The L1 Process:**
- Does this feature improve prediction? â†’ **Keep**
- Does this feature add noise? â†’ **Remove (weight = 0)**

---

# Slide 9: L1 Mathematical Foundation

## ğŸ’° The Budget Allocation Problem

### **Mathematical Form:**
```
Loss_total = Loss_original + Î»âˆ‘|wáµ¢|
```

### **The Company Budget Meeting:**
- **Normal Budget:** Marketing $400K, R&D $300K, Sales $250K, HR $50K
- **L1 Constraint:** Î£|department_budget| â‰¤ $1M
- **L1 Result:** HR gets $0 (eliminated!)

### **Geometric Interpretation:**
```
|wâ‚| + |wâ‚‚| â‰¤ Î»
```
- **Diamond-shaped constraint**
- **Sharp corners** â†’ Force weights to exactly zero
- **Automatic feature selection**

---

# Slide 10: Equal Opportunity Employer

## ğŸ¤ L2 Regularization - The Fair Workplace

### **The Philosophy:**
> *"No single feature should dominate the model"*

### **Company Culture A** (No Regularization)
- Star employee gets 90% of credit
- Others feel undervalued
- **ML:** One feature dominates

### **Company Culture B** (L2 Regularization)
- Equal opportunity for all employees
- Balanced workload distribution
- **ML:** All features contribute proportionally

### **Real-World Benefits:**
- **Stability** â†’ No single feature can break model
- **Fairness** â†’ All relevant features get a voice
- **Robustness** â†’ Works even if features missing

---

# Slide 11: L2 Mathematical Foundation

## ğŸ“ˆ Investment Portfolio Theory

### **Mathematical Form:**
```
Loss_total = Loss_original + Î»âˆ‘wáµ¢Â²
```

### **Portfolio Strategy:**
- **Risky Portfolio:** 80% Tesla, 20% others â†’ High risk
- **Diversified Portfolio:** Balanced across sectors â†’ Lower risk
- **L2 Translation:** Spread importance across features

### **Geometric Interpretation:**
```
wâ‚Â² + wâ‚‚Â² â‰¤ Î»
```
- **Circular constraint**
- **Smooth boundaries** â†’ Weights shrink gradually
- **No sparsity** â†’ All features remain active

---

# Slide 12: L1 vs L2 Comparison

## âš–ï¸ The Complete Comparison

| Aspect | L1 (LASSO) | L2 (Ridge) |
|--------|------------|------------|
| **Penalty** | Î£\|wáµ¢\| | Î£wáµ¢Â² |
| **Geometry** | ğŸ’ Diamond | â­• Circle |
| **Effect** | Feature Selection | Weight Smoothing |
| **Sparsity** | âœ… Yes (weights â†’ 0) | âŒ No (weights â†’ small) |
| **Use Case** | Remove irrelevant features | Handle correlated features |
| **Analogy** | ğŸ§¹ Marie Kondo | ğŸ¤ Equal Opportunity |
| **Interpretation** | Easy (fewer features) | Moderate (all features) |

### **When to Use Which?**
- **Many irrelevant features** â†’ L1
- **Correlated features** â†’ L2
- **Need interpretability** â†’ L1
- **Need stability** â†’ L2

---

# Slide 13: Hyperparameter Tuning

## ğŸ›ï¸ Finding the Sweet Spot

### **The Goldilocks Principle:**

| Î» Value | Effect | Result |
|---------|--------|---------|
| **Too Small** (< 0.001) | No effect | Still overfitting |
| **Just Right** (0.001-0.1) | Balanced | Good generalization |
| **Too Large** (> 1.0) | Over-regularization | Underfitting |

### **Tuning Strategy:**
1. **Start broad:** Try [0.001, 0.01, 0.1, 1.0]
2. **Cross-validate:** Use k-fold validation
3. **Monitor gap:** Watch train-validation difference
4. **Refine:** Narrow down around best performer

### **Visual Guide:**
```
Î» â†‘ â†’ Bias â†‘, Variance â†“
Î» â†“ â†’ Bias â†“, Variance â†‘
```

---

# Slide 14: The Model Doctor

## ğŸ¥ Complete Diagnostic System

### **Patient Examination:**
```python
def diagnose_model_health(train_acc, val_acc):
    gap = abs(train_acc - val_acc)

    if train_acc > 0.95 and val_acc < 0.8:
        return "Overfitting - Apply regularization"
    elif train_acc < 0.8 and val_acc < 0.8:
        return "Underfitting - Increase complexity"
    else:
        return "Healthy - Continue approach"
```

### **Treatment Prescriptions:**
- **Overfitting** â†’ L1/L2 regularization, early stopping
- **Underfitting** â†’ Reduce regularization, add complexity
- **Healthy** â†’ Monitor and deploy

---

# Slide 15: Real-World Case Studies

## ğŸŒ Where This Matters

### **Medical Diagnosis:**
- **Problem:** Model memorizes hospital A, fails at hospital B
- **Solution:** L2 regularization for robust feature weights

### **Finance:**
- **Problem:** Trading model perfect in backtest, loses money live
- **Solution:** L1 for feature selection, L2 for stability

### **Image Recognition:**
- **Problem:** 100% accuracy on training photos, poor on new images
- **Solution:** Combined L1/L2 + data augmentation

### **Text Classification:**
- **Problem:** Memorizes training documents, poor generalization
- **Solution:** L1 for keyword selection, L2 for balanced weights

---

# Slide 16: Implementation in TensorFlow

## ğŸ’» From Theory to Practice

### **L1 Implementation:**
```python
model = tf.keras.Sequential([
    tf.keras.layers.Dense(128, activation='relu',
        kernel_regularizer=tf.keras.regularizers.l1(0.01)),
    tf.keras.layers.Dense(10, activation='softmax')
])
```

### **L2 Implementation:**
```python
model = tf.keras.Sequential([
    tf.keras.layers.Dense(128, activation='relu',
        kernel_regularizer=tf.keras.regularizers.l2(0.01)),
    tf.keras.layers.Dense(10, activation='softmax')
])
```

### **Combined L1 + L2:**
```python
tf.keras.regularizers.l1_l2(l1=0.01, l2=0.01)
```

---

# Slide 17: Assessment Preparation

## ğŸ“ Unit Test 1 Ready Checklist (Sep 19)

### **Mathematical Mastery:**
- [ ] Can derive bias-variance decomposition
- [ ] Understands L1 vs L2 penalty differences
- [ ] Can calculate regularization penalties manually
- [ ] Knows geometric interpretations

### **Conceptual Understanding:**
- [ ] Can explain overfitting using analogies
- [ ] Identifies problems from learning curves
- [ ] Chooses appropriate regularization technique
- [ ] Understands hyperparameter tuning

### **Practical Skills:**
- [ ] Implements L1/L2 in TensorFlow
- [ ] Sets up overfitting detection
- [ ] Tunes Î» parameters effectively

---

# Slide 18: Tonight's Homework

## ğŸ  Your Mission Until Day 4

### **Must Complete:**
1. **Tutorial T6** â†’ Implement both L1 and L2 regularization
2. **Practice Session** â†’ Identify overfitting from learning curves
3. **Math Review** â†’ Work through penalty calculations
4. **Teach Back** â†’ Explain concepts using our analogies
5. **Read Ahead** â†’ Advanced regularization preview

### **Success Criteria:**
- Can implement regularization without looking at notes
- Can diagnose model problems from visual patterns
- Can explain trade-offs to a non-technical friend

---

# Slide 19: Day 4 Preview

## ğŸ”® Advanced Regularization Techniques

### **Coming Up:**
- **Dropout** â†’ "The Random Absence Policy"
- **Batch Normalization** â†’ "The Team Coordination System"
- **Data Augmentation** â†’ "The Experience Multiplier"
- **Early Stopping** â†’ "The Perfect Timing Strategy"

### **The Journey Continues:**
> *"We've learned the classical medicine.
> Next, we'll explore the modern treatments!"*

---

# Slide 20: Key Takeaways

## ğŸ¯ Remember These Core Concepts

### **The Analogies:**
1. **Bias-Variance** = Restaurant chef learning strategies
2. **Overfitting** = Student cramming + relationship red flags
3. **L1** = Marie Kondo decluttering
4. **L2** = Equal opportunity + portfolio diversification

### **The Mathematics:**
- **Total Error** = BiasÂ² + Variance + Irreducible Error
- **L1 Penalty** = Î»âˆ‘|wáµ¢| (Diamond constraint)
- **L2 Penalty** = Î»âˆ‘wáµ¢Â² (Circle constraint)

### **The Wisdom:**
> *"The art of machine learning is not in building perfect models, but in building models that fail gracefully and generalize beautifully."*

---

# Slide 21: Interactive Q&A

## ğŸ’¬ Let's Test Your Understanding

### **Quick Fire Round:**
1. When would you choose L1 over L2?
2. What's the danger of Î» = 0?
3. How do you know if Î» is too large?
4. Which regularization creates sparse models?
5. What's the geometric shape of L2 constraint?

### **Think-Pair-Share:**
*"Explain overfitting to someone who's never heard of machine learning using one of today's analogies"*

### **Practical Challenge:**
*"Given a model with 95% training accuracy and 70% validation accuracy, what's your diagnosis and treatment plan?"*

---

# Slide 22: Course Context

## ğŸ“ Where We Are in the Journey

### **Module 2 Progress:**
- âœ… **Week 4** â†’ Gradient Descent Fundamentals
- âœ… **Week 5** â†’ Gradient Problems (Vanishing/Exploding)
- âœ… **Week 6 Day 3** â†’ Classical Regularization â† **WE ARE HERE**
- ğŸ”œ **Week 6 Day 4** â†’ Advanced Regularization

### **Building Towards:**
- **Week 7-9** â†’ Image Processing & CNNs
- **Week 10-12** â†’ Transfer Learning
- **Week 13-15** â†’ Object Detection

### **Learning Outcomes Progress:**
- **CO-1** âœ… â†’ Create simple neural networks
- **CO-2** ğŸš§ â†’ Build multi-layer networks (in progress)

---

# Slide 23: Thank You

## ğŸ™ Session Complete!

### **What We Accomplished Today:**
- âœ… Mastered bias-variance tradeoff through analogies
- âœ… Learned to detect overfitting patterns
- âœ… Implemented L1 and L2 regularization
- âœ… Built practical diagnostic tools
- âœ… Prepared for Unit Test 1

### **Your Next Steps:**
1. Complete tonight's homework
2. Practice with Tutorial T6
3. Prepare questions for Day 4
4. Review for Unit Test 1

### **Remember:**
> *"Every expert was once a beginner.
> Every pro was once an amateur.
> Every icon was once an unknown."*

**See you for Advanced Regularization! ğŸš€**

---

## ğŸ“š Slide References

### **Required Reading:**
- Goodfellow et al., "Deep Learning" Chapter 7
- Chollet, "Deep Learning with Python" Chapter 4.4

### **Online Resources:**
- TensorFlow Regularizers Documentation
- Interactive Bias-Variance Tools
- Course Materials Repository

---

*Â© 2025 Prof. Ramesh Babu | SRM University | Deep Neural Network Architectures*
*"Making AI Accessible Through Analogies"*