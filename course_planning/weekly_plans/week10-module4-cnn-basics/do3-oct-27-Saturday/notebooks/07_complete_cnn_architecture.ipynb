{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 7: Complete CNN Architecture\n\n**Week 10 - Module 4: CNN Basics**\n**DO3 (October 27, 2025) - Saturday**\n**Duration:** 25-30 minutes\n\n## Learning Objectives\n\n1. \u2705 **Design** complete CNN architectures\n2. \u2705 **Trace** feature map dimensions through network\n3. \u2705 **Calculate** parameter counts\n4. \u2705 **Build** simple CNN in Keras (preview for Tutorial T10)\n\n---"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\n\nprint(f\"TensorFlow version: {tf.__version__}\")\nprint(\"\u2705 Setup complete!\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. CNN Architecture Components\n\nA complete CNN has:\n\n1. **Input Layer**: Image (H \u00d7 W \u00d7 C)\n2. **Convolutional Blocks**: [Conv \u2192 ReLU \u2192 Pool] \u00d7 N\n3. **Flattening**: Convert to 1D vector\n4. **Fully Connected Layers**: Classification\n5. **Output Layer**: Softmax for probabilities\n\n### Standard Pattern:\n\n```\nInput (28\u00d728\u00d71)\n    \u2193\n[Conv 3\u00d73, 32 filters] \u2192 ReLU \u2192 MaxPool 2\u00d72\n    \u2193 (14\u00d714\u00d732)\n[Conv 3\u00d73, 64 filters] \u2192 ReLU \u2192 MaxPool 2\u00d72\n    \u2193 (7\u00d77\u00d764)\nFlatten \u2192 3136\n    \u2193\nDense(128) \u2192 ReLU\n    \u2193\nDense(10) \u2192 Softmax\n    \u2193\nOutput (10 classes)\n```\n\n---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dimension Tracing Example\n\nLet's trace dimensions through a simple CNN for MNIST (28\u00d728 grayscale images).\n\n**Architecture:**\n\n| Layer | Operation | Output Shape | Parameters |\n|-------|-----------|--------------|------------|\n| Input | - | (28, 28, 1) | 0 |\n| Conv1 | 3\u00d73, 32 filters, stride=1, padding=same | (28, 28, 32) | 320 |\n| MaxPool1 | 2\u00d72, stride=2 | (14, 14, 32) | 0 |\n| Conv2 | 3\u00d73, 64 filters, stride=1, padding=same | (14, 14, 64) | 18,496 |\n| MaxPool2 | 2\u00d72, stride=2 | (7, 7, 64) | 0 |\n| Flatten | - | (3136,) | 0 |\n| Dense1 | 128 units | (128,) | 401,536 |\n| Dense2 | 10 units | (10,) | 1,290 |\n\n**Total Parameters:** 421,642\n\n---"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Build the exact architecture above\nmodel = keras.Sequential([\n    # Conv Block 1\n    keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(28, 28, 1)),\n    keras.layers.MaxPooling2D((2, 2)),\n\n    # Conv Block 2\n    keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n    keras.layers.MaxPooling2D((2, 2)),\n\n    # Fully Connected Layers\n    keras.layers.Flatten(),\n    keras.layers.Dense(128, activation='relu'),\n    keras.layers.Dense(10, activation='softmax')\n])\n\nmodel.summary()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Parameter Calculation\n\n### Conv Layer Parameters:\n\n$$\n\\text{Params} = (F_h \\times F_w \\times C_{in} + 1) \\times C_{out}\n$$\n\nWhere:\n- $F_h, F_w$ = filter height, width\n- $C_{in}$ = input channels\n- $C_{out}$ = output channels (number of filters)\n- $+1$ = bias term\n\n**Example (Conv1):**\n- Filter: 3\u00d73\n- Input channels: 1\n- Output channels: 32\n- Params = $(3 \\times 3 \\times 1 + 1) \\times 32 = 320$\n\n### Dense Layer Parameters:\n\n$$\n\\text{Params} = (\\text{input\\_units} + 1) \\times \\text{output\\_units}\n$$\n\n**Example (Dense1):**\n- Input: 3136 (7\u00d77\u00d764 flattened)\n- Output: 128\n- Params = $(3136 + 1) \\times 128 = 401,536$\n\n---"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def count_conv_params(filter_size, in_channels, out_channels):\n    \"\"\"Calculate conv layer parameters.\"\"\"\n    return (filter_size * filter_size * in_channels + 1) * out_channels\n\ndef count_dense_params(in_units, out_units):\n    \"\"\"Calculate dense layer parameters.\"\"\"\n    return (in_units + 1) * out_units\n\n# Verify our calculations\nconv1_params = count_conv_params(3, 1, 32)\nconv2_params = count_conv_params(3, 32, 64)\ndense1_params = count_dense_params(7*7*64, 128)\ndense2_params = count_dense_params(128, 10)\n\nprint(\"Parameter Verification:\")\nprint(f\"Conv1: {conv1_params:,}\")\nprint(f\"Conv2: {conv2_params:,}\")\nprint(f\"Dense1: {dense1_params:,}\")\nprint(f\"Dense2: {dense2_params:,}\")\nprint(f\"Total: {conv1_params + conv2_params + dense1_params + dense2_params:,}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Famous CNN Architecture: LeNet-5\n\n**LeNet-5** (Yann LeCun, 1998) - First successful CNN\n\n```\nInput (32\u00d732\u00d71)\n    \u2193\nConv 5\u00d75, 6 filters \u2192 Tanh \u2192 AvgPool 2\u00d72\n    \u2193\nConv 5\u00d75, 16 filters \u2192 Tanh \u2192 AvgPool 2\u00d72\n    \u2193\nFlatten\n    \u2193\nDense(120) \u2192 Tanh\n    \u2193\nDense(84) \u2192 Tanh\n    \u2193\nDense(10) \u2192 Softmax\n```\n\n**Key Innovations:**\n- Hierarchical feature learning\n- Weight sharing (convolution)\n- Pooling for translation invariance\n\n---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. CNN vs MLP: Parameter Comparison\n\n**Same task:** Classify 28\u00d728 MNIST images (10 classes)\n\n### MLP Approach:\n- Flatten input: 784 units\n- Hidden layer: 128 units\n- Output: 10 units\n- **Total params:** $(784 + 1) \\times 128 + (128 + 1) \\times 10 = 101,770$\n\n### CNN Approach (our model):\n- Convolutional layers + Dense layers\n- **Total params:** 421,642\n\n**Wait, CNN has MORE parameters?**\n\nYes, but:\n1. CNN learns **hierarchical features**\n2. CNN achieves **better accuracy** (98%+ vs 95%)\n3. CNN is **translation invariant**\n4. With **regularization**, CNN generalizes better\n\n**Modern CNNs** (MobileNet, EfficientNet) have FEWER params than MLPs!\n\n---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n\n### \ud83c\udfaf Key Architecture Principles\n\n1. **Pattern**: [Conv \u2192 Activation \u2192 Pool] \u00d7 N \u2192 Flatten \u2192 Dense\n2. **Channels increase**: 1 \u2192 32 \u2192 64 \u2192 128 (doubling)\n3. **Spatial size decreases**: 28 \u2192 14 \u2192 7 (pooling)\n4. **Parameters**: Mostly in Dense layers (for small CNNs)\n\n### \ud83d\udd2e Next\n\n**Notebook 8:** 3D Convolution (video, medical imaging)\n\n---\n\n*Week 10 - Deep Neural Network Architectures (21CSE558T)*\n*SRM University - M.Tech Program*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}