{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 1: Convolution Concept & Intuition\n",
    "\n",
    "**Week 10 - Module 4: CNN Basics**  \n",
    "**DO3 (October 27, 2025) - Saturday**  \n",
    "**Duration:** 15-20 minutes\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. ‚úÖ Explain the **intuitive concept** of convolution as a \"sliding window operation\"\n",
    "2. ‚úÖ Understand **why** convolution is useful for pattern detection\n",
    "3. ‚úÖ Connect convolution to **Week 9 manual feature extraction** (LBP, GLCM)\n",
    "4. ‚úÖ Visualize the **relationship** between input, filter, and output\n",
    "5. ‚úÖ Recognize convolution in **real-world applications**\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- ‚úÖ Completed Notebook 0 (Setup & Prerequisites)\n",
    "- ‚úÖ Understanding of Week 9 manual features (LBP, GLCM)\n",
    "- ‚úÖ Basic array operations in NumPy\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "Let's start by importing the necessary libraries and helper functions from Notebook 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure matplotlib for better-looking plots\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "print(\"‚úÖ All imports successful!\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. The Story Begins: Detective Kavya's Pattern Recognition\n",
    "\n",
    "### üìñ Narrative Introduction\n",
    "\n",
    "**Character: Detective Kavya** (carrying forward from Week 9) has been analyzing security camera footage to identify suspicious patterns. Last week, she manually extracted features using LBP and GLCM to detect textures and patterns in images.\n",
    "\n",
    "**The Challenge:**\n",
    "\n",
    "> \"I need to detect edges in these images to identify objects,\" Detective Kavya explains to her colleague **Character: Arjun**. \"Last week, I manually created feature extractors that looked at pixel neighborhoods. But what if I could **learn** the best pattern detector automatically?\"\n",
    "\n",
    "**Character: Arjun** responds: \"That's exactly what convolution does! It's like having a **magnifying glass** that you slide across the image, looking for specific patterns.\"\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Key Insight\n",
    "\n",
    "**Convolution is:**\n",
    "- A **sliding window operation** that applies a pattern detector (filter/kernel) across data\n",
    "- A way to **automatically learn** what patterns matter (when used in CNNs)\n",
    "- The **bridge** between manual feature extraction (Week 9) and learned features (Week 10)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Intuitive Understanding: The Magnifying Glass Analogy\n",
    "\n",
    "### üîç Visual Metaphor\n",
    "\n",
    "Think of convolution as using a **magnifying glass** to examine a document:\n",
    "\n",
    "1. **Input Data** = The document (image, signal, text)\n",
    "2. **Filter/Kernel** = The magnifying glass (pattern detector)\n",
    "3. **Sliding** = Moving the magnifying glass across the document\n",
    "4. **Output** = What you observe at each position\n",
    "\n",
    "Let's visualize this with a simple example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple visual representation\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Input \"Document\"\n",
    "input_data = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "axes[0].bar(range(len(input_data)), input_data, color='skyblue', alpha=0.7)\n",
    "axes[0].set_title('üìÑ Input Data (The Document)', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Position')\n",
    "axes[0].set_ylabel('Value')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Filter \"Magnifying Glass\"\n",
    "filter_kernel = np.array([1, 0, -1])  # Edge detector\n",
    "axes[1].bar(range(len(filter_kernel)), filter_kernel, color='orange', alpha=0.7)\n",
    "axes[1].set_title('üîç Filter/Kernel (Magnifying Glass)', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Position')\n",
    "axes[1].set_ylabel('Weight')\n",
    "axes[1].axhline(y=0, color='black', linestyle='--', linewidth=0.8)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Output \"Observations\"\n",
    "output = np.convolve(input_data, filter_kernel, mode='valid')\n",
    "axes[2].bar(range(len(output)), output, color='green', alpha=0.7)\n",
    "axes[2].set_title('üìä Output (What We Observe)', fontsize=14, fontweight='bold')\n",
    "axes[2].set_xlabel('Position')\n",
    "axes[2].set_ylabel('Response')\n",
    "axes[2].axhline(y=0, color='black', linestyle='--', linewidth=0.8)\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Input shape: {input_data.shape}\")\n",
    "print(f\"Filter shape: {filter_kernel.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"\\nOutput values: {output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üí° Observation\n",
    "\n",
    "Notice how:\n",
    "- The **output is shorter** than the input (we'll learn about padding to fix this)\n",
    "- The output shows **where changes happen** (edge detection!)\n",
    "- The filter `[1, 0, -1]` detects **differences** between neighboring values\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Connection to Week 9: From Manual to Learned Features\n",
    "\n",
    "### üîó Bridging Concepts\n",
    "\n",
    "Last week (Week 9), we manually designed features:\n",
    "- **LBP (Local Binary Patterns)**: Compared center pixel with neighbors\n",
    "- **GLCM (Gray-Level Co-occurrence Matrix)**: Analyzed texture relationships\n",
    "- **Shape Features**: Calculated geometric properties\n",
    "\n",
    "**The Problem:** We had to **manually design** these feature extractors.\n",
    "\n",
    "**The Solution:** Convolution + CNNs **learn** the best features automatically!\n",
    "\n",
    "Let's visualize this connection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate a Week 9 vs Week 10 comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Week 9: Manual Feature Design\n",
    "manual_steps = ['Input\\nImage', 'LBP\\nGLCM\\nShape', 'Manual\\nFeatures', 'Classifier']\n",
    "axes[0].plot([0, 1, 2, 3], [0, 0, 0, 0], 'o-', markersize=20, linewidth=3, color='red')\n",
    "for i, step in enumerate(manual_steps):\n",
    "    axes[0].text(i, -0.1, step, ha='center', va='top', fontsize=11, fontweight='bold')\n",
    "axes[0].set_title('Week 9: Manual Feature Extraction', fontsize=14, fontweight='bold', color='red')\n",
    "axes[0].set_xlim(-0.5, 3.5)\n",
    "axes[0].set_ylim(-0.5, 0.5)\n",
    "axes[0].axis('off')\n",
    "axes[0].text(1.5, 0.3, '‚ùå Manual Design', ha='center', fontsize=12, color='red')\n",
    "\n",
    "# Week 10: Learned Features (CNN)\n",
    "learned_steps = ['Input\\nImage', 'Conv\\nLayers', 'Learned\\nFeatures', 'Classifier']\n",
    "axes[1].plot([0, 1, 2, 3], [0, 0, 0, 0], 'o-', markersize=20, linewidth=3, color='green')\n",
    "for i, step in enumerate(learned_steps):\n",
    "    axes[1].text(i, -0.1, step, ha='center', va='top', fontsize=11, fontweight='bold')\n",
    "axes[1].set_title('Week 10: Learned Features (CNN)', fontsize=14, fontweight='bold', color='green')\n",
    "axes[1].set_xlim(-0.5, 3.5)\n",
    "axes[1].set_ylim(-0.5, 0.5)\n",
    "axes[1].axis('off')\n",
    "axes[1].text(1.5, 0.3, '‚úÖ Automatic Learning', ha='center', fontsize=12, color='green')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key Difference:\")\n",
    "print(\"  Week 9: YOU design the feature extractor (LBP, GLCM)\")\n",
    "print(\"  Week 10: CNN LEARNS the best feature extractor (convolution filters)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. The Three Components of Convolution\n",
    "\n",
    "Every convolution operation has three key components:\n",
    "\n",
    "### 1Ô∏è‚É£ Input Data\n",
    "- The signal, image, or data you want to analyze\n",
    "- Example: Camera image, audio signal, text sequence\n",
    "\n",
    "### 2Ô∏è‚É£ Filter/Kernel (Pattern Detector)\n",
    "- A small matrix that defines what pattern to look for\n",
    "- Example: Edge detector, blur filter, sharpening filter\n",
    "- **In CNNs**: These are **learned automatically** during training!\n",
    "\n",
    "### 3Ô∏è‚É£ Output (Feature Map)\n",
    "- The result of applying the filter across the input\n",
    "- Shows **where** and **how strongly** the pattern appears\n",
    "\n",
    "Let's visualize this relationship:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual representation of the three components\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Component 1: Input\n",
    "input_example = np.array([\n",
    "    [1, 2, 3, 4],\n",
    "    [5, 6, 7, 8],\n",
    "    [9, 10, 11, 12],\n",
    "    [13, 14, 15, 16]\n",
    "])\n",
    "im1 = axes[0].imshow(input_example, cmap='Blues', interpolation='nearest')\n",
    "axes[0].set_title('1Ô∏è‚É£ Input Data\\n(4√ó4 Image)', fontsize=13, fontweight='bold')\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        axes[0].text(j, i, f'{input_example[i, j]}', ha='center', va='center', fontsize=10)\n",
    "axes[0].set_xticks([])\n",
    "axes[0].set_yticks([])\n",
    "\n",
    "# Component 2: Filter/Kernel\n",
    "kernel_example = np.array([\n",
    "    [1, 0, -1],\n",
    "    [2, 0, -2],\n",
    "    [1, 0, -1]\n",
    "])\n",
    "im2 = axes[1].imshow(kernel_example, cmap='RdBu', interpolation='nearest', vmin=-2, vmax=2)\n",
    "axes[1].set_title('2Ô∏è‚É£ Filter/Kernel\\n(3√ó3 Edge Detector)', fontsize=13, fontweight='bold')\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        color = 'white' if kernel_example[i, j] != 0 else 'black'\n",
    "        axes[1].text(j, i, f'{kernel_example[i, j]}', ha='center', va='center', \n",
    "                    fontsize=10, color=color, fontweight='bold')\n",
    "axes[1].set_xticks([])\n",
    "axes[1].set_yticks([])\n",
    "\n",
    "# Component 3: Output (simplified for visualization)\n",
    "output_example = np.array([\n",
    "    [-20, -20],\n",
    "    [-20, -20]\n",
    "])\n",
    "im3 = axes[2].imshow(output_example, cmap='Greens', interpolation='nearest')\n",
    "axes[2].set_title('3Ô∏è‚É£ Output (Feature Map)\\n(2√ó2 Result)', fontsize=13, fontweight='bold')\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        axes[2].text(j, i, f'{output_example[i, j]}', ha='center', va='center', fontsize=10)\n",
    "axes[2].set_xticks([])\n",
    "axes[2].set_yticks([])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Three Components Summary:\")\n",
    "print(f\"  1. Input:  {input_example.shape} ‚Üí The data we analyze\")\n",
    "print(f\"  2. Filter: {kernel_example.shape} ‚Üí The pattern detector\")\n",
    "print(f\"  3. Output: {output_example.shape} ‚Üí Where patterns are found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Real-World Applications: Where is Convolution Used?\n",
    "\n",
    "### üåç Applications Across Domains\n",
    "\n",
    "Convolution is not just for images! Let's see where it's used:\n",
    "\n",
    "| Domain | Application | Filter Purpose |\n",
    "|--------|-------------|----------------|\n",
    "| **Computer Vision** | Face detection, object recognition | Detect edges, shapes, textures |\n",
    "| **Medical Imaging** | X-ray analysis, MRI processing | Enhance features, detect anomalies |\n",
    "| **Audio Processing** | Speech recognition, music analysis | Extract frequency patterns |\n",
    "| **Natural Language** | Text classification, sentiment analysis | Find word patterns |\n",
    "| **Video Analysis** | Action recognition, surveillance | Track motion, detect events |\n",
    "| **Signal Processing** | ECG analysis, sensor data | Filter noise, detect patterns |\n",
    "\n",
    "### üéØ Detective Kavya's Use Cases\n",
    "\n",
    "**Character: Detective Kavya** uses convolution for:\n",
    "1. **Edge Detection** in surveillance footage (find object boundaries)\n",
    "2. **Pattern Recognition** in financial fraud detection (unusual transaction patterns)\n",
    "3. **Motion Detection** in security cameras (identify suspicious movement)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Interactive Exploration: Sliding Window Visualization\n",
    "\n",
    "Let's create an interactive visualization showing how the filter \"slides\" across the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_sliding_window_step(input_data, kernel, position):\n",
    "    \"\"\"\n",
    "    Visualize one step of the sliding window operation.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    input_data : array\n",
    "        Input signal/data\n",
    "    kernel : array\n",
    "        Convolution kernel\n",
    "    position : int\n",
    "        Current position of the window\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    # Input with highlighted window\n",
    "    axes[0].bar(range(len(input_data)), input_data, color='lightgray', alpha=0.5)\n",
    "    window_indices = range(position, position + len(kernel))\n",
    "    axes[0].bar(window_indices, input_data[position:position+len(kernel)], \n",
    "                color='skyblue', alpha=0.8, label='Current Window')\n",
    "    axes[0].set_title(f'Input (Window at position {position})', fontsize=13, fontweight='bold')\n",
    "    axes[0].set_xlabel('Position')\n",
    "    axes[0].set_ylabel('Value')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Kernel\n",
    "    axes[1].bar(range(len(kernel)), kernel, color='orange', alpha=0.7)\n",
    "    axes[1].set_title('Filter/Kernel', fontsize=13, fontweight='bold')\n",
    "    axes[1].set_xlabel('Position')\n",
    "    axes[1].set_ylabel('Weight')\n",
    "    axes[1].axhline(y=0, color='black', linestyle='--', linewidth=0.8)\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Calculation\n",
    "    window = input_data[position:position+len(kernel)]\n",
    "    result = np.sum(window * kernel)\n",
    "    calculation_text = \" + \".join([f\"{w}√ó{k}\" for w, k in zip(window, kernel)])\n",
    "    axes[2].text(0.5, 0.6, f\"Calculation:\", ha='center', fontsize=12, fontweight='bold')\n",
    "    axes[2].text(0.5, 0.5, calculation_text, ha='center', fontsize=10)\n",
    "    axes[2].text(0.5, 0.4, f\"= {result}\", ha='center', fontsize=14, fontweight='bold', color='green')\n",
    "    axes[2].set_xlim(0, 1)\n",
    "    axes[2].set_ylim(0, 1)\n",
    "    axes[2].axis('off')\n",
    "    axes[2].set_title('Output Calculation', fontsize=13, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Example: Show sliding window at different positions\n",
    "input_signal = np.array([1, 2, 3, 4, 5, 6, 7])\n",
    "edge_kernel = np.array([1, 0, -1])\n",
    "\n",
    "print(\"üé¨ Sliding Window Animation (Step-by-Step)\\n\")\n",
    "print(\"Watch how the filter slides across the input:\\n\")\n",
    "\n",
    "# Show 3 positions\n",
    "for pos in [0, 2, 4]:\n",
    "    print(f\"\\n--- Position {pos} ---\")\n",
    "    result = visualize_sliding_window_step(input_signal, edge_kernel, pos)\n",
    "    print(f\"Output value at position {pos}: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üí° Key Observation\n",
    "\n",
    "Notice how:\n",
    "1. The **filter slides** one position at a time (this is called **stride = 1**)\n",
    "2. At each position, we **multiply and sum** (element-wise multiplication, then sum)\n",
    "3. The output shows **where changes occur** (edge detection!)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Why Convolution is Powerful\n",
    "\n",
    "### ‚ú® Key Properties of Convolution\n",
    "\n",
    "1. **Local Connectivity**: Filter looks at small neighborhoods (like LBP from Week 9!)\n",
    "2. **Weight Sharing**: Same filter applied everywhere (efficient!)\n",
    "3. **Translation Equivariance**: If input shifts, output shifts too\n",
    "4. **Hierarchical Learning**: Stack multiple convolutions to learn complex patterns\n",
    "\n",
    "Let's visualize these properties:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate translation equivariance\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "# Original signal\n",
    "signal1 = np.array([0, 0, 1, 2, 3, 2, 1, 0, 0])\n",
    "kernel = np.array([1, 0, -1])\n",
    "output1 = np.convolve(signal1, kernel, mode='valid')\n",
    "\n",
    "# Shifted signal\n",
    "signal2 = np.array([0, 0, 0, 0, 1, 2, 3, 2, 1])\n",
    "output2 = np.convolve(signal2, kernel, mode='valid')\n",
    "\n",
    "# Plot original\n",
    "axes[0, 0].stem(signal1, basefmt=' ')\n",
    "axes[0, 0].set_title('Original Signal', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Value')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 0].stem(output1, basefmt=' ', linefmt='r-', markerfmt='ro')\n",
    "axes[1, 0].set_title('Original Output', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Position')\n",
    "axes[1, 0].set_ylabel('Value')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot shifted\n",
    "axes[0, 1].stem(signal2, basefmt=' ')\n",
    "axes[0, 1].set_title('Shifted Signal (‚Üí3 positions)', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 1].stem(output2, basefmt=' ', linefmt='r-', markerfmt='ro')\n",
    "axes[1, 1].set_title('Shifted Output (‚Üí3 positions)', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Position')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üéØ Translation Equivariance:\")\n",
    "print(\"  When input shifts ‚Üí Output shifts by same amount\")\n",
    "print(\"  This makes CNNs good at detecting patterns ANYWHERE in the image!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Summary and Key Takeaways\n",
    "\n",
    "### üéØ What We Learned\n",
    "\n",
    "1. **Convolution = Sliding Window Operation**\n",
    "   - Filter slides across input\n",
    "   - Multiply and sum at each position\n",
    "   - Output shows where patterns appear\n",
    "\n",
    "2. **Three Components**\n",
    "   - Input data (image, signal, etc.)\n",
    "   - Filter/Kernel (pattern detector)\n",
    "   - Output/Feature Map (detection results)\n",
    "\n",
    "3. **Connection to Week 9**\n",
    "   - Week 9: Manual features (LBP, GLCM) - we design\n",
    "   - Week 10: Learned features (CNN) - network learns\n",
    "\n",
    "4. **Why Convolution is Powerful**\n",
    "   - Local connectivity\n",
    "   - Weight sharing\n",
    "   - Translation equivariance\n",
    "   - Hierarchical learning\n",
    "\n",
    "### üîÆ What's Next?\n",
    "\n",
    "In **Notebook 2**, we'll dive into **1D Convolution Math & Code**:\n",
    "- Step-by-step calculation by hand\n",
    "- NumPy implementation\n",
    "- Understanding the mathematics\n",
    "- Real-world signal processing examples\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Practice Exercises\n",
    "\n",
    "### Exercise 1: Conceptual Understanding\n",
    "**Question:** Explain convolution to a friend who has never heard of it. Use the magnifying glass analogy.\n",
    "\n",
    "### Exercise 2: Pattern Recognition\n",
    "**Question:** What filter would you use to detect:\n",
    "- Vertical edges?\n",
    "- Horizontal edges?\n",
    "- Blur (smoothing)?\n",
    "\n",
    "### Exercise 3: Connection to Week 9\n",
    "**Question:** How is convolution similar to and different from LBP (Local Binary Patterns) from Week 9?\n",
    "\n",
    "---\n",
    "\n",
    "## References and Further Reading\n",
    "\n",
    "1. **Goodfellow et al. (2016)**: Deep Learning, Chapter 9 (Convolutional Networks)\n",
    "2. **Chollet (2021)**: Deep Learning with Python, Chapter 5\n",
    "3. **3Blue1Brown**: \"But what is a convolution?\" (YouTube)\n",
    "4. **Week 9 Lecture Notes**: Manual feature extraction (LBP, GLCM)\n",
    "\n",
    "---\n",
    "\n",
    "**Next Notebook:** [Notebook 2: 1D Convolution Math & Code](02_1d_convolution_math_code.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "*Week 10 - Deep Neural Network Architectures (21CSE558T)*  \n",
    "*SRM University - M.Tech Program*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
