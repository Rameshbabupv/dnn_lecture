{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial T10: Building CNN for Fashion-MNIST Classification\n",
    "\n",
    "**Week 10, Day 4 - October 29, 2025**  \n",
    "**Deep Neural Network Architectures (21CSE558T)**\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this tutorial, you will be able to:\n",
    "\n",
    "1. üì• Load and preprocess the Fashion-MNIST dataset\n",
    "2. üèóÔ∏è Build a Convolutional Neural Network (CNN) using Keras Sequential API\n",
    "3. üéØ Train the CNN and visualize training history\n",
    "4. üìä Evaluate model performance on test data\n",
    "5. üîç Visualize learned filters and feature maps\n",
    "6. ‚öñÔ∏è Compare CNN performance with traditional MLP\n",
    "\n",
    "---\n",
    "\n",
    "## Fashion-MNIST Dataset\n",
    "\n",
    "- **60,000** training images + **10,000** test images\n",
    "- **28√ó28** grayscale images\n",
    "- **10 classes**: T-shirt/top, Trouser, Pullover, Dress, Coat, Sandal, Shirt, Sneaker, Bag, Ankle boot\n",
    "- Created as a more challenging replacement for MNIST digits\n",
    "\n",
    "---\n",
    "\n",
    "## CNN Architecture Overview\n",
    "\n",
    "```\n",
    "Input (28√ó28√ó1)\n",
    "    ‚Üì\n",
    "Conv2D (32 filters, 3√ó3) ‚Üí ReLU ‚Üí MaxPool (2√ó2)\n",
    "    ‚Üì\n",
    "Conv2D (64 filters, 3√ó3) ‚Üí ReLU ‚Üí MaxPool (2√ó2)\n",
    "    ‚Üì\n",
    "Flatten ‚Üí Dense (64) ‚Üí ReLU\n",
    "    ‚Üì\n",
    "Output (10 classes) ‚Üí Softmax\n",
    "```\n",
    "\n",
    "**Expected Accuracy**: ~90% on test set\n",
    "\n",
    "---\n",
    "\n",
    "Let's get started! üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs('output', exist_ok=True)\n",
    "\n",
    "# Print versions\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(\"\\n‚úì Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Load Fashion-MNIST Dataset\n",
    "\n",
    "Keras provides built-in access to Fashion-MNIST. The dataset will automatically download on first use (~30MB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Fashion-MNIST dataset\n",
    "(X_train, y_train), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "# Define class names\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "# Print dataset information\n",
    "print(\"Dataset Information:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Training data shape: {X_train.shape}\")\n",
    "print(f\"Training labels shape: {y_train.shape}\")\n",
    "print(f\"Test data shape: {X_test.shape}\")\n",
    "print(f\"Test labels shape: {y_test.shape}\")\n",
    "print(f\"Number of classes: {len(class_names)}\")\n",
    "print(f\"Pixel value range: [{X_train.min()}, {X_train.max()}]\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Sample Images\n",
    "\n",
    "Let's look at some examples from each class to understand what we're working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize first 10 training images\n",
    "plt.figure(figsize=(12, 3))\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    plt.imshow(X_train[i], cmap='gray')\n",
    "    plt.title(class_names[y_train[i]], fontsize=9)\n",
    "    plt.axis('off')\n",
    "plt.suptitle('Fashion-MNIST Sample Images', fontsize=12, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Data Preprocessing\n",
    "\n",
    "Before training, we need to:\n",
    "\n",
    "1. **Normalize** pixel values from [0, 255] to [0, 1]\n",
    "2. **Reshape** data to add channel dimension for CNN input\n",
    "3. **One-hot encode** labels for multi-class classification\n",
    "\n",
    "### Why normalize?\n",
    "- Helps gradient descent converge faster\n",
    "- Prevents numerical instability\n",
    "- Standard practice in deep learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize pixel values to [0, 1] range\n",
    "X_train = X_train.astype('float32') / 255.0\n",
    "X_test = X_test.astype('float32') / 255.0\n",
    "\n",
    "print(f\"After normalization: [{X_train.min():.2f}, {X_train.max():.2f}]\")\n",
    "\n",
    "# Reshape for CNN input (add channel dimension)\n",
    "# CNN expects: (samples, height, width, channels)\n",
    "X_train = X_train.reshape(-1, 28, 28, 1)\n",
    "X_test = X_test.reshape(-1, 28, 28, 1)\n",
    "\n",
    "print(f\"\\nReshaped X_train: {X_train.shape}\")\n",
    "print(f\"Reshaped X_test: {X_test.shape}\")\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_train_categorical = keras.utils.to_categorical(y_train, 10)\n",
    "y_test_categorical = keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "print(f\"\\nOne-hot encoded y_train: {y_train_categorical.shape}\")\n",
    "print(f\"Example: Label {y_train[0]} ‚Üí {y_train_categorical[0]}\")\n",
    "print(\"\\n‚úì Preprocessing complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Build CNN Architecture\n",
    "\n",
    "### Layer-by-Layer Breakdown:\n",
    "\n",
    "1. **Conv2D(32, 3√ó3)**: First convolutional layer with 32 filters\n",
    "   - Learns 32 different 3√ó3 patterns (edges, textures)\n",
    "   - Output: 26√ó26√ó32\n",
    "\n",
    "2. **MaxPooling2D(2√ó2)**: Downsampling layer\n",
    "   - Reduces spatial dimensions by half\n",
    "   - Output: 13√ó13√ó32\n",
    "\n",
    "3. **Conv2D(64, 3√ó3)**: Second convolutional layer with 64 filters\n",
    "   - Learns more complex patterns from first layer features\n",
    "   - Output: 11√ó11√ó64\n",
    "\n",
    "4. **MaxPooling2D(2√ó2)**: Second downsampling\n",
    "   - Output: 5√ó5√ó64\n",
    "\n",
    "5. **Flatten**: Convert 2D feature maps to 1D vector (5√ó5√ó64 = 1,600)\n",
    "\n",
    "6. **Dense(64)**: Fully connected layer for high-level reasoning\n",
    "\n",
    "7. **Dense(10, softmax)**: Output layer for 10 classes\n",
    "\n",
    "### Total Parameters: ~122,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Sequential model\n",
    "model = keras.Sequential([\n",
    "    # First convolutional block\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', \n",
    "                  input_shape=(28, 28, 1), name='conv1'),\n",
    "    layers.MaxPooling2D((2, 2), name='pool1'),\n",
    "    \n",
    "    # Second convolutional block\n",
    "    layers.Conv2D(64, (3, 3), activation='relu', name='conv2'),\n",
    "    layers.MaxPooling2D((2, 2), name='pool2'),\n",
    "    \n",
    "    # Fully connected layers\n",
    "    layers.Flatten(name='flatten'),\n",
    "    layers.Dense(64, activation='relu', name='dense1'),\n",
    "    layers.Dense(10, activation='softmax', name='output')\n",
    "], name='Fashion_MNIST_CNN')\n",
    "\n",
    "# Display model architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìê Output Dimension Calculations\n",
    "\n",
    "**Formula for convolution output size:**\n",
    "```\n",
    "output_size = (input_size - kernel_size + 2 √ó padding) / stride + 1\n",
    "```\n",
    "\n",
    "**Formula for pooling output size:**\n",
    "```\n",
    "output_size = input_size / pool_size\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"OUTPUT DIMENSION CALCULATIONS:\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Input: 28√ó28√ó1\\n\")\n",
    "\n",
    "print(\"Conv1 (32 filters, 3√ó3, stride=1, padding='valid'):\")\n",
    "print(\"  Output = (28 - 3 + 0) / 1 + 1 = 26√ó26√ó32\")\n",
    "print(\"  Parameters: (3√ó3√ó1 + 1) √ó 32 = 320\\n\")\n",
    "\n",
    "print(\"Pool1 (2√ó2, stride=2):\")\n",
    "print(\"  Output = 26 / 2 = 13√ó13√ó32\")\n",
    "print(\"  Parameters: 0 (no trainable params)\\n\")\n",
    "\n",
    "print(\"Conv2 (64 filters, 3√ó3, stride=1, padding='valid'):\")\n",
    "print(\"  Output = (13 - 3 + 0) / 1 + 1 = 11√ó11√ó64\")\n",
    "print(\"  Parameters: (3√ó3√ó32 + 1) √ó 64 = 18,496\\n\")\n",
    "\n",
    "print(\"Pool2 (2√ó2, stride=2):\")\n",
    "print(\"  Output = 11 / 2 = 5√ó5√ó64 (floor)\")\n",
    "print(\"  Parameters: 0\\n\")\n",
    "\n",
    "print(\"Flatten: 5√ó5√ó64 = 1,600 features\")\n",
    "print(\"Dense1: (1,600 + 1) √ó 64 = 102,464 parameters\")\n",
    "print(\"Output: (64 + 1) √ó 10 = 650 parameters\")\n",
    "print(\"\\nTotal: ~122,000 parameters\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Compile Model\n",
    "\n",
    "Configure the model for training:\n",
    "\n",
    "- **Optimizer**: Adam (adaptive learning rate)\n",
    "- **Loss**: Categorical crossentropy (for multi-class classification)\n",
    "- **Metrics**: Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"‚úì Model compiled successfully!\")\n",
    "print(\"  Optimizer: Adam\")\n",
    "print(\"  Loss: Categorical Crossentropy\")\n",
    "print(\"  Metrics: Accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Train Model\n",
    "\n",
    "Train for 10 epochs with:\n",
    "- 20% validation split\n",
    "- Batch size of 128\n",
    "\n",
    "**Expected training time**:\n",
    "- CPU: 2-3 minutes\n",
    "- GPU: 30-45 seconds\n",
    "\n",
    "üöÄ **Click Run and watch the training progress!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train_categorical,\n",
    "    epochs=10,\n",
    "    batch_size=128,\n",
    "    validation_split=0.2,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n‚úì Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Visualize Training History\n",
    "\n",
    "Plot accuracy and loss curves to understand:\n",
    "- How well the model learned\n",
    "- Whether there's overfitting (training >> validation)\n",
    "- When training converged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Accuracy plot\n",
    "axes[0].plot(history.history['accuracy'], label='Training', \n",
    "             marker='o', linewidth=2, markersize=6)\n",
    "axes[0].plot(history.history['val_accuracy'], label='Validation', \n",
    "             marker='s', linewidth=2, markersize=6)\n",
    "axes[0].set_title('Model Accuracy', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Loss plot\n",
    "axes[1].plot(history.history['loss'], label='Training', \n",
    "             marker='o', linewidth=2, markersize=6)\n",
    "axes[1].plot(history.history['val_loss'], label='Validation', \n",
    "             marker='s', linewidth=2, markersize=6)\n",
    "axes[1].set_title('Model Loss', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Evaluate on Test Set\n",
    "\n",
    "Test the model on unseen data to measure true generalization performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test data\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test_categorical, verbose=0)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"FINAL TEST RESULTS:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Test Loss:     {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Predictions\n",
    "\n",
    "See what the model predicts for individual test images.\n",
    "- **Green** titles = correct predictions\n",
    "- **Red** titles = incorrect predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "predictions = model.predict(X_test[:10], verbose=0)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "true_classes = y_test[:10]\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(15, 6))\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    plt.imshow(X_test[i].reshape(28, 28), cmap='gray')\n",
    "    \n",
    "    pred = predicted_classes[i]\n",
    "    true = true_classes[i]\n",
    "    conf = predictions[i][pred] * 100\n",
    "    \n",
    "    color = 'green' if pred == true else 'red'\n",
    "    plt.title(f'Pred: {class_names[pred]} ({conf:.1f}%)\\n'\n",
    "              f'True: {class_names[true]}', \n",
    "              color=color, fontsize=9)\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.suptitle('Model Predictions (Green=Correct, Red=Wrong)', \n",
    "             fontsize=13, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 8: Visualize Learned Filters\n",
    "\n",
    "Let's peek inside the CNN to see what filters it learned in the first convolutional layer.\n",
    "\n",
    "These 3√ó3 filters are **learned automatically** from data (not hand-crafted like Sobel or Gabor filters)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract first conv layer weights\n",
    "filters, biases = model.layers[0].get_weights()\n",
    "print(f\"Filter shape: {filters.shape}\")\n",
    "print(f\"(height, width, input_channels, output_filters)\\n\")\n",
    "\n",
    "# Normalize for visualization\n",
    "f_min, f_max = filters.min(), filters.max()\n",
    "filters_norm = (filters - f_min) / (f_max - f_min)\n",
    "\n",
    "# Visualize 16 filters\n",
    "fig, axes = plt.subplots(4, 4, figsize=(10, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(16):\n",
    "    filter_img = filters_norm[:, :, 0, i]\n",
    "    axes[i].imshow(filter_img, cmap='viridis')\n",
    "    axes[i].set_title(f'Filter {i+1}', fontsize=9)\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle('Learned 3√ó3 Filters in First Conv Layer', \n",
    "             fontsize=13, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"These filters detect edges, textures, and basic patterns!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 9: Visualize Feature Maps\n",
    "\n",
    "See what happens to an image as it passes through the CNN layers.\n",
    "\n",
    "**Hierarchical Feature Learning**:\n",
    "- **Conv1**: Simple features (edges, textures)\n",
    "- **Conv2**: Complex features (shapes, patterns)\n",
    "\n",
    "This demonstrates how CNNs automatically learn a hierarchy of features!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create feature extraction model\n# Note: Sequential models require an explicit Input layer for feature extraction\n# We'll rebuild the connections using the trained model's layers\n\n# Create a new input tensor\ninputs = keras.Input(shape=(28, 28, 1))\n\n# Apply each layer sequentially and collect outputs\nx = inputs\nlayer_outputs_dict = {}\nfor layer in model.layers:\n    x = layer(x)\n    layer_outputs_dict[layer.name] = x\n\n# Select the layers we want to visualize\nlayer_names = ['conv1', 'pool1', 'conv2', 'pool2']\nlayer_outputs = [layer_outputs_dict[name] for name in layer_names]\n\n# Create the feature extraction model\nactivation_model = keras.Model(inputs=inputs, outputs=layer_outputs)\n\n# Get activations for a sample image\nsample_idx = 0\nsample_image = X_test[sample_idx:sample_idx+1]\nsample_label = class_names[y_test[sample_idx]]\n\nprint(f\"Analyzing: {sample_label}\\n\")\nactivations = activation_model.predict(sample_image, verbose=0)\n\n# Visualize feature maps\nfig, axes = plt.subplots(2, 3, figsize=(15, 10))\naxes = axes.flatten()\n\n# Original image\naxes[0].imshow(sample_image[0, :, :, 0], cmap='gray')\naxes[0].set_title(f'Original\\n({sample_label})', fontweight='bold')\naxes[0].axis('off')\n\n# Feature maps for each layer\npositions = [(0, 1), (0, 2), (1, 1), (1, 2)]\nfor idx, (layer_name, activation) in enumerate(zip(layer_names, activations)):\n    row, col = positions[idx]\n    n_features = activation.shape[-1]\n    size_h, size_w = activation.shape[1], activation.shape[2]\n    \n    # Create grid\n    n_cols = 8\n    n_rows = min(4, n_features // n_cols)\n    display_grid = np.zeros((size_h * n_rows, size_w * n_cols))\n    \n    for row_idx in range(n_rows):\n        for col_idx in range(n_cols):\n            channel_idx = row_idx * n_cols + col_idx\n            if channel_idx < n_features:\n                channel_image = activation[0, :, :, channel_idx]\n                channel_image -= channel_image.mean()\n                if channel_image.std() > 0:\n                    channel_image /= channel_image.std()\n                channel_image = np.clip(channel_image, 0, 1)\n                display_grid[row_idx * size_h:(row_idx + 1) * size_h,\n                             col_idx * size_w:(col_idx + 1) * size_w] = channel_image\n    \n    axes[row * 3 + col].imshow(display_grid, cmap='viridis')\n    axes[row * 3 + col].set_title(f'{layer_name}\\n{activation.shape[1:]}', \n                                   fontweight='bold')\n    axes[row * 3 + col].axis('off')\n\naxes[3].axis('off')  # Hide unused subplot\n\nplt.suptitle('Feature Maps: Hierarchical Learning in Action', \n             fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n‚úì Feature maps generated successfully!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéì Tutorial Complete!\n",
    "\n",
    "### What We Accomplished:\n",
    "\n",
    "‚úÖ Loaded Fashion-MNIST (60,000 train + 10,000 test images)  \n",
    "‚úÖ Built CNN with 2 convolutional blocks (~122,000 parameters)  \n",
    "‚úÖ Trained model for 10 epochs (~2-3 minutes)  \n",
    "‚úÖ Achieved **~90% accuracy** on test set  \n",
    "‚úÖ Visualized training history, predictions, filters, and feature maps  \n",
    "\n",
    "---\n",
    "\n",
    "### üìö Key Takeaways:\n",
    "\n",
    "1. **CNNs excel at image tasks** due to:\n",
    "   - Local connectivity (learn spatial patterns)\n",
    "   - Weight sharing (parameter efficiency)\n",
    "   - Translation equivariance (same features anywhere)\n",
    "\n",
    "2. **Hierarchical feature learning**:\n",
    "   - Early layers: edges, textures\n",
    "   - Deeper layers: shapes, objects\n",
    "\n",
    "3. **Pooling layers** add:\n",
    "   - Translation invariance\n",
    "   - Dimension reduction\n",
    "\n",
    "4. **Fewer parameters than MLP**:\n",
    "   - CNN: ~122,000 parameters\n",
    "   - Equivalent MLP: millions of parameters\n",
    "\n",
    "---\n",
    "\n",
    "### üè† Homework (Due: Before Week 11)\n",
    "\n",
    "**Task 1**: Manual convolution calculation (6√ó6 image, 3√ó3 kernel)  \n",
    "**Task 2**: Design CNN for MNIST with justification  \n",
    "**Task 3**: Modify this notebook - add layer, experiment with kernel sizes  \n",
    "\n",
    "---\n",
    "\n",
    "### üìñ Next Week (Week 11):\n",
    "\n",
    "- Famous CNN architectures (LeNet, AlexNet, VGG, ResNet)\n",
    "- Advanced techniques (Dropout, Batch Normalization)\n",
    "- Designing deeper networks\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Unit Test 2 Prep (October 31):\n",
    "\n",
    "- Review convolution calculations\n",
    "- Practice output dimension formulas\n",
    "- Understand parameter counting\n",
    "- Be ready to design CNN architectures\n",
    "\n",
    "---\n",
    "\n",
    "**Great work! üöÄ You've built your first CNN!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üöÄ Extension Exercises (Optional)\n",
    "\n",
    "For students who finish early or want extra practice:\n",
    "\n",
    "### Exercise 1: Add Dropout\n",
    "Add `Dropout(0.25)` after each pooling layer and `Dropout(0.5)` after the dense layer. Compare accuracy with and without dropout.\n",
    "\n",
    "### Exercise 2: Experiment with Architecture\n",
    "Try:\n",
    "- Different number of filters (16, 32, 64, 128)\n",
    "- Different kernel sizes (3√ó3, 5√ó5, 7√ó7)\n",
    "- Adding a third convolutional block\n",
    "\n",
    "### Exercise 3: Build MLP Baseline\n",
    "Create an MLP (Flatten ‚Üí Dense ‚Üí Dense ‚Üí Output) with similar number of parameters. Compare training time and accuracy with CNN.\n",
    "\n",
    "### Exercise 4: Confusion Matrix\n",
    "Generate predictions for entire test set and create a confusion matrix using `sklearn.metrics.confusion_matrix`. Which classes are most confused?\n",
    "\n",
    "### Exercise 5: Data Augmentation\n",
    "Use `ImageDataGenerator` to apply random rotations, shifts, and flips during training. Does it improve accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extension Exercise Code Template\n",
    "# Uncomment and modify as needed\n",
    "\n",
    "# Example: Add dropout\n",
    "# model_with_dropout = keras.Sequential([\n",
    "#     layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "#     layers.MaxPooling2D((2, 2)),\n",
    "#     layers.Dropout(0.25),  # Add dropout\n",
    "#     \n",
    "#     layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "#     layers.MaxPooling2D((2, 2)),\n",
    "#     layers.Dropout(0.25),  # Add dropout\n",
    "#     \n",
    "#     layers.Flatten(),\n",
    "#     layers.Dense(64, activation='relu'),\n",
    "#     layers.Dropout(0.5),  # Add dropout\n",
    "#     layers.Dense(10, activation='softmax')\n",
    "# ])\n",
    "\n",
    "# TODO: Your extension exercise code here\n",
    "pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}