{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: ROI Extraction & Morphological Operations\n",
    "## Week 8, Day 4 - Module 3: Image Processing & DNNs\n",
    "\n",
    "**Course**: Deep Neural Network Architectures (21CSE558T)  \n",
    "**Date**: Wednesday, October 9, 2025  \n",
    "**Duration**: 1 Hour (60 minutes)  \n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "1. Extract Regions of Interest (ROI) from images using OpenCV\n",
    "2. Apply morphological operations to clean binary images\n",
    "3. Build an integrated image processing pipeline\n",
    "4. Solve real-world image preprocessing problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Import Libraries and Create Output Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs('output', exist_ok=True)\n",
    "\n",
    "# Configure matplotlib for inline display\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "# Helper function to display images in Jupyter\n",
    "def show_images(images, titles, rows=1, cols=None, figsize=(15, 8), cmap='gray'):\n",
    "    \"\"\"Display multiple images in a grid\"\"\"\n",
    "    if cols is None:\n",
    "        cols = len(images)\n",
    "    \n",
    "    fig, axes = plt.subplots(rows, cols, figsize=figsize)\n",
    "    \n",
    "    if rows == 1 and cols == 1:\n",
    "        axes = [axes]\n",
    "    elif rows == 1 or cols == 1:\n",
    "        axes = axes.flatten()\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "    \n",
    "    for i, (img, title) in enumerate(zip(images, titles)):\n",
    "        if len(img.shape) == 3 and img.shape[2] == 3:\n",
    "            # BGR to RGB conversion for color images\n",
    "            axes[i].imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "        else:\n",
    "            axes[i].imshow(img, cmap=cmap)\n",
    "        axes[i].set_title(title)\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"✓ Libraries imported successfully\")\n",
    "print(\"✓ Output directory created\")\n",
    "print(\"✓ Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 1: ROI Extraction (25 minutes)\n",
    "\n",
    "## Concept: Region of Interest (ROI)\n",
    "ROI is a portion of an image selected for specific processing or analysis.\n",
    "\n",
    "**Real-World Examples:**\n",
    "- Cropping faces from group photos\n",
    "- Extracting license plates from traffic cameras\n",
    "- Isolating tumor regions in medical imaging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1: Rectangular ROI Extraction (Array Slicing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample image (or load your own)\n",
    "# For demo purposes, we'll create a synthetic image\n",
    "img = np.zeros((400, 600, 3), dtype=np.uint8)\n",
    "\n",
    "# Draw some shapes\n",
    "cv2.rectangle(img, (50, 50), (250, 250), (255, 0, 0), -1)  # Blue rectangle\n",
    "cv2.circle(img, (450, 200), 80, (0, 255, 0), -1)  # Green circle\n",
    "cv2.putText(img, 'ROI DEMO', (200, 350), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "\n",
    "print(f\"Image shape: {img.shape}\")\n",
    "print(f\"Image dimensions: {img.shape[1]} x {img.shape[0]} (width x height)\")\n",
    "\n",
    "# Display\n",
    "show_images([img], ['Original Image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define ROI coordinates\n",
    "x, y, w, h = 50, 50, 200, 200  # Extract the blue rectangle\n",
    "\n",
    "# Visualize ROI selection (draw rectangle)\n",
    "img_with_roi = img.copy()\n",
    "cv2.rectangle(img_with_roi, (x, y), (x+w, y+h), (0, 255, 255), 3)  # Yellow rectangle\n",
    "\n",
    "# Extract ROI using array slicing\n",
    "roi = img[y:y+h, x:x+w]\n",
    "\n",
    "print(f\"ROI coordinates: x={x}, y={y}, width={w}, height={h}\")\n",
    "print(f\"ROI shape: {roi.shape}\")\n",
    "\n",
    "# Display results\n",
    "show_images([img_with_roi, roi], \n",
    "            ['ROI Selection (Yellow Box)', 'Extracted ROI'],\n",
    "            cols=2)\n",
    "\n",
    "# Save ROI\n",
    "cv2.imwrite('output/roi_extracted.jpg', roi)\n",
    "print(\"✓ ROI saved to output/roi_extracted.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.1: Extract Multiple ROIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Extract the green circle as a separate ROI\n",
    "# Hint: The circle center is at (450, 200) with radius 80\n",
    "\n",
    "# Define coordinates for circle ROI\n",
    "x2 = 370  # 450 - 80\n",
    "y2 = 120  # 200 - 80\n",
    "w2 = 160  # diameter = 2 * radius\n",
    "h2 = 160\n",
    "\n",
    "# Extract circle ROI\n",
    "roi_circle = img[y2:y2+h2, x2:x2+w2]\n",
    "\n",
    "# Display\n",
    "show_images([img, roi_circle], \n",
    "            ['Original Image', 'Circle ROI'],\n",
    "            cols=2)\n",
    "\n",
    "print(\"✓ Exercise 1.1 completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2: Contour-Based ROI Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create image with multiple objects (coins simulation)\n",
    "coins_img = np.zeros((400, 600), dtype=np.uint8)\n",
    "\n",
    "# Draw circles (simulate coins)\n",
    "cv2.circle(coins_img, (100, 100), 40, 255, -1)\n",
    "cv2.circle(coins_img, (250, 150), 50, 255, -1)\n",
    "cv2.circle(coins_img, (450, 120), 45, 255, -1)\n",
    "cv2.circle(coins_img, (200, 300), 35, 255, -1)\n",
    "cv2.circle(coins_img, (400, 300), 55, 255, -1)\n",
    "\n",
    "# Add some noise\n",
    "cv2.circle(coins_img, (50, 350), 5, 255, -1)  # Small noise\n",
    "\n",
    "print(f\"Coins image shape: {coins_img.shape}\")\n",
    "show_images([coins_img], ['Binary Image with Objects'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find contours\n",
    "contours, hierarchy = cv2.findContours(coins_img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "print(f\"Total contours found: {len(contours)}\")\n",
    "\n",
    "# Visualize all contours\n",
    "coins_color = cv2.cvtColor(coins_img, cv2.COLOR_GRAY2BGR)\n",
    "cv2.drawContours(coins_color, contours, -1, (0, 255, 0), 2)\n",
    "\n",
    "show_images([coins_color], ['Detected Contours (Green)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract ROI for each contour with filtering\n",
    "coins_with_boxes = cv2.cvtColor(coins_img, cv2.COLOR_GRAY2BGR)\n",
    "extracted_rois = []\n",
    "valid_count = 0\n",
    "\n",
    "for i, contour in enumerate(contours):\n",
    "    # Calculate area to filter noise\n",
    "    area = cv2.contourArea(contour)\n",
    "    \n",
    "    # Filter: only process contours with area > 500 (removes small noise)\n",
    "    if area > 500:\n",
    "        # Get bounding rectangle\n",
    "        x, y, w, h = cv2.boundingRect(contour)\n",
    "        \n",
    "        # Draw bounding box\n",
    "        cv2.rectangle(coins_with_boxes, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "        \n",
    "        # Extract ROI\n",
    "        roi = coins_img[y:y+h, x:x+w]\n",
    "        extracted_rois.append(roi)\n",
    "        \n",
    "        # Save individual ROI\n",
    "        cv2.imwrite(f'output/coin_{valid_count}.jpg', roi)\n",
    "        \n",
    "        print(f\"Coin {valid_count}: Area={area:.0f}, Size={w}x{h}\")\n",
    "        valid_count += 1\n",
    "\n",
    "print(f\"\\n✓ Extracted {valid_count} valid objects (filtered out noise)\")\n",
    "\n",
    "# Display results\n",
    "show_images([coins_with_boxes], ['Bounding Boxes (Blue)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all extracted ROIs in a grid\n",
    "if len(extracted_rois) > 0:\n",
    "    titles = [f'Coin {i}' for i in range(len(extracted_rois))]\n",
    "    show_images(extracted_rois, titles, rows=2, cols=3)\n",
    "    print(\"✓ All extracted coins displayed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.2: Contour-Based Extraction Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Modify the area threshold and observe the results\n",
    "# Try different values: 100, 500, 1000, 2000\n",
    "\n",
    "min_area = 1000  # Change this value\n",
    "\n",
    "filtered_count = 0\n",
    "for contour in contours:\n",
    "    if cv2.contourArea(contour) > min_area:\n",
    "        filtered_count += 1\n",
    "\n",
    "print(f\"With min_area = {min_area}:\")\n",
    "print(f\"  Objects detected: {filtered_count}\")\n",
    "print(f\"  Objects filtered out: {len(contours) - filtered_count}\")\n",
    "\n",
    "# Question: What's the optimal min_area to keep all coins but remove noise?\n",
    "# Answer: Around 500-800 pixels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 2: Morphological Operations (25 minutes)\n",
    "\n",
    "## Concept: Morphological Operations\n",
    "Operations that process images based on shapes using a structuring element (kernel).\n",
    "\n",
    "**Key Operations:**\n",
    "- **Erosion**: Shrinks objects, removes noise\n",
    "- **Dilation**: Expands objects, fills holes\n",
    "- **Opening**: Erosion → Dilation (removes small objects)\n",
    "- **Closing**: Dilation → Erosion (fills small holes)\n",
    "- **Gradient**: Dilation - Erosion (extracts edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo: All Morphological Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a noisy binary image\n",
    "noisy_img = np.zeros((300, 300), dtype=np.uint8)\n",
    "\n",
    "# Main object (filled rectangle)\n",
    "cv2.rectangle(noisy_img, (50, 50), (250, 250), 255, -1)\n",
    "\n",
    "# Add holes (simulate broken parts)\n",
    "cv2.circle(noisy_img, (100, 100), 10, 0, -1)\n",
    "cv2.circle(noisy_img, (200, 150), 15, 0, -1)\n",
    "\n",
    "# Add noise (small white dots)\n",
    "for _ in range(20):\n",
    "    x, y = np.random.randint(10, 290, 2)\n",
    "    cv2.circle(noisy_img, (x, y), 3, 255, -1)\n",
    "\n",
    "show_images([noisy_img], ['Noisy Binary Image'])\n",
    "print(\"Created noisy image with:\")\n",
    "print(\"  - Main object (rectangle)\")\n",
    "print(\"  - Holes inside object (black circles)\")\n",
    "print(\"  - Noise outside object (white dots)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define structuring element (kernel)\n",
    "kernel = np.ones((5, 5), np.uint8)\n",
    "\n",
    "# Apply all morphological operations\n",
    "erosion = cv2.erode(noisy_img, kernel, iterations=1)\n",
    "dilation = cv2.dilate(noisy_img, kernel, iterations=1)\n",
    "opening = cv2.morphologyEx(noisy_img, cv2.MORPH_OPEN, kernel)\n",
    "closing = cv2.morphologyEx(noisy_img, cv2.MORPH_CLOSE, kernel)\n",
    "gradient = cv2.morphologyEx(noisy_img, cv2.MORPH_GRADIENT, kernel)\n",
    "\n",
    "# Display all results\n",
    "images = [noisy_img, erosion, dilation, opening, closing, gradient]\n",
    "titles = ['Original', 'Erosion\\n(Shrink)', 'Dilation\\n(Expand)', \n",
    "          'Opening\\n(Remove Noise)', 'Closing\\n(Fill Holes)', 'Gradient\\n(Edges)']\n",
    "\n",
    "show_images(images, titles, rows=2, cols=3, figsize=(18, 10))\n",
    "\n",
    "print(\"\\n📊 Observations:\")\n",
    "print(\"  Erosion: Made everything smaller, removed small noise\")\n",
    "print(\"  Dilation: Made everything larger, filled some holes\")\n",
    "print(\"  Opening: Removed noise but preserved main object\")\n",
    "print(\"  Closing: Filled holes inside the object\")\n",
    "print(\"  Gradient: Extracted the boundary/edge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.1: Noise Removal with Opening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a document-like image with text and noise\n",
    "doc_img = np.zeros((200, 400), dtype=np.uint8)\n",
    "\n",
    "# Simulate text (white rectangles)\n",
    "cv2.rectangle(doc_img, (20, 50), (100, 70), 255, -1)\n",
    "cv2.rectangle(doc_img, (120, 50), (200, 70), 255, -1)\n",
    "cv2.rectangle(doc_img, (20, 90), (150, 110), 255, -1)\n",
    "cv2.rectangle(doc_img, (20, 130), (180, 150), 255, -1)\n",
    "\n",
    "# Add salt-and-pepper noise\n",
    "noise = np.random.randint(0, 2, doc_img.shape) * 255\n",
    "noise = (noise * 0.05).astype(np.uint8)  # 5% noise\n",
    "doc_noisy = cv2.bitwise_or(doc_img, noise)\n",
    "\n",
    "show_images([doc_noisy], ['Noisy Document'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Apply opening to remove noise\n",
    "# Experiment with different kernel sizes\n",
    "\n",
    "# Try different kernels\n",
    "kernel_3x3 = np.ones((3, 3), np.uint8)\n",
    "kernel_5x5 = np.ones((5, 5), np.uint8)\n",
    "kernel_7x7 = np.ones((7, 7), np.uint8)\n",
    "\n",
    "# Apply opening with different kernels\n",
    "opening_3x3 = cv2.morphologyEx(doc_noisy, cv2.MORPH_OPEN, kernel_3x3)\n",
    "opening_5x5 = cv2.morphologyEx(doc_noisy, cv2.MORPH_OPEN, kernel_5x5)\n",
    "opening_7x7 = cv2.morphologyEx(doc_noisy, cv2.MORPH_OPEN, kernel_7x7)\n",
    "\n",
    "# Display comparison\n",
    "images = [doc_noisy, opening_3x3, opening_5x5, opening_7x7]\n",
    "titles = ['Noisy Original', 'Opening 3×3', 'Opening 5×5', 'Opening 7×7']\n",
    "\n",
    "show_images(images, titles, rows=1, cols=4, figsize=(20, 5))\n",
    "\n",
    "print(\"\\n🔍 Kernel Size Effects:\")\n",
    "print(\"  3×3: Mild noise removal, text preserved\")\n",
    "print(\"  5×5: Good balance, most noise removed\")\n",
    "print(\"  7×7: Aggressive, may damage thin text\")\n",
    "print(\"\\n✓ Exercise 2.1 completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.2: Fill Holes with Closing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create text with holes\n",
    "text_img = np.zeros((150, 300), dtype=np.uint8)\n",
    "\n",
    "# Draw text-like shapes with intentional holes\n",
    "cv2.rectangle(text_img, (20, 50), (80, 100), 255, -1)\n",
    "cv2.circle(text_img, (50, 75), 10, 0, -1)  # Hole 1\n",
    "\n",
    "cv2.rectangle(text_img, (100, 50), (160, 100), 255, -1)\n",
    "cv2.circle(text_img, (130, 75), 8, 0, -1)  # Hole 2\n",
    "\n",
    "cv2.rectangle(text_img, (180, 50), (240, 100), 255, -1)\n",
    "cv2.circle(text_img, (210, 75), 12, 0, -1)  # Hole 3\n",
    "\n",
    "show_images([text_img], ['Text with Holes'])\n",
    "print(\"Created text with internal holes (black circles)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Apply closing to fill holes\n",
    "kernel = np.ones((7, 7), np.uint8)\n",
    "closing = cv2.morphologyEx(text_img, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "# Display comparison\n",
    "show_images([text_img, closing], \n",
    "            ['Text with Holes', 'After Closing (Holes Filled)'],\n",
    "            cols=2)\n",
    "\n",
    "print(\"✓ Holes successfully filled!\")\n",
    "print(\"✓ Exercise 2.2 completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.3: Morphological Gradient for Edge Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the coins image from earlier\n",
    "kernel = np.ones((5, 5), np.uint8)\n",
    "\n",
    "# Apply morphological gradient\n",
    "gradient = cv2.morphologyEx(coins_img, cv2.MORPH_GRADIENT, kernel)\n",
    "\n",
    "# Compare with Canny edge detection\n",
    "canny_edges = cv2.Canny(coins_img, 100, 200)\n",
    "\n",
    "# Display comparison\n",
    "show_images([coins_img, gradient, canny_edges], \n",
    "            ['Original', 'Morphological Gradient', 'Canny Edges'],\n",
    "            cols=3, figsize=(18, 5))\n",
    "\n",
    "print(\"\\n📊 Comparison:\")\n",
    "print(\"  Morphological Gradient: Thick edges, shape-based\")\n",
    "print(\"  Canny: Thin edges, intensity-based\")\n",
    "print(\"\\n✓ Exercise 2.3 completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 3: Integration Exercise (10 minutes)\n",
    "\n",
    "## Mini-Project: Complete Document Processing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a complex document image\n",
    "document = np.zeros((400, 600), dtype=np.uint8)\n",
    "\n",
    "# Add text blocks\n",
    "cv2.rectangle(document, (50, 50), (250, 120), 255, -1)\n",
    "cv2.rectangle(document, (300, 50), (550, 120), 255, -1)\n",
    "cv2.rectangle(document, (50, 150), (550, 220), 255, -1)\n",
    "cv2.rectangle(document, (50, 250), (350, 320), 255, -1)\n",
    "\n",
    "# Add holes in text\n",
    "cv2.circle(document, (150, 85), 8, 0, -1)\n",
    "cv2.circle(document, (425, 85), 10, 0, -1)\n",
    "cv2.circle(document, (300, 185), 12, 0, -1)\n",
    "\n",
    "# Add noise\n",
    "for _ in range(50):\n",
    "    x, y = np.random.randint(10, 590), np.random.randint(10, 390)\n",
    "    cv2.circle(document, (x, y), 2, 255, -1)\n",
    "\n",
    "show_images([document], ['Complex Document: Noisy with Holes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document_pipeline(binary_image, min_area=1000):\n",
    "    \"\"\"\n",
    "    Complete document processing pipeline\n",
    "    \n",
    "    Steps:\n",
    "    1. Remove noise (Opening)\n",
    "    2. Fill holes (Closing)\n",
    "    3. Find text regions (Contours)\n",
    "    4. Extract ROIs\n",
    "    \"\"\"\n",
    "    # Step 1: Remove noise with opening\n",
    "    kernel_open = np.ones((3, 3), np.uint8)\n",
    "    cleaned = cv2.morphologyEx(binary_image, cv2.MORPH_OPEN, kernel_open)\n",
    "    \n",
    "    # Step 2: Fill holes with closing\n",
    "    kernel_close = np.ones((9, 9), np.uint8)\n",
    "    filled = cv2.morphologyEx(cleaned, cv2.MORPH_CLOSE, kernel_close)\n",
    "    \n",
    "    # Step 3: Find contours\n",
    "    contours, _ = cv2.findContours(filled, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # Step 4: Extract ROIs\n",
    "    rois = []\n",
    "    valid_count = 0\n",
    "    \n",
    "    # Create visualization image\n",
    "    result_img = cv2.cvtColor(filled, cv2.COLOR_GRAY2BGR)\n",
    "    \n",
    "    for contour in contours:\n",
    "        area = cv2.contourArea(contour)\n",
    "        if area > min_area:\n",
    "            x, y, w, h = cv2.boundingRect(contour)\n",
    "            \n",
    "            # Draw bounding box\n",
    "            cv2.rectangle(result_img, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "            \n",
    "            # Extract ROI\n",
    "            roi = filled[y:y+h, x:x+w]\n",
    "            rois.append(roi)\n",
    "            \n",
    "            # Save ROI\n",
    "            cv2.imwrite(f'output/text_region_{valid_count}.jpg', roi)\n",
    "            valid_count += 1\n",
    "    \n",
    "    return cleaned, filled, result_img, rois, valid_count\n",
    "\n",
    "# Run the pipeline\n",
    "cleaned, filled, result, rois, num_regions = process_document_pipeline(document)\n",
    "\n",
    "# Display pipeline stages\n",
    "show_images([document, cleaned, filled, result], \n",
    "            ['1. Original', '2. After Opening', '3. After Closing', '4. Detected Regions'],\n",
    "            rows=1, cols=4, figsize=(20, 5))\n",
    "\n",
    "print(f\"\\n✅ Pipeline Results:\")\n",
    "print(f\"  ✓ Noise removed\")\n",
    "print(f\"  ✓ Holes filled\")\n",
    "print(f\"  ✓ {num_regions} text regions extracted\")\n",
    "print(f\"  ✓ ROIs saved to output/ folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all extracted text regions\n",
    "if len(rois) > 0:\n",
    "    titles = [f'Text Region {i}' for i in range(len(rois))]\n",
    "    show_images(rois, titles, rows=2, cols=2, figsize=(12, 8))\n",
    "    print(\"✓ All text regions successfully extracted!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Summary and Key Takeaways"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ✅ What We Learned\n",
    "\n",
    "### ROI Extraction:\n",
    "1. **Rectangular ROI**: `img[y:y+h, x:x+w]` for known coordinates\n",
    "2. **Contour-based ROI**: Automatic extraction using `cv2.findContours()` and `cv2.boundingRect()`\n",
    "3. **Filtering**: Use `cv2.contourArea()` to remove noise\n",
    "\n",
    "### Morphological Operations:\n",
    "1. **Opening (Erode→Dilate)**: Removes noise, preserves large objects\n",
    "2. **Closing (Dilate→Erode)**: Fills holes, connects gaps\n",
    "3. **Gradient**: Edge detection for binary images\n",
    "4. **Kernel size**: Larger = stronger effect, but can damage details\n",
    "\n",
    "### Pipeline Thinking:\n",
    "1. Process in stages: Segment → Clean → Extract\n",
    "2. Save intermediate results for debugging\n",
    "3. Apply operations on binary, extract ROI from original\n",
    "4. Filter by area/size to remove unwanted regions\n",
    "\n",
    "---\n",
    "\n",
    "## 🔗 Connection to Future Topics\n",
    "\n",
    "- **Week 9**: Feature extraction from these ROIs (shape, color, texture)\n",
    "- **Module 4**: R-CNN uses ROI extraction for object detection\n",
    "- **Real Applications**: Document digitization, medical imaging, quality control\n",
    "\n",
    "---\n",
    "\n",
    "## 📝 Practice Exercises\n",
    "\n",
    "Try these on your own:\n",
    "1. Load a real document image and extract text regions\n",
    "2. Process a photo with multiple objects (fruits, coins, etc.)\n",
    "3. Build a pipeline for license plate detection\n",
    "4. Experiment with different kernel shapes (cross, ellipse)\n",
    "\n",
    "---\n",
    "\n",
    "**End of Tutorial**\n",
    "\n",
    "*Course: 21CSE558T - Deep Neural Network Architectures*  \n",
    "*Module 3: Image Processing & Deep Neural Networks*  \n",
    "*Week 8, Day 4 - October 9, 2025*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
